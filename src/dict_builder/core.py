# Path: src/dict_builder/core.py
import sqlite3
import time
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Tuple
from rich import print

from src.db.db_helpers import get_db_session
from src.db.models import DpdHeadword, Lookup
from src.tools.paths import ProjectPaths
from src.tools.text_scanner import get_ebts_word_set
from src.tools.deconstructed_words import make_words_in_deconstructions
from src.tools.pali_sort_key import pali_sort_key

from .config import BuilderConfig
from .renderer import DpdRenderer

# --- WORKER FUNCTION (Must be top-level for pickling) ---
def process_batch(ids: List[int], config: BuilderConfig) -> Tuple[List, List]:
    """
    H√†m n√†y ch·∫°y trong m·ªôt Process ri√™ng bi·ªát.
    N√≥ t·ª± t·∫°o connection DB ri√™ng ƒë·ªÉ tr√°nh l·ªói thread-safety c·ªßa SQLAlchemy.
    """
    # Kh·ªüi t·∫°o renderer v√† db session c·ª•c b·ªô cho process n√†y
    renderer = DpdRenderer(config)
    session = get_db_session(config.DPD_DB_PATH)
    
    entries_data = []
    lookups_data = []
    
    try:
        # Query m·ªôt l·∫ßn l·∫•y h·∫øt c√°c items trong batch ƒë·ªÉ t·ªëi ∆∞u
        headwords = session.query(DpdHeadword).filter(DpdHeadword.id.in_(ids)).all()
        
        # Sort l·∫°i theo th·ª© t·ª± ID ƒë·∫ßu v√†o (n·∫øu c·∫ßn, ho·∫∑c sort sau)
        # ·ªû ƒë√¢y ta kh√¥ng quan tr·ªçng th·ª© t·ª± x·ª≠ l√Ω, ch·ªâ quan tr·ªçng k·∫øt qu·∫£
        
        for i in headwords:
            # 1. Render HTML
            grammar = renderer.render_grammar(i)
            examples = renderer.render_examples(i)
            # [UPDATED] Kh√¥ng truy·ªÅn grammar/example v√†o render_entry n·ªØa
            definition = renderer.render_entry(i)
            data_json = renderer.extract_json_data(i)
            
            # 2. Prepare Data Tuple
            entries_data.append((
                i.id,
                i.lemma_1,
                i.lemma_clean,
                definition,
                grammar,
                examples,
                data_json
            ))
            
            # 3. Lookups
            lookups_data.append((i.lemma_clean, i.id, 'entry', 0))
            for inf in i.inflections_list_all:
                if inf:
                    lookups_data.append((inf, i.id, 'entry', 1))
                    
    except Exception as e:
        print(f"[red]Error in worker process: {e}")
    finally:
        session.close()
        
    return entries_data, lookups_data

# --- MAIN CLASS ---
class DictBuilder:
    def __init__(self, mode: str = "mini"):
        self.config = BuilderConfig(mode=mode)
        self.pth = ProjectPaths()
        # Renderer ·ªü ƒë√¢y ch·ªâ d√πng cho c√°c t√°c v·ª• ƒë∆°n l·∫ª (deconstructions)
        self.renderer = DpdRenderer(self.config)
        
    def _init_db(self):
        """Kh·ªüi t·∫°o file SQLite v√† n·∫°p Schema."""
        if not self.config.OUTPUT_DIR.exists():
            self.config.OUTPUT_DIR.mkdir(parents=True)
            
        if self.config.output_path.exists():
            self.config.output_path.unlink()
            
        self.conn = sqlite3.connect(self.config.output_path)
        self.cursor = self.conn.cursor()
        
        # T·∫Øt journal mode ho·∫∑c d√πng WAL ƒë·ªÉ insert nhanh h∆°n
        self.cursor.execute("PRAGMA synchronous = OFF")
        self.cursor.execute("PRAGMA journal_mode = MEMORY")
        
        schema_path = self.config.TEMPLATES_DIR.parent / "schema.sql"
        with open(schema_path, "r", encoding="utf-8") as f:
            self.cursor.executescript(f.read())
            
        self.cursor.execute("INSERT INTO metadata (key, value) VALUES (?, ?)", 
                            ("version", datetime.now().strftime("%Y-%m-%d")))
        self.cursor.execute("INSERT INTO metadata (key, value) VALUES (?, ?)", 
                            ("mode", self.config.mode))
        self.conn.commit()

    def _get_target_ids(self, session) -> List[int]:
        """L·∫•y danh s√°ch ID c·∫ßn x·ª≠ l√Ω (nhanh h∆°n l·∫•y full object)."""
        print(f"[green]Scanning DPD DB (Mode: {self.config.mode})...")
        
        # L·∫•y t·∫•t c·∫£ (ID, Lemma, Inflections) ƒë·ªÉ l·ªçc nhanh
        # Ch·ªâ l·∫•y c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øt ki·ªám RAM
        query = session.query(DpdHeadword.id, DpdHeadword.lemma_1, DpdHeadword.inflections, DpdHeadword.inflections_api_ca_eva_iti)
        
        if self.config.is_full_mode:
            print("[green]Full Mode: Selecting all IDs.")
            return [row.id for row in query.all()]

        # --- MINI MODE ---
        print("[yellow]Calculating EBTS word set...")
        bilara_path = self.config.PROJECT_ROOT / "data/bilara/root/pli/ms"
        if not bilara_path.exists():
            print(f"[red]Bilara data missing at {bilara_path}")
            return []

        target_set = get_ebts_word_set(bilara_path, self.config.EBTS_BOOKS)
        decon_set = make_words_in_deconstructions(session)
        target_set = target_set | decon_set
        
        print(f"[green]Target words: {len(target_set)}")
        
        target_ids = []
        for row in query.all():
            # T√°i t·∫°o logic lemma_clean
            lemma_clean = row.lemma_1.split(" ")[0] # basic clean
            
            # Logic check nhanh
            if lemma_clean in target_set:
                target_ids.append(row.id)
                continue
            
            # Check inflections
            infs = []
            if row.inflections: infs.extend(row.inflections.split(","))
            if row.inflections_api_ca_eva_iti: infs.extend(row.inflections_api_ca_eva_iti.split(","))
            
            if not set(infs).isdisjoint(target_set):
                target_ids.append(row.id)
                
        print(f"[green]Filtered down to {len(target_ids)} entries.")
        return target_ids

    def run(self):
        start_time = time.time()
        print(f"üöÄ Starting Multi-process Dictionary Builder...")
        self._init_db()
        
        session = get_db_session(self.config.DPD_DB_PATH)
        
        # 1. L·∫•y danh s√°ch ID c·∫ßn x·ª≠ l√Ω
        target_ids = self._get_target_ids(session)
        if not target_ids:
            return

        # 2. Chia nh·ªè th√†nh batches (Chunks)
        BATCH_SIZE = 1000
        chunks = [target_ids[i:i + BATCH_SIZE] for i in range(0, len(target_ids), BATCH_SIZE)]
        print(f"[green]Processing {len(target_ids)} items in {len(chunks)} chunks...")

        # 3. Ch·∫°y Multi-processing
        # S·ª≠ d·ª•ng s·ªë core CPU t·ªëi ƒëa
        processed_count = 0
        
        with ProcessPoolExecutor() as executor:
            # Submit t·∫•t c·∫£ tasks
            futures = [executor.submit(process_batch, chunk, self.config) for chunk in chunks]
            
            for future in as_completed(futures):
                entries, lookups = future.result()
                
                # Ghi v√†o DB ngay khi c√≥ k·∫øt qu·∫£
                if entries:
                    self.cursor.executemany(
                        "INSERT INTO entries (id, headword, headword_clean, definition_html, grammar_html, example_html, data_json) VALUES (?,?,?,?,?,?,?)",
                        entries
                    )
                if lookups:
                    self.cursor.executemany(
                        "INSERT INTO lookups (key, target_id, target_type, is_inflection) VALUES (?,?,?,?)",
                        lookups
                    )
                
                processed_count += len(entries)
                print(f"   Saved batch... ({processed_count}/{len(target_ids)})", end="\r")
        
        print(f"\n[green]Headwords processing finished in {time.time() - start_time:.2f}s")

        # 4. X·ª≠ l√Ω Deconstructions (Nhanh n√™n ch·∫°y ƒë∆°n lu·ªìng c≈©ng ƒë∆∞·ª£c)
        print("[green]Processing Deconstructions...")
        deconstructions = session.query(Lookup).filter(Lookup.deconstructor != "").all()
        
        decon_batch = []
        decon_lookup_batch = []
        
        for idx, d in enumerate(deconstructions, start=1):
            html = self.renderer.render_deconstruction(d)
            split_str = " + ".join(d.deconstructor_unpack_list)
            
            decon_batch.append((idx, d.lookup_key, split_str, html))
            decon_lookup_batch.append((d.lookup_key, idx, 'deconstruction', 0))
            
        self.cursor.executemany(
            "INSERT INTO deconstructions (id, lookup_key, split_string, html) VALUES (?,?,?,?)",
            decon_batch
        )
        self.cursor.executemany(
            "INSERT INTO lookups (key, target_id, target_type, is_inflection) VALUES (?,?,?,?)",
            decon_lookup_batch
        )
        
        self.conn.commit()
        
        print("[green]Indexing & Optimizing (VACUUM)...")
        self.conn.execute("VACUUM")
        
        self.conn.close()
        session.close()
        
        print(f"‚úÖ Build Complete: {self.config.output_path}")
        print(f"‚è±Ô∏è Total Time: {time.time() - start_time:.2f}s")

def run_builder():
    # C·∫ßn b·∫£o v·ªá entry point khi d√πng multiprocessing tr√™n m·ªôt s·ªë OS
    builder = DictBuilder(mode="mini")
    builder.run()