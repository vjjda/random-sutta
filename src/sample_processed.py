#!/usr/bin/env python3
# Path: src/sample_processed.py
import json
import argparse
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

# --- Configuration ---
PROJECT_ROOT = Path(__file__).parent.parent
DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
TMP_DIR = PROJECT_ROOT / "tmp"

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger("Sampler")

def load_json(path: Path) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"‚ùå Failed to load {path.name}: {e}")
        return None

def find_book_file(book_id: str) -> Optional[Path]:
    """T√¨m file json trong data/processed kh·ªõp v·ªõi book_id."""
    # Pattern: {book_id}_book.json ho·∫∑c c√°c bi·∫øn th·ªÉ
    # ∆Øu ti√™n t√¨m ch√≠nh x√°c
    exact_path = DATA_PROCESSED / f"{book_id}_book.json"
    if exact_path.exists():
        return exact_path
    
    # T√¨m ki·∫øm r·ªông h∆°n n·∫øu c·∫ßn (v√≠ d·ª• file t√™n vinaya_pli-tv-bi-pm_book.json)
    for f in DATA_PROCESSED.glob(f"*{book_id}*_book.json"):
        return f
    
    return None

def infer_book_id(sutta_id: str, available_books: List[str]) -> Optional[str]:
    """ƒêo√°n book_id d·ª±a tr√™n ti·ªÅn t·ªë c·ªßa sutta_id."""
    sutta_id_lower = sutta_id.lower()
    # S·∫Øp x·∫øp available books theo ƒë·ªô d√†i gi·∫£m d·∫ßn ƒë·ªÉ match ch√≠nh x√°c nh·∫•t
    # V√≠ d·ª•: match 'pli-tv-bi-vb' tr∆∞·ªõc 'pli-tv'
    sorted_books = sorted(available_books, key=len, reverse=True)
    
    for bid in sorted_books:
        if sutta_id_lower.startswith(bid.lower()):
            return bid
    return None

# --- Logic C·∫Øt T·ªâa (Pruning) ---

def prune_structure(node: Any, target_uid: str, found_target: bool = False) -> Optional[Any]:
    """
    ƒê·ªá quy c·∫Øt t·ªâa c√¢y c·∫•u tr√∫c.
    - Gi·ªØ l·∫°i ƒë∆∞·ªùng d·∫´n cha -> con ƒë·∫øn target_uid.
    - Gi·ªØ l·∫°i to√†n b·ªô c√¢y con ch√°u c·ªßa target_uid.
    """
    # 1. N·∫øu node hi·ªán t·∫°i ch√≠nh l√† target (Leaf string ho·∫∑c Key trong Dict)
    # Logic n√†y ƒë∆∞·ª£c x·ª≠ l√Ω b√™n trong c√°c block if/else b√™n d∆∞·ªõi
    
    if isinstance(node, str):
        # Leaf node
        if node == target_uid or found_target:
            return node
        return None

    if isinstance(node, list):
        # List node (th∆∞·ªùng l√† danh s√°ch con c·ªßa m·ªôt branch)
        new_list = []
        for item in node:
            # N·∫øu ƒë√£ t√¨m th·∫•y target ·ªü c·∫•p cao h∆°n, gi·ªØ l·∫°i to√†n b·ªô con ch√°u
            if found_target:
                new_list.append(item)
            else:
                # N·∫øu ch∆∞a, ti·∫øp t·ª•c t√¨m ki·∫øm
                res = prune_structure(item, target_uid, found_target)
                if res:
                    new_list.append(res)
        return new_list if new_list else None

    if isinstance(node, dict):
        new_dict = {}
        for key, value in node.items():
            # Case A: Key ch√≠nh l√† target (Branch n√†y l√† c√°i ta c·∫ßn t√¨m)
            if key == target_uid:
                # Gi·ªØ l·∫°i key n√†y v√† TO√ÄN B·ªò n·ªôi dung b√™n trong n√≥ (found_target=True)
                # L∆∞u √Ω: Ta v·∫´n g·ªçi ƒë·ªá quy ƒë·ªÉ copy structure, nh∆∞ng c·ªù True s·∫Ω k√≠ch ho·∫°t vi·ªác copy h·∫øt.
                # Ho·∫∑c ƒë∆°n gi·∫£n l√† return c·∫£ dict n√†y n·∫øu c·∫•u tr√∫c ƒë∆°n gi·∫£n.
                # ƒê·ªÉ an to√†n v√† nh·∫•t qu√°n, ta t√°i t·∫°o l·∫°i dict.
                new_dict[key] = value # L·∫•y nguy√™n kh·ªëi
                # (N·∫øu mu·ªën filter s√¢u h∆°n trong con ch√°u th√¨ ph·∫£i ƒë·ªá quy, nh∆∞ng y√™u c·∫ßu l√† l·∫•y h·∫øt con ch√°u)
                return new_dict

            # Case B: ƒê√£ t√¨m th·∫•y ·ªü tr√™n, ƒëang copy xu·ªëng d∆∞·ªõi
            if found_target:
                new_dict[key] = value
                continue

            # Case C: Ch∆∞a t√¨m th·∫•y, ƒëi s√¢u v√†o t√¨m
            res = prune_structure(value, target_uid, found_target)
            if res:
                new_dict[key] = res
        
        return new_dict if new_dict else None

    return None

def extract_flat_ids(node: Any) -> List[str]:
    """L·∫•y danh s√°ch t·∫•t c·∫£ c√°c ID c√≥ trong structure ƒë√£ c·∫Øt t·ªâa ƒë·ªÉ l·ªçc meta/content."""
    ids = []
    if isinstance(node, str):
        ids.append(node)
    elif isinstance(node, list):
        for item in node:
            ids.extend(extract_flat_ids(item))
    elif isinstance(node, dict):
        for k, v in node.items():
            ids.append(k)
            ids.extend(extract_flat_ids(v))
    return ids

# --- Main Processor ---

def process_request(sutta_id: str, explicit_books: Optional[List[str]]) -> None:
    logger.info(f"üîé Analyzing request for: {sutta_id}")
    
    # 1. [FIX] D√πng rglob ƒë·ªÉ qu√©t ƒë·ªá quy v√†o c√°c th∆∞ m·ª•c con (sutta/, vinaya/...)
    all_book_files = list(DATA_PROCESSED.rglob("*_book.json"))
    
    # Map: book_id -> file_path
    book_map = {}
    for f in all_book_files:
        # L·∫•y t√™n file g·ªëc: "an_book.json" -> "an"
        # "pli-tv-bi-pm_book.json" -> "pli-tv-bi-pm"
        # Logic c≈© d√πng split('_')[-1] l√† r·ªßi ro n·∫øu t√™n s√°ch c√≥ d·∫•u g·∫°ch d∆∞·ªõi (d√π hi·ªán t·∫°i bilara d√πng g·∫°ch ngang)
        # Logic m·ªõi: Ch·ªâ c·∫ßn b·ªè ƒëu√¥i "_book.json" l√† ra ID
        b_id = f.name.replace("_book.json", "")
        book_map[b_id] = f

    if not book_map:
        logger.error(f"‚ùå No processed books found in {DATA_PROCESSED}. Did you run the processor?")
        return
    
    # 2. X√°c ƒë·ªãnh Book ID
    target_book_id = None
    target_file = None

    # C√°ch A: Check trong explicit books
    if explicit_books:
        for b in explicit_books:
            if b in book_map:
                # Ki·ªÉm tra s∆° b·ªô xem sutta c√≥ v·∫ª thu·ªôc book n√†y kh√¥ng (optional)
                target_book_id = b
                target_file = book_map[b]
                break
        if not target_file:
            logger.warning(f"   ‚ö†Ô∏è Could not find provided books {explicit_books} in processed data.")

    # C√°ch B: T·ª± suy di·ªÖn
    if not target_file:
        inferred = infer_book_id(sutta_id, list(book_map.keys()))
        if inferred:
            target_book_id = inferred
            target_file = book_map[inferred]
            logger.info(f"   ‚ú® Inferred book: {target_book_id}")
        else:
            logger.error(f"   ‚ùå Could not infer book_id for '{sutta_id}'. Please specify with -b.")
            return

    # 3. Load Book Data
    logger.info(f"   üìñ Loading book: {target_file.name}")
    book_data = load_json(target_file)
    if not book_data:
        return

    # 4. Extract Structure (Pruning)
    raw_structure = book_data.get("structure", {})
    pruned_structure = prune_structure(raw_structure, sutta_id)

    if not pruned_structure:
        logger.error(f"   ‚ùå Sutta ID '{sutta_id}' not found in structure of {target_book_id}.")
        return

    # 5. Extract Meta & Content
    valid_ids = set(extract_flat_ids(pruned_structure))
    
    # L·∫•y lu√¥n c·∫£ sutta_id g·ªëc ph√≤ng tr∆∞·ªùng h·ª£p n√≥ l√† l√° v√† prune_structure tr·∫£ v·ªÅ string
    valid_ids.add(sutta_id)

    raw_meta = book_data.get("meta", {})
    pruned_meta = {k: v for k, v in raw_meta.items() if k in valid_ids}

    raw_content = book_data.get("content", {})
    
    # Content th∆∞·ªùng ch·ªâ c√≥ ·ªü level Leaf. 
    # N·∫øu sutta_id l√† Branch, ta c·∫ßn l·∫•y content c·ªßa c√°c con ch√°u.
    pruned_content = {k: v for k, v in raw_content.items() if k in valid_ids}

    # 6. Output Generation
    output_data = {
        "source_book": target_book_id,
        "root_sutta": sutta_id,
        "structure": pruned_structure,
        "meta": pruned_meta,
        "content": pruned_content
    }

    TMP_DIR.mkdir(exist_ok=True)
    out_filename = f"{sutta_id}-in-{target_book_id}_context.txt"
    out_path = TMP_DIR / out_filename

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(f"SAMPLE EXTRACT FOR: {sutta_id}\n")
        f.write(f"SOURCE BOOK: {target_book_id}\n")
        f.write("="*60 + "\n\n")
        f.write(json.dumps(output_data, indent=2, ensure_ascii=False))
    
    logger.info(f"   ‚úÖ Sample extracted to: {out_path}")


def main():
    parser = argparse.ArgumentParser(description="Extract sample context from processed books.")
    parser.add_argument("suttas", nargs='+', help="List of Sutta IDs to extract (e.g. mn1 an1.1)")
    parser.add_argument("-b", "--books", nargs='*', help="Optional list of Book IDs to search in")
    
    args = parser.parse_args()

    if not DATA_PROCESSED.exists():
        logger.error(f"‚ùå Processed data directory not found at {DATA_PROCESSED}")
        sys.exit(1)

    for sid in args.suttas:
        process_request(sid, args.books)

if __name__ == "__main__":
    main()