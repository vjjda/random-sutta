#!/usr/bin/env python3
# Path: src/sutta_fetcher.py
import logging
import shutil
import subprocess
import sys
import os
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Configuration ---
REPO_URL = "https://github.com/suttacentral/sc-data.git"
CACHE_DIR = Path(".cache/sc_bilara_data")
PROJECT_ROOT = Path(__file__).parent.parent
DATA_ROOT = PROJECT_ROOT / "data" / "bilara"

# Branch m·ª•c ti√™u
BRANCH_NAME = "main"

# ƒê·ªãnh nghƒ©a c√°c ƒë∆∞·ªùng d·∫´n c·ª• th·ªÉ c·∫ßn l·∫•y t·ª´ Git (Sparse Checkout)
FETCH_MAPPING = {
    "sc_bilara_data/root/pli/ms": "root",
    "sc_bilara_data/html/pli/ms": "html",
    "sc_bilara_data/comment/en": "comment/en",
    "sc_bilara_data/translation/en/brahmali": "translation/en/brahmali",
    "sc_bilara_data/translation/en/kelly": "translation/en/kelly",
    "sc_bilara_data/translation/en/sujato/sutta": "translation/en/sujato/sutta",
    # M·ªöI: Th√™m th∆∞ m·ª•c tree v√†o danh s√°ch c·∫ßn fetch t·ª´ Git
    "sc_bilara_data/structure/tree": "tree",
}

# C√°c th∆∞ m·ª•c c·∫ßn lo·∫°i b·ªè (cho copy th√¥ng th∆∞·ªùng)
IGNORE_PATTERNS = {
    "root": ["xplayground"], 
}

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("SuttaFetcher")

def _run_git(cwd: Path, args: List[str]) -> None:
    """Helper ƒë·ªÉ ch·∫°y l·ªánh git an to√†n."""
    try:
        subprocess.run(
            ["git"] + args,
            cwd=cwd,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Git command failed: {' '.join(args)}\nError: {e.stderr.strip()}")

def _perform_clone():
    """Th·ª±c hi·ªán clone m·ªõi ho√†n to√†n."""
    logger.info("   üì• Cloning fresh repository...")
    if CACHE_DIR.exists():
        shutil.rmtree(CACHE_DIR)
    CACHE_DIR.parent.mkdir(parents=True, exist_ok=True)
    
    # 1. Init empty repo & add remote
    CACHE_DIR.mkdir()
    _run_git(CACHE_DIR, ["init"])
    _run_git(CACHE_DIR, ["remote", "add", "origin", REPO_URL])
    
    # 2. Configure Sparse Checkout
    _run_git(CACHE_DIR, ["config", "core.sparseCheckout", "true"])
    sparse_path = CACHE_DIR / ".git" / "info" / "sparse-checkout"
    with open(sparse_path, "w") as f:
        for path in FETCH_MAPPING.keys():
            f.write(path.strip("/") + "\n")
            
    # 3. Fetch & Reset to MAIN
    logger.info(f"   üì• Fetching {BRANCH_NAME}...")
    _run_git(CACHE_DIR, ["fetch", "--depth", "1", "origin", BRANCH_NAME])
    
    logger.info("   üî® Resetting to match remote...")
    _run_git(CACHE_DIR, ["reset", "--hard", "FETCH_HEAD"])

def _update_existing_repo():
    """C·ªë g·∫Øng update repo hi·ªán c√≥."""
    if not (CACHE_DIR / ".git").exists():
        raise RuntimeError("Invalid git repository")
        
    logger.info(f"   üîÑ Updating existing repository (Target: {BRANCH_NAME})...")
    
    # C·∫≠p nh·∫≠t sparse list
    sparse_path = CACHE_DIR / ".git" / "info" / "sparse-checkout"
    with open(sparse_path, "w") as f:
        for path in FETCH_MAPPING.keys():
            f.write(path.strip("/") + "\n")

    # Fetch ƒë√∫ng branch v√† Reset c·ª©ng
    _run_git(CACHE_DIR, ["fetch", "--depth", "1", "origin", BRANCH_NAME])
    _run_git(CACHE_DIR, ["reset", "--hard", "FETCH_HEAD"])
    _run_git(CACHE_DIR, ["clean", "-fdx"])

def _setup_repo():
    """ƒêi·ªÅu ph·ªëi vi·ªác Clone/Update v·ªõi c∆° ch·∫ø Self-Healing."""
    logger.info("‚ö° Setting up data repository...")
    
    if CACHE_DIR.exists():
        try:
            _update_existing_repo()
            logger.info("‚úÖ Repository updated.")
            return
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Update failed ({e}). Re-cloning...")
    
    try:
        _perform_clone()
        logger.info("‚úÖ Repository synced successfully.")
    except Exception as e:
        logger.error(f"‚ùå Sync failed: {e}")
        raise e

def _clean_destination():
    if DATA_ROOT.exists():
        logger.info("üßπ Cleaning old data...")
        shutil.rmtree(DATA_ROOT)
    DATA_ROOT.mkdir(parents=True, exist_ok=True)

# --- Logic m·ªõi cho Smart Tree Copy ---

def _get_installed_books() -> Set[str]:
    """Qu√©t th∆∞ m·ª•c data/bilara/root ƒë·ªÉ l·∫•y danh s√°ch c√°c s√°ch (mn, dn, dhp...) ƒëang c√≥."""
    root_dir = DATA_ROOT / "root"
    books = set()
    
    if not root_dir.exists():
        return books

    # Qu√©t ƒë·ªá quy c·∫•p 1 v√† c·∫•p 2 (cho tr∆∞·ªùng h·ª£p kn/dhp)
    for item in root_dir.rglob("*"):
        if item.is_dir():
            # T√™n s√°ch ch√≠nh l√† t√™n th∆∞ m·ª•c (v√≠ d·ª•: mn, dn, dhp, pli-tv-bi-vb)
            # Lo·∫°i b·ªè c√°c th∆∞ m·ª•c cha nh∆∞ 'sutta', 'vinaya', 'kn' n·∫øu ch√∫ng kh√¥ng ch·ª©a file json tr·ª±c ti·∫øp
            # Tuy nhi√™n, c√°ch ƒë∆°n gi·∫£n nh·∫•t l√† l·∫•y T·∫§T C·∫¢ t√™n th∆∞ m·ª•c, th·ª´a c√≤n h∆°n thi·∫øu
            books.add(item.name)
            
    return books

def _smart_copy_tree(src_path: Path, dest_path: Path) -> str:
    """
    Ch·ªâ copy super-tree.json v√† c√°c *-tree.json t∆∞∆°ng ·ª©ng v·ªõi s√°ch ƒë√£ t·∫£i.
    Gi·ªØ nguy√™n c·∫•u tr√∫c th∆∞ m·ª•c g·ªëc c·ªßa tree (kh√¥ng sort l·∫°i v√†o kn).
    """
    valid_books = _get_installed_books()
    logger.info(f"   ‚ÑπÔ∏è  Smart Tree Copy: Found {len(valid_books)} installed books to filter trees.")

    copied_count = 0
    
    # Duy·ªát qua source tree trong cache
    for root, dirs, files in os.walk(src_path):
        for file in files:
            # 1. Lu√¥n l·∫•y super-tree.json
            if file == "super-tree.json":
                should_copy = True
            
            # 2. L·ªçc file *-tree.json
            elif file.endswith("-tree.json"):
                # T√°ch book_id t·ª´ filename: "mn-tree.json" -> "mn"
                book_id = file.replace("-tree.json", "")
                should_copy = book_id in valid_books
            else:
                should_copy = False

            if should_copy:
                # T√≠nh ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªÉ gi·ªØ c·∫•u tr√∫c (v√≠ d·ª•: sutta/mn-tree.json)
                abs_src = Path(root) / file
                rel_path = abs_src.relative_to(src_path)
                abs_dest = dest_path / rel_path
                
                # T·∫°o th∆∞ m·ª•c cha n·∫øu ch∆∞a c√≥
                abs_dest.parent.mkdir(parents=True, exist_ok=True)
                
                shutil.copy2(abs_src, abs_dest)
                copied_count += 1

    return f"   -> Copied: tree ({copied_count} files filtered by installed books)"

# -------------------------------------

def _copy_worker(task: Tuple[str, str]) -> str:
    """Worker function ƒë·ªÉ copy m·ªôt th∆∞ m·ª•c c·ª• th·ªÉ."""
    src_rel, dest_rel = task
    src_path = CACHE_DIR / src_rel
    dest_path = DATA_ROOT / dest_rel
    
    if not src_path.exists():
        return f"‚ö†Ô∏è Source not found (skipped): {src_rel}"

    # ROUTING ƒê·∫∂C BI·ªÜT CHO TREE
    if dest_rel == "tree":
        # C·∫ßn x√≥a ƒë√≠ch tr∆∞·ªõc n·∫øu c√≥ ƒë·ªÉ ƒë·∫£m b·∫£o s·∫°ch
        if dest_path.exists():
            shutil.rmtree(dest_path)
        return _smart_copy_tree(src_path, dest_path)

    # Logic copy th√¥ng th∆∞·ªùng cho c√°c folder kh√°c
    ignore_list = []
    for key, patterns in IGNORE_PATTERNS.items():
        if dest_rel.startswith(key):
            ignore_list.extend(patterns)
    ignore_func = shutil.ignore_patterns(*ignore_list) if ignore_list else None
    
    dest_path.parent.mkdir(parents=True, exist_ok=True)
    
    if dest_path.exists():
        shutil.rmtree(dest_path)
        
    shutil.copytree(src_path, dest_path, ignore=ignore_func)
    return f"   -> Copied: {dest_rel}"

def _copy_data():
    """Copy d·ªØ li·ªáu song song s·ª≠ d·ª•ng ThreadPoolExecutor."""
    logger.info("üìÇ Copying and filtering data (Multi-threaded)...")
    
    workers = min(os.cpu_count() or 4, len(FETCH_MAPPING))
    
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(_copy_worker, item): item 
            for item in FETCH_MAPPING.items()
        }
        
        for future in as_completed(futures):
            try:
                result = future.result()
                logger.info(result)
            except Exception as e:
                logger.error(f"‚ùå Error copying: {e}")

    logger.info(f"‚úÖ Data copied to {DATA_ROOT}")

def orchestrate_fetch():
    try:
        _setup_repo()
        _clean_destination()
        _copy_data()
        logger.info("‚ú® Sutta Data Fetch completed successfully.")
    except Exception as e:
        logger.error(f"‚ùå Critical Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    orchestrate_fetch()