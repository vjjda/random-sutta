# Path: src/data_fetcher/api/client.py
import json
import urllib.request
import urllib.error
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Tuple

from src.logging_config import setup_logging
from ..fetcher_config import ApiConfig, BilaraConfig

logger = setup_logging("DataFetcher.API")

class MetadataClient:
    def __init__(self):
        self.priority_map = {item: i for i, item in enumerate(ApiConfig.PRIORITY_ORDER)}

    def discover_books(self) -> List[Tuple[str, str]]:
        """
        QuÃ©t thÆ° má»¥c dá»±a trÃªn DISCOVERY_RULES Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong config.
        """
        root_dir = BilaraConfig.ROOT_TEXT_DIR
        
        if not root_dir.exists():
            logger.error(f"âŒ Root text data not found at {root_dir}.")
            logger.error("   ğŸ‘‰ Please run 'make sync-text' or 'python -m src.data_fetcher -s' first.")
            return []

        discovered: List[Tuple[str, str]] = []
        logger.info(f"   ğŸ” Scanning Book IDs in {root_dir.name}...")

        # 1. Rule-based Discovery
        for rule in ApiConfig.DISCOVERY_RULES:
            scan_path = root_dir / rule["path"]
            category = rule["category"]
            exclude_set = rule["exclude"]

            if not scan_path.exists():
                logger.debug(f"   âš ï¸ Path not found (skipped): {rule['path']}")
                continue

            # Chá»‰ láº¥y cÃ¡c folder con trá»±c tiáº¿p (Immediate subdirectories)
            # ÄÃ¢y lÃ  Book ID (vÃ­ dá»¥: dn, mn, sn...)
            count = 0
            for item in scan_path.iterdir():
                if item.is_dir():
                    book_id = item.name
                    # Bá» qua folder há»‡ thá»‘ng vÃ  folder náº±m trong exclude list (vÃ­ dá»¥: kn)
                    if (book_id in ApiConfig.SYSTEM_IGNORE) or (book_id in exclude_set):
                        continue
                    
                    discovered.append((book_id, category))
                    count += 1
            
            logger.debug(f"   -> Scanned {rule['path']}: found {count} items.")

        # 2. Add Super Targets & Extras
        # ThÃªm cÃ¡c má»¥c lá»¥c lá»›n (sutta, vinaya...)
        for uid in ApiConfig.SUPER_TARGET_CATS:
            discovered.append((uid, "super"))
            
        # ThÃªm cÃ¡c má»¥c bá»• sung thá»§ cÃ´ng
        for uid, cat in ApiConfig.EXTRA_UIDS.items():
            discovered.append((uid, cat))

        # 3. Deduplicate & Sort
        # Loáº¡i bá» trÃ¹ng láº·p vÃ  sáº¯p xáº¿p theo Ä‘á»™ Æ°u tiÃªn
        seen = set()
        final_list = []
        
        # Priority items first
        priority_candidates = []
        normal_candidates = []

        for info in discovered:
            book_id, cat = info
            unique_key = (book_id, cat)
            
            if unique_key in seen:
                continue
            seen.add(unique_key)

            if info in self.priority_map:
                priority_candidates.append(info)
            else:
                normal_candidates.append(info)

        priority_candidates.sort(key=lambda x: self.priority_map[x])
        normal_candidates.sort(key=lambda x: x[0]) # Sort chá»¯ cÃ¡i cho pháº§n cÃ²n láº¡i

        final_list = priority_candidates + normal_candidates
        
        logger.info(f"   âœ… Discovered {len(final_list)} targets to fetch.")
        return final_list

    def fetch_book_json(self, book_info: Tuple[str, str]) -> str:
        book_id, category_path = book_info
        url = ApiConfig.API_TEMPLATE.format(book_id)
        
        category_dir = ApiConfig.DATA_JSON_DIR / category_path
        category_dir.mkdir(parents=True, exist_ok=True)
        dest_file = category_dir / f"{book_id}.json"
        
        # Check cache logic could be added here later
        
        try:
            timeout = ApiConfig.TIMEOUT_DEFAULT
            if category_path == "super": timeout = ApiConfig.TIMEOUT_SUPER
            elif book_id in ApiConfig.LARGE_BOOKS: timeout = ApiConfig.TIMEOUT_LARGE
            
            with urllib.request.urlopen(url, timeout=timeout) as response:
                if response.status != 200:
                    return f"âŒ {book_id}: HTTP {response.status}"
                
                data = json.loads(response.read().decode('utf-8'))
                with open(dest_file, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
            return f"âœ… {category_path}/{book_id}"
            
        except urllib.error.HTTPError as e:
            if e.code == 404:
                return f"âš ï¸ {category_path}/{book_id}: Not found (404)"
            return f"âŒ {category_path}/{book_id}: HTTP {e.code}"
        except Exception as e:
            return f"âŒ {category_path}/{book_id}: Error {e}"

    def run(self) -> None:
        logger.info("ğŸš€ Starting Metadata (API) Fetch...")
        
        target_books = self.discover_books()
        if not target_books:
            return

        if not ApiConfig.DATA_JSON_DIR.exists():
            ApiConfig.DATA_JSON_DIR.mkdir(parents=True)

        workers = ApiConfig.get_worker_count()
        logger.info(f"   Using {workers} threads...")

        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(self.fetch_book_json, info): info[0] 
                for info in target_books
            }
            
            for future in as_completed(futures):
                logger.info(future.result())

        logger.info("âœ¨ Metadata API Fetch completed.")

def run_api_fetch() -> None:
    client = MetadataClient()
    client.run()