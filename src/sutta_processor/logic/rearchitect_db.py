# Path: src/sutta_processor/logic/rearchitect_db.py
import json
import logging
import shutil
from pathlib import Path
from typing import Dict, Any, List, Set, Tuple

from ..shared.app_config import (
    PROCESSED_DIR, ASSETS_ROOT, PROJECT_ROOT, 
    WEB_DB_DIR, MIRROR_DB_DIR, CHUNK_SIZE_LIMIT,
    PRIMARY_BOOKS
)

logger = logging.getLogger("SuttaProcessor.Optimizer")

# --- Helper Class 1: Qu·∫£n l√Ω logic Pool & Filtering ---
class PoolManager:
    def __init__(self):
        self.primary_set = set(PRIMARY_BOOKS)
        self.sutta_books: Set[str] = set() # S·∫Ω ƒë∆∞·ª£c populate t·ª´ super_struct
        
        # Structure: { "primary": [...], "books": { "mn": [...] } }
        self.pools: Dict[str, Any] = {
            "primary": [],
            "books": {}
        }

    def register_sutta_books_from_super(self, super_data: Dict[str, Any]):
        """
        Duy·ªát c√¢y super_struct ƒë·ªÉ t√¨m t·∫•t c·∫£ s√°ch thu·ªôc nh√°nh 'sutta'.
        ƒê·ªÉ sau n√†y t√≠nh Secondary Books = All Sutta - Primary.
        """
        def _extract_leaves(node: Any, collection: Set[str]):
            if isinstance(node, str):
                collection.add(node)
            elif isinstance(node, list):
                for item in node:
                    _extract_leaves(item, collection)
            elif isinstance(node, dict):
                for key, value in node.items():
                    _extract_leaves(value, collection)

        # Ch·ªâ qu√©t nh√°nh 'sutta'
        structure = super_data.get("structure", [])
        for item in structure:
            if "sutta" in item:
                _extract_leaves(item["sutta"], self.sutta_books)
                break

    def build_smart_pool(self, raw_content: Dict[str, Any], meta_map: Dict[str, Any]) -> List[str]:
        """
        Logic l·ªçc UID th√¥ng minh:
        1. Leaf (c√≥ content) -> OK.
        2. Shortcut -> OK n·∫øu c√≥ scroll_target != null.
        3. N·∫øu Shortcut OK -> Blacklist Parent c·ªßa n√≥.
        """
        candidates = set()
        blacklist_parents = set()

        # 1. Add Leaves (UIDs c√≥ content th·ª±c)
        for uid in raw_content.keys():
            candidates.add(uid)

        # 2. Check Shortcuts
        for uid, info in meta_map.items():
            if info.get("type") == "shortcut":
                parent_uid = info.get("parent_uid")
                scroll_target = info.get("scroll_target")
                
                # Rule 4: B·ªè qua shortcut n·∫øu target null (v√≠ d·ª• sn56.100)
                if not scroll_target:
                    continue

                # Rule 2: Shortcut h·ª£p l·ªá -> Add v√†o candidate
                candidates.add(uid)
                
                # Rule 3: N·∫øu ƒë√£ d√πng shortcut, lo·∫°i b·ªè parent kh·ªèi pool
                if parent_uid:
                    blacklist_parents.add(parent_uid)

        # 3. Finalize
        final_pool = []
        for uid in candidates:
            if uid not in blacklist_parents:
                final_pool.append(uid)
        
        return sorted(final_pool)

    def add_to_pools(self, book_id: str, uids: List[str]):
        """Ph√¢n lo·∫°i UIDs v√†o c√°c pool t∆∞∆°ng ·ª©ng."""
        if not book_id: return

        # A. Book Pool (Lu√¥n c√≥)
        if book_id not in self.pools["books"]:
            self.pools["books"][book_id] = []
        self.pools["books"][book_id].extend(uids)

        # B. Primary Pool
        if book_id in self.primary_set:
            self.pools["primary"].extend(uids)

    def get_secondary_books(self) -> List[str]:
        """T√≠nh to√°n danh s√°ch s√°ch ph·ª• (Secondary)."""
        # Secondary = All Sutta - Primary
        secondary = list(self.sutta_books - self.primary_set)
        return sorted(secondary)


# --- Helper Class 2: Sinh file Constants.js ---
class JsConfigGenerator:
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir

    def generate(self, secondary_books: List[str]):
        """T·∫°o file constants.js ch·ª©a danh s√°ch Primary v√† Secondary."""
        file_path = self.output_dir / "data" / "constants.js"
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
        file_path.parent.mkdir(parents=True, exist_ok=True)

        js_content = (
            "// Path: web/assets/modules/data/constants.js\n"
            "// Auto-generated by SuttaProcessor (rearchitect_db.py)\n\n"
            f"export const PRIMARY_BOOKS = {json.dumps(PRIMARY_BOOKS, indent=2)};\n\n"
            f"export const SECONDARY_BOOKS = {json.dumps(secondary_books, indent=2)};\n"
        )

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(js_content)
            logger.info(f"   ‚ú® Generated JS Constants: {file_path.name}")
        except Exception as e:
            logger.error(f"‚ùå Failed to generate constants.js: {e}")


# --- Main Controller ---
class DBOptimizer:
    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.locator: Dict[str, str] = {} 
        
        # Modules con
        self.pool_manager = PoolManager()
        # JS Generator ghi th·∫≥ng v√†o source code web/assets/modules
        self.js_gen = JsConfigGenerator(ASSETS_ROOT / "modules") 
        
    def _setup_directories(self):
        if MIRROR_DB_DIR.exists():
            shutil.rmtree(MIRROR_DB_DIR)
        MIRROR_DB_DIR.mkdir(parents=True)
        (MIRROR_DB_DIR / "structure").mkdir()
        (MIRROR_DB_DIR / "content").mkdir()

        if not self.dry_run:
            if WEB_DB_DIR.exists():
                shutil.rmtree(WEB_DB_DIR)
            WEB_DB_DIR.mkdir(parents=True)
            (WEB_DB_DIR / "structure").mkdir()
            (WEB_DB_DIR / "content").mkdir()
        else:
            logger.info("   üß™ Dry-run: Skipping Web DB write")

    def _get_safe_name(self, relative_path: Path) -> str:
        name = relative_path.name.replace("_book.json", "").replace(".json", "")
        parts = list(relative_path.parent.parts)
        parts.append(name)
        return "_".join(parts)

    def _save_dual(self, relative_path: str, data: Any):
        mirror_path = MIRROR_DB_DIR / relative_path
        try:
            with open(mirror_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Failed to write Mirror file {relative_path}: {e}")

        if not self.dry_run:
            web_path = WEB_DB_DIR / relative_path
            try:
                with open(web_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, separators=(',', ':'))
            except Exception as e:
                logger.error(f"‚ùå Failed to write Web file {relative_path}: {e}")

    def _process_content_chunks(self, safe_name: str, content: Dict[str, Any]) -> None:
        chunk_idx = 1
        current_chunk: Dict[str, Any] = {}
        current_size = 0
        sorted_keys = sorted(content.keys()) 
        
        for uid in sorted_keys:
            item_data = content[uid]
            item_str = json.dumps(item_data, ensure_ascii=False, separators=(',', ':'))
            item_size = len(item_str.encode('utf-8'))
            
            if current_size + item_size > CHUNK_SIZE_LIMIT and current_chunk:
                chunk_filename = f"{safe_name}_chunk_{chunk_idx}.json"
                self._save_dual(f"content/{chunk_filename}", current_chunk)
                for saved_uid in current_chunk.keys():
                    self.locator[saved_uid] = chunk_filename.replace(".json", "")
                
                chunk_idx += 1
                current_chunk = {}
                current_size = 0
            
            current_chunk[uid] = item_data
            current_size += item_size
            
        if current_chunk:
            chunk_filename = f"{safe_name}_chunk_{chunk_idx}.json"
            self._save_dual(f"content/{chunk_filename}", current_chunk)
            for saved_uid in current_chunk.keys():
                self.locator[saved_uid] = chunk_filename.replace(".json", "")

    def _process_book_file(self, file_path: Path):
        try:
            rel_path = file_path.relative_to(PROCESSED_DIR)
            safe_name = self._get_safe_name(rel_path)
            
            logger.info(f"   üî® Processing: {safe_name}...")
            
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                
            # 1. Save Structure
            struct_data = {
                "id": data.get("id"),
                "title": data.get("title"),
                "structure": data.get("structure", {}),
                "meta": data.get("meta", {})
            }
            self._save_dual(f"structure/{safe_name}_struct.json", struct_data)
            
            # 2. Process Content & Pools
            raw_content = data.get("content", {})
            meta_map = data.get("meta", {})
            
            if raw_content:
                # A. Chunking
                self._process_content_chunks(safe_name, raw_content)
                
                # B. Map Shortcuts (tr∆∞·ªõc khi build pool ƒë·ªÉ locator chu·∫©n)
                for uid, info in meta_map.items():
                    if info.get("type") == "shortcut":
                        parent_uid = info.get("parent_uid")
                        if parent_uid and parent_uid in self.locator:
                            self.locator[uid] = self.locator[parent_uid]

                # C. Build Smart Pool
                book_id = data.get("id", "").lower()
                valid_uids = self.pool_manager.build_smart_pool(raw_content, meta_map)
                
                # Add to managed pools
                self.pool_manager.add_to_pools(book_id, valid_uids)

        except Exception as e:
            logger.error(f"‚ùå Failed to process {file_path.name}: {e}")

    def run(self):
        mode_str = "DRY-RUN (Mirror Only)" if self.dry_run else "PRODUCTION (Dual Write)"
        logger.info(f"üöÄ Starting Database Optimization: {mode_str}")
        self._setup_directories()
        
        all_files = sorted(list(PROCESSED_DIR.rglob("*.json")))
        
        # B∆∞·ªõc 1: T√¨m v√† x·ª≠ l√Ω super_book tr∆∞·ªõc ƒë·ªÉ l·∫•y danh s√°ch s√°ch cho secondary
        for f in all_files:
            if f.name == "super_book.json":
                try:
                    with open(f, "r", encoding="utf-8") as sf:
                        sdata = json.load(sf)
                    self._save_dual("structure/super_struct.json", sdata)
                    self.pool_manager.register_sutta_books_from_super(sdata)
                    logger.info("   üåü Processed Super Book & Registered Suttas")
                except Exception as e:
                    logger.error(f"‚ùå Error super_book: {e}")
                break # Xong super th√¨ break ƒë·ªÉ loop sau x·ª≠ l√Ω s√°ch th∆∞·ªùng

        # B∆∞·ªõc 2: X·ª≠ l√Ω s√°ch th∆∞·ªùng
        for f in all_files:
            if f.name == "super_book.json": continue
            self._process_book_file(f)
            
        # B∆∞·ªõc 3: Ghi Master Index
        master_index = {
            "pools": self.pool_manager.pools,
            "locator": self.locator
        }
        self._save_dual("uid_index.json", master_index)
        
        # B∆∞·ªõc 4: Generate JS Constants (Ch·ªâ l√†m ·ªü PROD mode ho·∫∑c n·∫øu mu·ªën test logic gen)
        if not self.dry_run:
            secondary = self.pool_manager.get_secondary_books()
            self.js_gen.generate(secondary)
        
        logger.info(f"‚úÖ Database Re-architected!")
        logger.info(f"   üëâ Mirror DB: {MIRROR_DB_DIR}")
        if not self.dry_run:
            logger.info(f"   üëâ Web DB: {WEB_DB_DIR}")

def run_optimizer(dry_run: bool = False):
    optimizer = DBOptimizer(dry_run)
    optimizer.run()