# Path: src/sutta_processor/build_manager.py
import logging
import os
import shutil
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Any

from .shared.app_config import OUTPUT_DB_DIR, PROCESSED_DIR
from .ingestion.metadata_parser import load_names_map
from .ingestion.file_crawler import generate_book_tasks
from .logic.content_merger import process_worker
from .logic.structure_handler import build_book_data
from .logic.super_generator import generate_super_book_data
from .output.asset_generator import write_book_file
from .logic.rearchitect_db import run_optimizer # [NEW]


logger = logging.getLogger("SuttaProcessor.BuildManager")

class BuildManager:
    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.names_map = load_names_map()
        
        self.buffers: Dict[str, Dict[str, Any]] = {}
        self.book_totals: Dict[str, int] = {}
        self.book_progress: Dict[str, int] = {}
        self.completed_files: List[str] = []
        
        # Theo d√µi ID c√°c s√°ch ƒë√£ build ƒë·ªÉ truy·ªÅn cho SuperGen
        self.processed_book_ids: List[str] = [] 
        
        self.sutta_group_map: Dict[str, str] = {}

    def _prepare_environment(self) -> None:
        """
        Chu·∫©n b·ªã th∆∞ m·ª•c output.
        - Lu√¥n reset th∆∞ m·ª•c JSON (PROCESSED_DIR).
        - N·∫øu Production: Reset th√™m th∆∞ m·ª•c JS (OUTPUT_DB_DIR).
        """
        # 1. Always prepare JSON dir (Dry-run data)
        if PROCESSED_DIR.exists():
            shutil.rmtree(PROCESSED_DIR)
        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
        
        # 2. If Production, prepare JS dir
        if not self.dry_run:
            if OUTPUT_DB_DIR.exists():
                shutil.rmtree(OUTPUT_DB_DIR)
            OUTPUT_DB_DIR.mkdir(parents=True, exist_ok=True)
            mode = "üöÄ PRODUCTION (Dual Output: JSON + JS)"
        else:
            mode = "üß™ DRY-RUN (JSON Only)"
            
        logger.info(f"{mode} MODE INITIALIZED")

    def _handle_task_completion(self, group: str, sutta_id: str, content: Any) -> None:
        """X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ worker."""
        if content:
            self.buffers[group][sutta_id] = content
        
        self.book_progress[group] += 1
        
        # N·∫øu ƒë√£ x·ª≠ l√Ω xong to√†n b·ªô s√°ch trong nh√≥m -> Ghi file
        if self.book_progress[group] >= self.book_totals[group]:
            self._finalize_book(group)
            if group in self.buffers:
                del self.buffers[group]

    def _finalize_book(self, group: str) -> None:
        """T·ªïng h·ª£p d·ªØ li·ªáu v√† ghi file s√°ch."""
        raw_data = self.buffers.get(group, {})
        book_obj = build_book_data(group, raw_data, self.names_map)
        
        # L∆∞u l·∫°i ID s√°ch (v√≠ d·ª•: 'dn', 'mn', 'pli-tv-bi-pm') ƒë·ªÉ d√πng cho Super Book
        if book_obj and "id" in book_obj:
            self.processed_book_ids.append(book_obj["id"])

        # H√†m n√†y gi·ªù s·∫Ω t·ª± x·ª≠ l√Ω vi·ªác ghi c·∫£ JSON v√† JS (n·∫øu ko ph·∫£i dry-run)
        filename = write_book_file(group, book_obj, self.dry_run)
        
        # Ch·ªâ th√™m v√†o danh s√°ch completed n·∫øu c√≥ file JS (t·ª©c l√† return filename h·ª£p l·ªá)
        if filename and not self.dry_run:
            self.completed_files.append(filename)

    def run(self) -> None:
        self._prepare_environment()
        
        # 1. Generate Tasks
        book_tasks = generate_book_tasks(self.names_map)
        all_tasks = []
        
        for group_name, tasks in book_tasks.items():
            self.book_totals[group_name] = len(tasks)
            self.book_progress[group_name] = 0
            self.buffers[group_name] = {}
            for task in tasks:
                all_tasks.append(task)
                self.sutta_group_map[task[0]] = group_name

        # 2. Execute Workers
        workers = os.cpu_count() or 4
        logger.info(f"üöÄ Processing {len(all_tasks)} items with {workers} workers...")

        with ProcessPoolExecutor(max_workers=workers) as executor:
            futures = [executor.submit(process_worker, task) for task in all_tasks]
            
            for i, future in enumerate(as_completed(futures)):
                try:
                    res_status, res_sid, content = future.result()
                    target_group = self.sutta_group_map.get(res_sid)
                    
                    if target_group:
                        success_content = content if res_status == "success" else None
                        self._handle_task_completion(target_group, res_sid, success_content)
                except Exception as e:
                    logger.error(f"‚ùå Worker exception: {e}")

                if (i + 1) % 1000 == 0:
                    logger.info(f"   Processed {i + 1}/{len(all_tasks)} items...")

        # 3. Generate Super Book (Menu Structure)
        if self.processed_book_ids:
            super_book_data = generate_super_book_data(self.processed_book_ids)
            if super_book_data:
                # Ghi file super_book JSON (ƒë·ªÉ rearchitect d√πng)
                # L∆∞u √Ω: write_book_file b√¢y gi·ªù ch·ªâ c·∫ßn ghi JSON v√†o processed l√† ƒë·ªß
                # Kh√¥ng c·∫ßn n√≥ ghi JS ra assets/books c≈© n·ªØa (tr·ª´ khi dry_run)
                write_book_file("super", super_book_data, self.dry_run) 

        # [NEW PHASE] 4. Run Optimizer (Re-architect DB)
        if not self.dry_run:
            logger.info("‚ö° Transforming processed data to Optimized DB...")
            run_optimizer()
            
            # [NOTE] Kh√¥ng c·∫ßn g·ªçi write_loader_script c≈© n·ªØa 
            # v√¨ loader m·ªõi s·∫Ω ƒë·ªçc uid_index.json
            
        logger.info("‚úÖ All processing tasks completed.")