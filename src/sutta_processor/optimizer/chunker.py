# Path: src/sutta_processor/optimizer/chunker.py
import json
import logging
from typing import Dict, Any, List, Tuple
from .config import CHUNK_SIZE_LIMIT

logger = logging.getLogger("Optimizer.Chunker")

# Ng∆∞·ª°ng dung sai: 20%
# N·∫øu chunk cu·ªëi c√πng < 20% c·ªßa Limit (vd: < 100KB), s·∫Ω g·ªôp v√†o chunk tr∆∞·ªõc ƒë√≥.
# ƒêi·ªÅu n√†y c√≥ nghƒ©a chunk √°p ch√≥t c√≥ th·ªÉ ph√¨nh l√™n t·ªëi ƒëa 120% (600KB).
MERGE_THRESHOLD_RATIO = 0.2 
MIN_TAIL_SIZE = CHUNK_SIZE_LIMIT * MERGE_THRESHOLD_RATIO

def chunk_content(
    safe_name: str, 
    content: Dict[str, Any]
) -> List[Tuple[str, Dict[str, Any]]]:
    """
    Chia nh·ªè content th√†nh c√°c file chunks.
    [UPDATED] Merge Tiny Tail: G·ªôp chunk cu·ªëi n·∫øu qu√° nh·ªè.
    """
    chunks = []
    chunk_idx = 0
    current_chunk = {}
    current_size = 0
    
    sorted_keys = sorted(content.keys()) 
    
    # 1. Standard Chunking Loop
    for uid in sorted_keys:
        item_data = content[uid]
        
        # ∆Ø·ªõc l∆∞·ª£ng size
        item_str = json.dumps({uid: item_data}, ensure_ascii=False)
        item_size = len(item_str.encode('utf-8'))
        
        # Check Limit
        if current_size + item_size > CHUNK_SIZE_LIMIT and current_chunk:
            fname = f"{safe_name}_chunk_{chunk_idx}"
            chunks.append((fname, current_chunk))
            chunk_idx += 1
            current_chunk = {}
            current_size = 0
        
        current_chunk[uid] = item_data
        current_size += item_size
        
    # Push chunk cu·ªëi c√πng (n·∫øu c√≥)
    if current_chunk:
        fname = f"{safe_name}_chunk_{chunk_idx}"
        chunks.append((fname, current_chunk))
        
    # 2. Optimization: Merge Tiny Tail
    # Ch·ªâ merge n·∫øu c√≥ √≠t nh·∫•t 2 chunks
    if len(chunks) >= 2:
        last_name, last_data = chunks[-1]
        
        # T√≠nh k√≠ch th∆∞·ªõc th·ª±c c·ªßa chunk cu·ªëi
        last_json = json.dumps(last_data, ensure_ascii=False)
        last_bytes = len(last_json.encode('utf-8'))
        
        if last_bytes < MIN_TAIL_SIZE:
            # L·∫•y chunk √°p ch√≥t
            prev_name, prev_data = chunks[-2]
            
            # G·ªôp data
            prev_data.update(last_data)
            
            # X√≥a chunk cu·ªëi kh·ªèi danh s√°ch
            chunks.pop()
            
            logger.debug(f"   ü§è Merged tiny tail {last_name} ({last_bytes/1024:.1f}KB) into {prev_name}")

    return chunks