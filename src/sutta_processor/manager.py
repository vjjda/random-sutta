# Path: src/sutta_processor/manager.py
import json
import logging
import os
import shutil
import re
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Any
from pathlib import Path

from .config import (
    OUTPUT_SUTTA_BASE, 
    OUTPUT_SUTTA_BOOKS, 
    PROCESSED_DIR,
    PROCESS_LIMIT
)
from .finder import generate_book_tasks
from .converter import process_worker
from .name_parser import load_names_map 

logger = logging.getLogger("SuttaProcessor")

def natural_sort_key(s: str) -> List[Any]:
    return [int(text) if text.isdigit() else text.lower()
            for text in re.split(r'(\d+)', s)]

class SuttaManager:
    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.names_map = load_names_map()
        
        # Output config
        if self.dry_run:
            logger.info("üß™ RUNNING IN DRY-RUN MODE")
            self.output_base = PROCESSED_DIR
            self.json_indent = 2
        else:
            logger.info("üöÄ RUNNING IN PRODUCTION MODE")
            self.output_base = OUTPUT_SUTTA_BOOKS
            self.json_indent = None

        # State management cho Map-Reduce
        self.buffers: Dict[str, Dict[str, Any]] = {} # Ch·ª©a d·ªØ li·ªáu ƒëang x·ª≠ l√Ω: {'sutta/mn': {'mn1': ...}}
        self.book_totals: Dict[str, int] = {}        # T·ªïng s·ªë b√†i c·∫ßn x·ª≠ l√Ω m·ªói cu·ªën: {'sutta/mn': 152}
        self.book_progress: Dict[str, int] = {}      # Ti·∫øn ƒë·ªô hi·ªán t·∫°i: {'sutta/mn': 50}
        self.completed_books: List[str] = []

    def run(self):
        self._prepare_output_dir()

        # 1. PLAN: L·∫•y danh s√°ch task v√† t√≠nh to√°n t·ªïng s·ªë l∆∞·ª£ng
        book_tasks = generate_book_tasks(limit=PROCESS_LIMIT)
        
        all_tasks = []
        for group_name, tasks in book_tasks.items():
            self.book_totals[group_name] = len(tasks)
            self.book_progress[group_name] = 0
            self.buffers[group_name] = {}
            all_tasks.extend(tasks) # Flatten th√†nh 1 list duy nh·∫•t

        workers = os.cpu_count() or 4
        logger.info(f"üöÄ Processing {len(all_tasks)} items from {len(book_tasks)} books with {workers} workers...")

        # 2. EXECUTE: X·ª≠ l√Ω song song to√†n c·ª•c
        with ProcessPoolExecutor(max_workers=workers) as executor:
            # Submit t·∫•t c·∫£ tasks m·ªôt l√∫c
            futures = [executor.submit(process_worker, task) for task in all_tasks]
            
            # 3. ACCUMULATE & FLUSH: Nh·∫≠n k·∫øt qu·∫£ d·∫ßn d·∫ßn
            for i, future in enumerate(as_completed(futures)):
                group, sid, content = future.result()
                
                # B·ªè qua c√°c b√†i l·ªói/skipped
                if not content:
                    # V·∫´n ph·∫£i tƒÉng progress ƒë·ªÉ bi·∫øt l√† ƒë√£ x·ª≠ l√Ω xong (d√π fail)
                    self._update_progress_and_flush_if_ready(group)
                    continue

                # L∆∞u v√†o buffer
                self.buffers[group][sid] = content
                
                # Check xem cu·ªën s√°ch n√†y ƒë√£ ƒë·ªß ch∆∞a -> Ghi ƒëƒ©a
                self._update_progress_and_flush_if_ready(group)

                # Log ti·∫øn ƒë·ªô t·ªïng (m·ªói 500 b√†i)
                if (i + 1) % 500 == 0:
                    logger.info(f"   Processed {i + 1}/{len(all_tasks)} total items...")

        # 4. FINISH
        if not self.dry_run:
            self._write_loader(self.completed_books)
        
        logger.info(f"‚úÖ All done. Output: {self.output_base}")

    def _update_progress_and_flush_if_ready(self, group: str):
        """C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô c·ªßa m·ªôt cu·ªën s√°ch v√† ghi ƒëƒ©a n·∫øu n√≥ ƒë√£ ho√†n th√†nh."""
        self.book_progress[group] += 1
        
        # N·∫øu ƒë√£ x·ª≠ l√Ω ƒë·ªß s·ªë l∆∞·ª£ng b√†i c·ªßa s√°ch n√†y
        if self.book_progress[group] >= self.book_totals[group]:
            if self.buffers[group]: # Ch·ªâ ghi n·∫øu c√≥ d·ªØ li·ªáu (tr√°nh s√°ch r·ªóng to√†n b·ªô)
                generated_file = self._write_single_book(group, self.buffers[group])
                self.completed_books.append(generated_file)
            
            # QUAN TR·ªåNG: X√≥a kh·ªèi RAM ngay l·∫≠p t·ª©c
            del self.buffers[group]

    def _prepare_output_dir(self):
        if self.output_base.exists():
            shutil.rmtree(self.output_base)
        self.output_base.mkdir(parents=True, exist_ok=True)

    def _write_single_book(self, group_name: str, raw_data: Dict[str, Any]) -> str:
        """Ghi file s√°ch (JSON/JS) v√† tr·∫£ v·ªÅ t√™n file."""
        sorted_sids = sorted(raw_data.keys(), key=natural_sort_key)
        linked_data = {}
        
        for sid in sorted_sids:
            name_info = self.names_map.get(sid, {
                "acronym": "",
                "translated_title": "",
                "original_title": ""
            })

            linked_data[sid] = {
                "acronym": name_info["acronym"],
                "translated_title": name_info["translated_title"],
                "original_title": name_info["original_title"],
                "content": raw_data[sid] 
            }

        # T·∫°o file path: data/processed/sutta/mn.json
        if self.dry_run:
            file_name = f"{group_name}.json"
            file_path = self.output_base / file_name
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(linked_data, f, ensure_ascii=False, indent=self.json_indent)
        else:
            file_name = f"{group_name}.js"
            file_path = self.output_base / file_name
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            json_str = json.dumps(linked_data, ensure_ascii=False, indent=self.json_indent)
            safe_group = group_name.replace("/", "_")
            js_content = f"window.SUTTA_DB = window.SUTTA_DB || {{}}; Object.assign(window.SUTTA_DB, {json_str});"
            
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(js_content)
        
        logger.info(f"   üíæ Saved: {file_name} ({len(linked_data)} items)")
        return file_name

    def _write_loader(self, files: list):
        files.sort()
        loader_path = OUTPUT_SUTTA_BASE / "sutta_loader.js"
        js_content = f"window.ALL_SUTTA_FILES = {json.dumps(files, indent=2)};\n"
        with open(loader_path, "w", encoding="utf-8") as f:
            f.write(js_content)