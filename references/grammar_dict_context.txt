dpd-db/
├── db/
│   ├── db_helpers.py
│   └── models.py
├── exporter/
│   └── grammar_dict/
│       ├── README.md
│       └── grammar_dict.py
└── tools/
    ├── cache_load.py
    ├── configger.py
    ├── css_manager.py
    ├── date_and_time.py
    ├── degree_of_completion.py
    ├── goldendict_exporter.py
    ├── goldendict_path.py
    ├── lemma_traditional.py
    ├── mdict_exporter.py
    ├── meaning_construction.py
    ├── niggahitas.py
    ├── pali_sort_key.py
    ├── paths.py
    ├── pos.py
    ├── printer.py
    ├── sinhala_tools.py
    ├── sutta_codes.py
    └── writemdict/
        ├── pureSalsa20.py
        ├── ripemd128.py
        └── writemdict.py

================================================================================

<file path="db/db_helpers.py">
"""DB related functions:
1. Create db if doesn't already exist,
2. Create db Session
3. Get column names,
4. Print column names.
"""

import os
import sys
from pathlib import Path

from sqlalchemy import create_engine, inspect
from sqlalchemy.orm import Session, sessionmaker

from db.models import Base
from tools.printer import printer as pr


def create_db_if_not_exists(db_path: Path):
    """Create the db if it does not exist already."""
    engine = create_engine(f"sqlite+pysqlite:///{db_path}", echo=False)
    if not db_path.is_file():
        Base.metadata.create_all(bind=engine)


def create_tables(db_path: Path):
    """Create tables if they don't exist."""
    engine = create_engine(f"sqlite+pysqlite:///{db_path}", echo=False)
    Base.metadata.create_all(bind=engine)


def get_db_session(db_path: Path) -> Session:
    """Get the db session, used ubiquitously."""
    if not os.path.isfile(db_path):
        pr.red(f"Database file doesn't exist: {db_path}")
        sys.exit(1)

    try:
        db_eng = create_engine(f"sqlite+pysqlite:///{db_path}", echo=False)
        # db_conn = db_eng.connect()

        Session = sessionmaker(db_eng)
        Session.configure(bind=db_eng)
        db_sess = Session()

    except Exception as e:
        pr.red(f"Can't connect to database: {e}")
        sys.exit(1)

    return db_sess


def print_column_names(tables_name):
    """Print a numbered list of all the column names in a given table."""

    inspector = inspect(tables_name)
    column_names = [column.name for column in inspector.columns]
    for counter, column_name in enumerate(column_names):
        print(f"{counter}. {column_name}")


def get_column_names(tables_name):
    inspector = inspect(tables_name)
    column_names = [column.name for column in inspector.columns]
    return column_names

</file>


<file path="db/models.py">
"""Database model for use by SQLAlchemy."""

import json
import re
from typing import List, Optional

from sqlalchemy import DateTime, ForeignKey, and_, case, null, or_
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.orm import (
    DeclarativeBase,
    Mapped,
    foreign,
    mapped_column,
    object_session,
    relationship,
)
from sqlalchemy.sql import func

from tools.pali_sort_key import pali_sort_key
from tools.pos import CONJUGATIONS, DECLENSIONS, EXCLUDE_FROM_FREQ


class Base(DeclarativeBase):
    pass


class DbInfo(Base):
    """
    Store general key-value data such as
    1. dpd_db info, release_version, etc
    2. cached values, cf_set, etc.
    """

    __tablename__ = "db_info"
    id: Mapped[int] = mapped_column(primary_key=True)
    key: Mapped[str] = mapped_column(unique=True)
    value: Mapped[str] = mapped_column(default="")

    # value pack unpack
    def value_pack(self, data) -> None:
        self.value = json.dumps(data, ensure_ascii=False)

    @property
    def value_unpack(self) -> list[str]:
        return json.loads(self.value)


class InflectionTemplates(Base):
    """Inflection templates for generating html tables."""

    __tablename__ = "inflection_templates"
    pattern: Mapped[str] = mapped_column(primary_key=True)
    like: Mapped[str] = mapped_column(default="")
    data: Mapped[str] = mapped_column(default="")

    # inflection templates pack unpack
    def inflection_template_pack(self, list: list[str]) -> None:
        self.data = json.dumps(list, ensure_ascii=False)

    @property
    def inflection_template_unpack(self) -> list[str]:
        return json.loads(self.data)

    def __repr__(self) -> str:
        return f"InflectionTemplates: {self.pattern} {self.like} {self.data}"


class DpdRoot(Base):
    __tablename__ = "dpd_roots"

    root: Mapped[str] = mapped_column(primary_key=True)
    root_in_comps: Mapped[str] = mapped_column(default="")
    root_has_verb: Mapped[str] = mapped_column(default="")
    root_group: Mapped[int] = mapped_column(default=0)
    root_sign: Mapped[str] = mapped_column(default="")
    root_meaning: Mapped[str] = mapped_column(default="")
    sanskrit_root: Mapped[str] = mapped_column(default="")
    sanskrit_root_meaning: Mapped[str] = mapped_column(default="")
    sanskrit_root_class: Mapped[str] = mapped_column(default="")
    root_example: Mapped[str] = mapped_column(default="")
    dhatupatha_num: Mapped[str] = mapped_column(default="")
    dhatupatha_root: Mapped[str] = mapped_column(default="")
    dhatupatha_pali: Mapped[str] = mapped_column(default="")
    dhatupatha_english: Mapped[str] = mapped_column(default="")
    dhatumanjusa_num: Mapped[int] = mapped_column(default=0)
    dhatumanjusa_root: Mapped[str] = mapped_column(default="")
    dhatumanjusa_pali: Mapped[str] = mapped_column(default="")
    dhatumanjusa_english: Mapped[str] = mapped_column(default="")
    dhatumala_root: Mapped[str] = mapped_column(default="")
    dhatumala_pali: Mapped[str] = mapped_column(default="")
    dhatumala_english: Mapped[str] = mapped_column(default="")
    panini_root: Mapped[str] = mapped_column(default="")
    panini_sanskrit: Mapped[str] = mapped_column(default="")
    panini_english: Mapped[str] = mapped_column(default="")
    note: Mapped[str] = mapped_column(default="")
    matrix_test: Mapped[str] = mapped_column(default="")
    root_info: Mapped[str] = mapped_column(default="")
    root_matrix: Mapped[str] = mapped_column(default="")

    created_at: Mapped[Optional[DateTime]] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )
    updated_at: Mapped[Optional[DateTime]] = mapped_column(
        DateTime(timezone=True), onupdate=func.now()
    )

    pw: Mapped[List["DpdHeadword"]] = relationship(back_populates="rt")

    @property
    def root_clean(self) -> str:
        """Remove digits from the end"""
        return re.sub(r" \d.*$", "", self.root)

    @property
    def root_no_sign(self) -> str:
        """Remove digits from the end and root sign"""
        return re.sub(r"\d| |√", "", self.root)

    @property
    def root_(self) -> str:
        """Replace whitespace with underscores"""
        return self.root.replace(" ", "_")

    @property
    def root_no_sign_(self) -> str:
        """Remove root sign and replace whitespace with underscores.
        Useful for html links."""
        return self.root.replace(" ", "_").replace("√", "")

    @property
    def root_link(self) -> str:
        return self.root.replace(" ", "%20")

    @property
    def root_count(self) -> int:
        db_session = object_session(self)
        if db_session is None:
            raise Exception("No db_session")

        return (
            db_session.query(DpdHeadword)
            .filter(DpdHeadword.root_key == self.root)
            .count()
        )

    @property
    def root_family_list(self) -> list:
        db_session = object_session(self)
        if db_session is None:
            raise Exception("No db_session")

        results = (
            db_session.query(DpdHeadword)
            .filter(DpdHeadword.root_key == self.root)
            .group_by(DpdHeadword.family_root)
            .all()
        )
        family_list = [i.family_root for i in results if i.family_root is not None]
        family_list = sorted(family_list, key=lambda x: pali_sort_key(x))
        return family_list

    def __repr__(self) -> str:
        return f"""DpdRoot: {self.root} {self.root_group} {self.root_sign} ({self.root_meaning})"""


class FamilyRoot(Base):
    __tablename__ = "family_root"
    root_family_key: Mapped[str] = mapped_column(primary_key=True)
    root_key: Mapped[str] = mapped_column(primary_key=True)
    root_family: Mapped[str] = mapped_column(default="")
    root_meaning: Mapped[str] = mapped_column(default="")
    html: Mapped[str] = mapped_column(default="")
    data: Mapped[str] = mapped_column(default="")
    count: Mapped[int] = mapped_column(default=0)

    # root family pack unpack
    def data_pack(self, list: list[str]) -> None:
        self.data = json.dumps(list, ensure_ascii=False, indent=1)

    @property
    def data_unpack(self) -> list[str]:
        return json.loads(self.data)

    @property
    def root_family_link(self) -> str:
        return self.root_family.replace(" ", "%20")

    @property
    def root_family_(self) -> str:
        return self.root_family.replace(" ", "_")

    @property
    def root_family_clean(self) -> str:
        """Remove root sign"""
        return self.root_family.replace("√", "")

    @property
    def root_family_clean_no_space(self) -> str:
        """Remove root sign and space"""
        return self.root_family.replace("√", "").replace(" ", "")

    @property
    def root_family_key_typst(self) -> str:
        return self.root_family_key.replace(" ", "_").replace("√", "")

    def __repr__(self) -> str:
        return f"FamilyRoot: {self.root_family_key} {self.count}"


class Lookup(Base):
    __tablename__ = "lookup"

    lookup_key: Mapped[str] = mapped_column(primary_key=True)
    headwords: Mapped[str] = mapped_column(default="")
    roots: Mapped[str] = mapped_column(default="")
    deconstructor: Mapped[str] = mapped_column(default="")
    variant: Mapped[str] = mapped_column(default="")
    spelling: Mapped[str] = mapped_column(default="")
    grammar: Mapped[str] = mapped_column(default="")
    help: Mapped[str] = mapped_column(default="")
    abbrev: Mapped[str] = mapped_column(default="")
    epd: Mapped[str] = mapped_column(default="")
    rpd: Mapped[str] = mapped_column(default="")
    other: Mapped[str] = mapped_column(default="")
    sinhala: Mapped[str] = mapped_column(default="")
    devanagari: Mapped[str] = mapped_column(default="")
    thai: Mapped[str] = mapped_column(default="")

    # headwords pack unpack

    def headwords_pack(self, list: list[int]) -> None:
        if list:
            self.headwords = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def headwords_unpack(self) -> list[int]:
        if self.headwords:
            return json.loads(self.headwords)
        else:
            return []

    # roots pack unpack

    def roots_pack(self, list: list[str]) -> None:
        if list:
            self.roots = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def roots_unpack(self) -> list[str]:
        if self.roots:
            return json.loads(self.roots)
        else:
            return []

    # deconstructor pack unpack

    def deconstructor_pack(self, list: list[str]) -> None:
        if list:
            self.deconstructor = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def deconstructor_unpack(self) -> list[str]:
        if self.deconstructor:
            return json.loads(self.deconstructor)
        else:
            return []

    # variants pack unpack

    def variants_pack(self, dict) -> None:
        if dict:
            self.variant = json.dumps(dict, ensure_ascii=False)
        else:
            raise ValueError("A dict must be provided to pack.")

    @property
    def variants_unpack(self) -> dict:
        if self.variant:
            return json.loads(self.variant)
        else:
            return {}

    # spelling pack unpack

    def spelling_pack(self, list: list[str]) -> None:
        if list:
            self.spelling = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def spelling_unpack(self) -> list[str]:
        if self.spelling:
            return json.loads(self.spelling)
        else:
            return []

    # grammar pack unpack
    # TODO add a method to unpack to html

    def grammar_pack(self, list: list[tuple[str, str, str]]) -> None:
        if list:
            self.grammar = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def grammar_unpack(self) -> list[str]:
        if self.grammar:
            return json.loads(self.grammar)
        else:
            return []

    # help pack unpack

    def help_pack(self, string: str) -> None:
        if string:
            self.help = json.dumps(string, ensure_ascii=False)
        else:
            raise ValueError("A string must be provided to pack.")

    @property
    def help_unpack(self) -> str:
        if self.help:
            return json.loads(self.help)
        else:
            return ""

    # abbreviations pack unpack

    def abbrev_pack(self, dict: dict[str, str]) -> None:
        if dict:
            self.abbrev = json.dumps(dict, ensure_ascii=False, indent=1)
        else:
            raise ValueError("A dict must be provided to pack.")

    @property
    def abbrev_unpack(self) -> dict[str, str]:
        if self.abbrev:
            return json.loads(self.abbrev)
        else:
            return {}

    # epd pack unpack

    def epd_pack(self, list: list[tuple[str, str, str]]) -> None:
        if dict:
            self.epd = json.dumps(list, ensure_ascii=False, indent=1)
        else:
            raise ValueError("A dict must be provided to pack.")

    @property
    def epd_unpack(self) -> list[tuple[str, str, str]]:
        if self.epd:
            return json.loads(self.epd)
        else:
            return []

    # rpd pack unpack

    def rpd_pack(self, list: list[tuple[str, str, str]]) -> None:
        if dict:
            self.rpd = json.dumps(list, ensure_ascii=False, indent=1)
        else:
            raise ValueError("A dict must be provided to pack.")

    @property
    def rpd_unpack(self) -> list[tuple[str, str, str]]:
        if self.rpd:
            return json.loads(self.rpd)
        else:
            return []

    # pack unpack sinhala

    def sinhala_pack(self, list: list[str]) -> None:
        if list:
            self.sinhala = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def sinhala_unpack(self) -> list[str]:
        if self.sinhala:
            return json.loads(self.sinhala)
        else:
            return []

    # pack unpack devanagari

    def devanagari_pack(self, list: list[str]) -> None:
        if list:
            self.devanagari = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def devanagari_unpack(self) -> list[str]:
        if self.devanagari:
            return json.loads(self.devanagari)
        else:
            return []

    # pack unpack thai

    def thai_pack(self, list: list[str]) -> None:
        if list:
            self.thai = json.dumps(list, ensure_ascii=False)
        else:
            raise ValueError("A list must be provided to pack.")

    @property
    def thai_unpack(self) -> list[str]:
        if self.thai:
            return json.loads(self.thai)
        else:
            return []

    def __repr__(self) -> str:
        return f"""
key:           {self.lookup_key}
headwords:     {self.headwords}
roots:         {self.roots}
deconstructor: {self.deconstructor}
variant:       {self.variant}
spelling:      {self.spelling}
grammar:       {self.grammar}
help:          {self.help}
abbrev:        {self.abbrev}
sinhala:       {self.sinhala}
devanagari:    {self.devanagari}
thai:          {self.thai}
"""


# class PaliWord(Base):
#     """DO NOT USE !!! JUST FOR CONVERTING OLD FILE FORMATS !!!"""
#     __tablename__ = "pali_words"

#     id: Mapped[int] = mapped_column(primary_key=True)
#     pali_1: Mapped[str] = mapped_column(unique=True)
#     pali_2: Mapped[str] = mapped_column(default='')
#     pos: Mapped[str] = mapped_column(default='')
#     grammar: Mapped[str] = mapped_column(default='')
#     derived_from: Mapped[str] = mapped_column(default='')
#     neg: Mapped[str] = mapped_column(default='')
#     verb: Mapped[str] = mapped_column(default='')
#     trans:  Mapped[str] = mapped_column(default='')
#     plus_case:  Mapped[str] = mapped_column(default='')

#     meaning_1: Mapped[str] = mapped_column(default='')
#     meaning_lit: Mapped[str] = mapped_column(default='')
#     meaning_2: Mapped[str] = mapped_column(default='')

#     non_ia: Mapped[str] = mapped_column(default='')
#     sanskrit: Mapped[str] = mapped_column(default='')

#     root_key: Mapped[str] = mapped_column(default='')
#     root_sign: Mapped[str] = mapped_column(default='')
#     root_base: Mapped[str] = mapped_column(default='')

#     family_root: Mapped[str] = mapped_column(default='')
#     family_word: Mapped[str] = mapped_column(default='')
#     family_compound: Mapped[str] = mapped_column(default='')
#     family_set: Mapped[str] = mapped_column(default='')

#     construction:  Mapped[str] = mapped_column(default='')
#     derivative: Mapped[str] = mapped_column(default='')
#     suffix: Mapped[str] = mapped_column(default='')
#     phonetic: Mapped[str] = mapped_column(default='')
#     compound_type: Mapped[str] = mapped_column(default='')
#     compound_construction: Mapped[str] = mapped_column(default='')
#     non_root_in_comps: Mapped[str] = mapped_column(default='')

#     source_1: Mapped[str] = mapped_column(default='')
#     sutta_1: Mapped[str] = mapped_column(default='')
#     example_1: Mapped[str] = mapped_column(default='')

#     source_2: Mapped[str] = mapped_column(default='')
#     sutta_2: Mapped[str] = mapped_column(default='')
#     example_2: Mapped[str] = mapped_column(default='')

#     antonym: Mapped[str] = mapped_column(default='')
#     synonym: Mapped[str] = mapped_column(default='')
#     variant: Mapped[str] = mapped_column(default='')
#     commentary: Mapped[str] = mapped_column(default='')
#     notes: Mapped[str] = mapped_column(default='')
#     cognate: Mapped[str] = mapped_column(default='')
#     link: Mapped[str] = mapped_column(default='')
#     origin: Mapped[str] = mapped_column(default='')

#     stem: Mapped[str] = mapped_column(default='')
#     pattern: Mapped[str] = mapped_column(default='')

#     created_at: Mapped[Optional[DateTime]] = mapped_column(
#         DateTime(timezone=True), server_default=func.now())
#     updated_at: Mapped[Optional[DateTime]] = mapped_column(
#         DateTime(timezone=True), onupdate=func.now())


class SuttaInfo(Base):
    __tablename__ = "sutta_info"
    # dpd
    book: Mapped[str] = mapped_column(default="")
    book_code: Mapped[str] = mapped_column(default="")
    dpd_code: Mapped[str] = mapped_column(default="")
    dpd_sutta: Mapped[str] = mapped_column(primary_key=True)
    dpd_sutta_var: Mapped[str] = mapped_column(default="")
    # cst
    cst_code: Mapped[str] = mapped_column(default="")
    cst_nikaya: Mapped[str] = mapped_column(default="")
    cst_book: Mapped[str] = mapped_column(default="")
    cst_section: Mapped[str] = mapped_column(default="")
    cst_vagga: Mapped[str] = mapped_column(default="")
    cst_sutta: Mapped[str] = mapped_column(default="")
    cst_paranum: Mapped[str] = mapped_column(default="")
    cst_m_page: Mapped[str] = mapped_column(default="")
    cst_v_page: Mapped[str] = mapped_column(default="")
    cst_p_page: Mapped[str] = mapped_column(default="")
    cst_t_page: Mapped[str] = mapped_column(default="")
    cst_file: Mapped[str] = mapped_column(default="")
    # sutta central
    sc_code: Mapped[str] = mapped_column(default="")
    sc_book: Mapped[str] = mapped_column(default="")
    sc_vagga: Mapped[str] = mapped_column(default="")
    sc_sutta: Mapped[str] = mapped_column(default="")
    sc_eng_sutta: Mapped[str] = mapped_column(default="")
    sc_blurb: Mapped[str] = mapped_column(default="")
    # sc_card_link: Mapped[str] = mapped_column(default="")
    # sc_pali_link: Mapped[str] = mapped_column(default="")
    # sc_eng_link: Mapped[str] = mapped_column(default="")
    sc_file_path: Mapped[str] = mapped_column(default="")
    dpr_code: Mapped[str] = mapped_column(default="")
    dpr_link: Mapped[str] = mapped_column(default="")
    # bjt
    bjt_sutta_code: Mapped[str] = mapped_column(default="")
    bjt_web_code: Mapped[str] = mapped_column(default="")
    bjt_filename: Mapped[str] = mapped_column(default="")
    bjt_book_id: Mapped[str] = mapped_column(default="")
    bjt_page_num: Mapped[str] = mapped_column(default="")
    bjt_page_offset: Mapped[str] = mapped_column(default="")
    bjt_piṭaka: Mapped[str] = mapped_column(default="")
    bjt_nikāya: Mapped[str] = mapped_column(default="")
    bjt_major_section: Mapped[str] = mapped_column(default="")
    bjt_book: Mapped[str] = mapped_column(default="")
    bjt_minor_section: Mapped[str] = mapped_column(default="")
    bjt_vagga: Mapped[str] = mapped_column(default="")
    bjt_sutta: Mapped[str] = mapped_column(default="")

    dv_pts: Mapped[str] = mapped_column(default="")
    dv_main_theme: Mapped[str] = mapped_column(default="")
    dv_subtopic: Mapped[str] = mapped_column(default="")
    dv_summary: Mapped[str] = mapped_column(default="")
    dv_similes: Mapped[str] = mapped_column(default="")
    dv_key_excerpt1: Mapped[str] = mapped_column(default="")
    dv_key_excerpt2: Mapped[str] = mapped_column(default="")
    dv_stage: Mapped[str] = mapped_column(default="")
    dv_training: Mapped[str] = mapped_column(default="")
    dv_aspect: Mapped[str] = mapped_column(default="")
    dv_teacher: Mapped[str] = mapped_column(default="")
    dv_audience: Mapped[str] = mapped_column(default="")
    dv_method: Mapped[str] = mapped_column(default="")
    dv_length: Mapped[str] = mapped_column(default="")
    dv_prominence: Mapped[str] = mapped_column(default="")
    dv_nikayas_parallels: Mapped[str] = mapped_column(default="")
    dv_āgamas_parallels: Mapped[str] = mapped_column(default="")
    dv_taisho_parallels: Mapped[str] = mapped_column(default="")
    dv_sanskrit_parallels: Mapped[str] = mapped_column(default="")
    dv_vinaya_parallels: Mapped[str] = mapped_column(default="")
    dv_others_parallels: Mapped[str] = mapped_column(default="")
    dv_partial_parallels_nā: Mapped[str] = mapped_column(default="")
    dv_partial_parallels_all: Mapped[str] = mapped_column(default="")
    dv_suggested_suttas: Mapped[str] = mapped_column(default="")

    @property
    def sc_card_link(self) -> str | None:
        if self.sc_code:
            return f"https://suttacentral.net/{self.sc_code}"
        else:
            return None

    @property
    def sc_pali_link(self) -> str | None:
        if self.sc_code:
            return f"https://suttacentral.net/{self.sc_code}/pli/ms"
        else:
            return None

    @property
    def sc_eng_link(self) -> str | None:
        if self.sc_code:
            return f"https://suttacentral.net/{self.sc_code}/en/sujato"
        else:
            return None

    @property
    def sc_book_code(self) -> str | None:
        if self.sc_code:
            return re.sub(r"\d+\.*-*\d*", "", self.sc_code)
        else:
            return None

    @property
    def sc_github(self) -> str | None:
        if self.sc_code:
            return (
                f"https://github.com/suttacentral/sc-data/blob/main/{self.sc_file_path}"
            )
        else:
            return None

    @property
    def sc_express_link(self) -> str | None:
        if self.sc_code:
            return f"https://suttacentral.express/{self.sc_code.lower()}/en/sujato"
        else:
            return None

    @property
    def dhamma_gift(self) -> str | None:
        if self.sc_code:
            return f"https://find.dhamma.gift/read/?q={self.sc_code}"
        else:
            return None

    @property
    def tbw(self) -> str | None:
        if self.sc_code:
            if self.book_code in [
                "DN",
                "MN",
                "SN",
                "AN",
                "KHP",
                "DHP",
                "UD",
                "ITI",
                "SNP",
                "TH",
                "THI",
            ]:
                if self.sc_book_code == "iti":
                    return "https://thebuddhaswords.net/it/it.html"
                else:
                    return f"https://thebuddhaswords.net/{self.sc_book_code.lower()}/{self.sc_code.lower()}.html"
            else:
                return None
        else:
            return None

    @property
    def tbw_legacy(self) -> str | None:
        if self.sc_code:
            if self.book_code in [
                "DN",
                "MN",
                "SN",
                "AN",
                "KHP",
                "DHP",
                "UD",
                "ITI",
                "SNP",
                "TH",
                "THI",
            ]:
                if self.sc_book_code == "iti":
                    return "https://find.dhamma.gift/bw/it/it.html"
                else:
                    return f"https://find.dhamma.gift/bw/{self.sc_book_code.lower()}/{self.sc_code.lower()}.html"
            else:
                return None
        else:
            return None

    @property
    def sc_voice_link(self) -> str | None:
        if self.sc_code:
            return f"https://www.sc-voice.net/#/sutta/{self.sc_code.lower()}/en/sujato"
        else:
            return None

    @property
    def tpp_org(self) -> str | None:
        if self.cst_code:
            tpp_org_code = re.sub(r"romn\/|\.xml", "", self.cst_file)
            return (
                f"https://tipitakapali.org/book/{tpp_org_code}#para{self.cst_paranum}"
            )
        else:
            return None

    @property
    def sutta_info_count(self) -> int:
        db_session = object_session(self)
        if db_session is None:
            raise Exception("No db_session")

        return (
            db_session.query(SuttaInfo)
            .filter(
                or_(
                    DpdHeadword.lemma_1 == self.dpd_sutta,
                    DpdHeadword.lemma_1 == self.dpd_sutta_var,
                )
            )
            .count()
        )

    @property
    def sutta_codes_list(self) -> list[str]:
        from tools.sutta_codes import make_list_of_sutta_codes

        return make_list_of_sutta_codes(self)

    @property
    def dv_exists(self) -> bool:
        if (
            self.dv_pts
            or self.dv_main_theme
            or self.dv_subtopic
            or self.dv_stage
            or self.dv_training
            or self.dv_aspect
            or self.dv_teacher
            or self.dv_audience
            or self.dv_method
            or self.dv_length
            or self.dv_prominence
            or self.dv_nikayas_parallels
            or self.dv_āgamas_parallels
            or self.dv_taisho_parallels
            or self.dv_sanskrit_parallels
            or self.dv_vinaya_parallels
            or self.dv_others_parallels
            or self.dv_partial_parallels_nā
            or self.dv_partial_parallels_all
            or self.dv_summary
            or self.dv_key_excerpt1
            or self.dv_key_excerpt2
            or self.dv_similes
            or self.dv_suggested_suttas
        ):
            return True
        else:
            return False

    @property
    def dv_parallels_exists(self) -> bool:
        if (
            self.dv_nikayas_parallels
            or self.dv_āgamas_parallels
            or self.dv_taisho_parallels
            or self.dv_sanskrit_parallels
            or self.dv_vinaya_parallels
            or self.dv_others_parallels
            or self.dv_partial_parallels_nā
            or self.dv_partial_parallels_all
        ):
            return True
        else:
            return False

    def __repr__(self) -> str:
        return f"SuttaInfo: {self.dpd_code} {self.dpd_sutta}"

    @property
    def bjt_github_link(self):
        if self.bjt_filename:
            return f"https://github.com/pathnirvana/tipitaka.lk/blob/master/public/static/text/{self.bjt_filename}.json"
        else:
            return None

    @property
    def bjt_tipitaka_lk_link(self):
        if self.bjt_web_code:
            return f"https://tipitaka.lk/{self.bjt_web_code}"
        else:
            return None

    @property
    def bjt_open_tipitaka_lk_link(self):
        if self.bjt_web_code:
            return f"https://open.tipitaka.lk/latn/{self.bjt_web_code}"
        else:
            return None


class DpdHeadword(Base):
    __tablename__ = "dpd_headwords"

    id: Mapped[int] = mapped_column(primary_key=True)
    lemma_1: Mapped[str] = mapped_column(ForeignKey("sutta_info.dpd_sutta"), default="")
    lemma_2: Mapped[str] = mapped_column(default="")
    pos: Mapped[str] = mapped_column(default="")
    grammar: Mapped[str] = mapped_column(default="")
    derived_from: Mapped[str] = mapped_column(default="")
    neg: Mapped[str] = mapped_column(default="")
    verb: Mapped[str] = mapped_column(default="")
    trans: Mapped[str] = mapped_column(default="")
    plus_case: Mapped[str] = mapped_column(default="")

    meaning_1: Mapped[str] = mapped_column(default="")
    meaning_lit: Mapped[str] = mapped_column(default="")
    meaning_2: Mapped[str] = mapped_column(default="")

    non_ia: Mapped[str] = mapped_column(default="")
    sanskrit: Mapped[str] = mapped_column(default="")

    root_key: Mapped[str] = mapped_column(ForeignKey("dpd_roots.root"), default="")
    root_sign: Mapped[str] = mapped_column(default="")
    root_base: Mapped[str] = mapped_column(default="")

    family_root: Mapped[str] = mapped_column(default="")
    family_word: Mapped[str] = mapped_column(
        ForeignKey("family_word.word_family"), default=""
    )
    family_compound: Mapped[str] = mapped_column(default="")
    family_idioms: Mapped[str] = mapped_column(default="")
    family_set: Mapped[str] = mapped_column(default="")

    construction: Mapped[str] = mapped_column(default="")
    derivative: Mapped[str] = mapped_column(default="")
    suffix: Mapped[str] = mapped_column(default="")
    phonetic: Mapped[str] = mapped_column(default="")
    compound_type: Mapped[str] = mapped_column(default="")
    compound_construction: Mapped[str] = mapped_column(default="")
    non_root_in_comps: Mapped[str] = mapped_column(default="")

    source_1: Mapped[str] = mapped_column(default="")
    sutta_1: Mapped[str] = mapped_column(default="")
    example_1: Mapped[str] = mapped_column(default="")

    source_2: Mapped[str] = mapped_column(default="")
    sutta_2: Mapped[str] = mapped_column(default="")
    example_2: Mapped[str] = mapped_column(default="")

    antonym: Mapped[str] = mapped_column(default="")
    synonym: Mapped[str] = mapped_column(default="")
    variant: Mapped[str] = mapped_column(default="")
    var_phonetic: Mapped[str] = mapped_column(default="")
    var_text: Mapped[str] = mapped_column(default="")
    commentary: Mapped[str] = mapped_column(default="")
    notes: Mapped[str] = mapped_column(default="")
    cognate: Mapped[str] = mapped_column(default="")
    link: Mapped[str] = mapped_column(default="")
    origin: Mapped[str] = mapped_column(default="")

    stem: Mapped[str] = mapped_column(default="")
    pattern: Mapped[str] = mapped_column(
        ForeignKey("inflection_templates.pattern"), default=""
    )

    created_at: Mapped[Optional[DateTime]] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )
    updated_at: Mapped[Optional[DateTime]] = mapped_column(
        DateTime(timezone=True), onupdate=func.now()
    )

    # derived data

    inflections: Mapped[str] = mapped_column(default="")
    inflections_api_ca_eva_iti: Mapped[str] = mapped_column(default="")
    inflections_sinhala: Mapped[str] = mapped_column(default="")
    inflections_devanagari: Mapped[str] = mapped_column(default="")
    inflections_thai: Mapped[str] = mapped_column(default="")
    inflections_html: Mapped[str] = mapped_column(default="")
    freq_data: Mapped[str] = mapped_column(default="")
    freq_html: Mapped[str] = mapped_column(default="")
    ebt_count: Mapped[int] = mapped_column(default=0, server_default="0")

    # pali_root
    rt: Mapped[DpdRoot] = relationship(uselist=False)

    fr = relationship(
        "FamilyRoot",
        primaryjoin=and_(
            root_key == foreign(FamilyRoot.root_key),
            family_root == foreign(FamilyRoot.root_family),
        ),
        uselist=False,
        sync_backref=False,
    )

    #  FamilyWord
    fw = relationship("FamilyWord", uselist=False)

    # inflection templates
    it: Mapped[InflectionTemplates] = relationship()

    # sutta info
    su: Mapped[SuttaInfo] = relationship()

    @hybrid_property
    def root_family_key(self):  # type:ignore
        if self.root_key and self.family_root:
            return f"{self.root_key} {self.family_root}"
        else:
            return ""

    @root_family_key.expression
    def root_family_key(cls):
        return case(
            (
                and_(cls.root_key != null(), cls.family_root != null()),  # type:ignore
                cls.root_key + " " + cls.family_root,
            ),
            else_="",
        )

    @property
    def lemma_1_(self) -> str:
        return self.lemma_1.replace(" ", "_").replace(".", "_")

    @property
    def lemma_link(self) -> str:
        return self.lemma_1.replace(" ", "%20")

    @property
    def lemma_clean(self) -> str:
        return re.sub(r" \d.*$", "", self.lemma_1)

    @property
    def lemma_ipa(self) -> str:
        # from tools.ipa import convert_uni_to_ipa
        # return convert_uni_to_ipa(self.lemma_clean, "ipa")
        from aksharamukha import transliterate

        return str(
            transliterate.process(
                "IASTPali",
                "IPA",
                self.lemma_clean,
            )
        )

    # meaning construction

    @property
    def meaning_combo(self) -> str:
        """`meaning_1` if it exists, else `meaning_2`, plus `literal meaning` if it exists."""
        from tools.meaning_construction import make_meaning_combo

        return make_meaning_combo(self)

    @property
    def meaning_combo_html(self) -> str:
        """`meaning_1` in bold tags if it exists, else `meaning_1`, plus `literal meaning` if it exists."""
        from tools.meaning_construction import make_meaning_combo_html

        return make_meaning_combo_html(self)

    @property
    def root_base_clean(self) -> str:
        from tools.meaning_construction import clean_construction

        return clean_construction(self.root_base)

    @property
    def construction_summary(self) -> str:
        from tools.meaning_construction import summarize_construction

        return summarize_construction(self)

    @property
    def construction_clean(self) -> str:
        from tools.meaning_construction import clean_construction

        return clean_construction(self.construction)

    @property
    def degree_of_completion_html(self) -> str:
        """How complete is a word's information?
        ✔ complete, meaning_1 and example.
        ◑ semi-complete, meaning_1 and no example.
        ✘ incomplete, meaning_2 and no example.
        Styled in gray html tags.
        """
        from tools.degree_of_completion import degree_of_completion

        return degree_of_completion(self)

    @property
    def degree_of_completion(self) -> str:
        """How complete is a word's information?
        ✔ complete, meaning_1 and example.
        ◑ semi-complete, meaning_1 and no example.
        ✘ incomplete, meaning_2 and no example.
        Styled in plain text.
        """
        from tools.degree_of_completion import degree_of_completion

        return degree_of_completion(self, html=False)

    @property
    def lemma_trad(self) -> str:
        from tools.lemma_traditional import make_lemma_trad

        return make_lemma_trad(self)

    @property
    def lemma_trad_clean(self) -> str:
        from tools.lemma_traditional import make_lemma_trad_clean

        return make_lemma_trad_clean(self)

    # root

    @property
    def root_clean(self) -> str:
        try:
            if self.root_key is None:
                return ""
            else:
                return re.sub(r" \d.*$", "", self.root_key)
        except Exception as e:
            print(f"{self.lemma_1}: {e}")
            return ""

    @property
    def construction_line1(self) -> str:
        if self.construction:
            return re.sub("\n.*", "", self.construction)
        else:
            return ""

    @property
    def construction_line1_clean(self) -> str:
        from tools.meaning_construction import clean_construction

        return clean_construction(self.construction_line1)

    @property
    def construction_line1_clean_list(self) -> list[str]:
        return self.construction_line1_clean.split(" + ")

    @property
    def family_compound_list(self) -> list:
        if self.family_compound:
            return self.family_compound.split(" ")
        else:
            return [self.lemma_clean]

    @property
    def family_idioms_list(self) -> list:
        if self.family_idioms:
            return self.family_idioms.split(" ")
        else:
            return [self.lemma_clean]

    @property
    def family_set_list(self) -> list:
        if self.family_set:
            return self.family_set.split("; ")
        else:
            return []

    @property
    def root_count(self) -> int:
        db_session = object_session(self)
        if db_session is None:
            raise Exception("No db_session")

        return (
            db_session.query(DpdHeadword.id)
            .filter(DpdHeadword.root_key == self.root_key)
            .count()
        )

    @property
    def pos_list(self) -> list:
        db_session = object_session(self)
        if db_session is None:
            raise Exception("No db_session")

        pos_db = db_session.query(DpdHeadword.pos).group_by(DpdHeadword.pos).all()
        return sorted([i.pos for i in pos_db])

    @property
    def antonym_list(self) -> list:
        if self.antonym:
            return self.antonym.split(", ")
        else:
            return []

    @property
    def synonym_list(self) -> list:
        if self.synonym:
            return self.synonym.split(", ")
        else:
            return []

    @property
    def variant_list(self) -> list:
        if self.variant:
            return self.variant.split(", ")
        else:
            return []

    @property
    def sanskrit_clean(self) -> str:
        sanskrit_clean = re.sub(r"\[.+\]", "", self.sanskrit)
        return sanskrit_clean.strip()

    # derived data properties

    @property
    def inflections_list(self) -> list[str]:
        if self.inflections:
            return self.inflections.split(",")
        else:
            return []

    @property
    def inflections_list_api_ca_eva_iti(self) -> list[str]:
        if self.inflections_api_ca_eva_iti:
            return self.inflections_api_ca_eva_iti.split(",")
        else:
            return []

    @property
    def inflections_list_all(self) -> list[str]:
        all_inflections = []
        all_inflections.extend(self.inflections.split(","))
        all_inflections.extend(self.inflections_api_ca_eva_iti.split(","))
        return all_inflections

    @property
    def inflections_sinhala_list(self) -> list[str]:
        if self.inflections_sinhala:
            return self.inflections_sinhala.split(",")
        else:
            return []

    @property
    def inflections_devanagari_list(self) -> list[str]:
        if self.inflections_devanagari:
            return self.inflections_devanagari.split(",")
        else:
            return []

    @property
    def inflections_thai_list(self) -> list[str]:
        if self.inflections_thai:
            return self.inflections_thai.split(",")
        else:
            return []

    @property
    def freq_data_unpack(self) -> dict[str, int]:
        if self.freq_data:
            return json.loads(self.freq_data)
        else:
            return {}

    # typst

    @property
    def meaning_1_typst(self) -> str:
        return self.meaning_1.replace("*", r"\*")

    @property
    def meaning_2_typst(self) -> str:
        return self.meaning_2.replace("*", r"\*")

    @property
    def sanskrit_typst(self) -> str:
        return self.sanskrit.replace("[", r"\[").replace("]", r"\]")

    @property
    def root_family_key_typst(self) -> str:
        return self.root_family_key.replace(" ", "_").replace("√", "")

    @property
    def root_base_typst(self) -> str:
        return self.root_base.replace("*", "\\*")

    @property
    def root_sign_typst(self) -> str:
        return self.root_sign.replace("*", "\\*")

    @property
    def construction_typst(self) -> str:
        return self.construction.replace("\n", r"\ ").replace("*", "\\*")

    @property
    def construction_summary_typst(self) -> str:
        from tools.meaning_construction import summarize_construction

        return summarize_construction(self).replace("*", "\\*")

    @property
    def suffix_typst(self) -> str:
        return self.suffix.replace("*", "\\*")

    @property
    def compound_construction_typst(self) -> str:
        return (
            self.compound_construction.replace("*", "\\*")
            .replace("<b>", "#strong[")
            .replace("</b>", "]")
        )

    @property
    def compound_construction_txt(self) -> str:
        return self.compound_construction.replace("<b>", "").replace("</b>", "")

    @property
    def phonetic_typst(self) -> str:
        return self.phonetic.replace("\n", r"\ ")

    @property
    def phonetic_txt(self) -> str:
        return self.phonetic.replace("\n", r", ")

    @property
    def commentary_typst(self) -> str:
        return (
            self.commentary.replace("\n", r"\ ")
            .replace("<b>", "#strong[")
            .replace("</b>", "]")
        )

    @property
    def notes_typst(self) -> str:
        return (
            self.notes.replace("*", r"\*")
            .replace("\n", r"\ ")
            .replace("<b>", "#strong[")
            .replace("</b>", "]")
            .replace("<i>", "_")
            .replace("</i>", "_")
        )

    @property
    def notes_txt(self) -> str:
        notes_clean = (
            self.notes.replace("\n", r" ")
            .replace("<b>", "")
            .replace("</b>", "")
            .replace("<i>", "")
            .replace("</i>", "")
        )
        return re.sub(r"\.$", "", notes_clean)

    @property
    def cognate_typst(self) -> str:
        return self.cognate.replace("*", "\\*")

    @property
    def link_typst(self) -> str:
        link_string: str = ""
        for website in self.link.split("\n"):
            link_string += f"""#link("{website}")\\n"""
        return link_string

    @property
    def link_txt(self) -> str:
        return ", ".join(self.link.split("\n"))

    @property
    def link_list(self) -> list[str]:
        return self.link.split("\n")

    @property
    def example_1_typst(self) -> str:
        return (
            self.example_1.replace("\n", r"\ ")
            .replace("<b>", "#strong[")
            .replace("</b>", "]")
        )

    @property
    def example_2_typst(self) -> str:
        return (
            self.example_2.replace("\n", r"\ ")
            .replace("<b>", "#strong[")
            .replace("</b>", "]")
        )

    @property
    def sutta_1_typst(self) -> str:
        return self.sutta_1.replace("\n", ", ")

    @property
    def sutta_2_typst(self) -> str:
        return self.sutta_2.replace("\n", ", ")

    # needs_button

    @property
    def needs_sutta_info_button(self) -> int:
        from tools.cache_load import load_sutta_info_set

        sutta_info_set = load_sutta_info_set()
        return self.lemma_1 in sutta_info_set

    @property
    def needs_grammar_button(self) -> bool:
        return bool(self.meaning_1)

    @property
    def needs_example_button(self) -> bool:
        return bool(self.meaning_1 and self.example_1 and not self.example_2)

    @property
    def needs_examples_button(self) -> bool:
        return bool(self.meaning_1 and self.example_1 and self.example_2)

    @property
    def needs_conjugation_button(self) -> bool:
        return bool(self.pos in CONJUGATIONS)

    @property
    def needs_declension_button(self) -> bool:
        return bool(self.pos in DECLENSIONS)

    @property
    def needs_root_family_button(self) -> bool:
        return bool(self.family_root)

    @property
    def needs_word_family_button(self) -> bool:
        return bool(self.family_word)

    @property
    def cf_set(self) -> set[str]:
        from tools.cache_load import load_cf_set

        return load_cf_set()

    @property
    def idioms_set(self) -> set[str]:
        from tools.cache_load import load_idioms_set

        return load_idioms_set()

    @property
    def needs_compound_family_button(self) -> bool:
        return bool(
            self.meaning_1
            and " " not in self.family_compound
            and "sandhi" not in self.pos
            and "idiom" not in self.pos
            and "?" not in self.compound_type
            and (
                any(item in self.cf_set for item in self.family_compound_list)
                or (
                    self.lemma_clean in self.cf_set  # type:ignore
                    and not self.family_compound
                )
            )
        )

        # alternative logic
        # i.meaning_1
        # and i.lemma_clean in cf_set)
        # or (
        #     i.meaning_1
        #     and i.family_compound
        #     and any(item in cf_set
        #         for item in i.family_compound_list))

    @property
    def needs_compound_families_button(self) -> bool:
        return bool(
            self.meaning_1
            and " " in self.family_compound
            and "sandhi" not in self.pos
            and "idiom" not in self.pos
            and len(self.lemma_clean) < 30
            and (
                any(item in self.cf_set for item in self.family_compound_list)
                or (
                    self.lemma_clean in self.cf_set  # type:ignore
                    and not self.family_compound
                )
            )
        )

    @property
    def needs_idioms_button(self) -> bool:
        return bool(
            self.meaning_1
            and (
                any(item in self.idioms_set for item in self.family_idioms_list)
                or (not self.family_idioms_list and self.lemma_clean in self.idioms_set)
            )
        )

    @property
    def needs_set_button(self) -> bool:
        return bool(
            self.meaning_1 and self.family_set and len(self.family_set_list) == 1
        )

    @property
    def needs_sets_button(self) -> bool:
        return bool(
            self.meaning_1 and self.family_set and len(self.family_set_list) > 1
        )

    @property
    def needs_frequency_button(self) -> bool:
        return bool(self.pos not in EXCLUDE_FROM_FREQ)

    def __repr__(self) -> str:
        return f"""DpdHeadword: {self.id} {self.lemma_1} {self.pos} {self.meaning_1}"""


class FamilyCompound(Base):
    __tablename__ = "family_compound"
    compound_family: Mapped[str] = mapped_column(primary_key=True)
    html: Mapped[str] = mapped_column(default="")
    data: Mapped[str] = mapped_column(default="")
    count: Mapped[int] = mapped_column(default=0)

    # family_compound pack unpack
    def data_pack(self, list: list[str]) -> None:
        self.data = json.dumps(list, ensure_ascii=False, indent=1)

    @property
    def data_unpack(self) -> list[str]:
        return json.loads(self.data)

    def __repr__(self) -> str:
        return f"FamilyCompound: {self.compound_family} {self.count}"


class FamilyWord(Base):
    __tablename__ = "family_word"
    word_family: Mapped[str] = mapped_column(primary_key=True)
    html: Mapped[str] = mapped_column(default="")
    data: Mapped[str] = mapped_column(default="")
    count: Mapped[int] = mapped_column(default=0)

    dpd_headwords: Mapped[List["DpdHeadword"]] = relationship(
        "DpdHeadword", back_populates="fw"
    )

    # family_word pack unpack
    def data_pack(self, list: list[str]) -> None:
        self.data = json.dumps(list, ensure_ascii=False, indent=1)

    @property
    def data_unpack(self) -> list[str]:
        return json.loads(self.data)

    def __repr__(self) -> str:
        return f"FamilyWord: {self.word_family} {self.count}"


class FamilySet(Base):
    __tablename__ = "family_set"
    set: Mapped[str] = mapped_column(primary_key=True)
    html: Mapped[str] = mapped_column(default="")
    data: Mapped[str] = mapped_column(default="")
    count: Mapped[int] = mapped_column(default=0)

    # family_set pack unpack
    def data_pack(self, list: list[str]) -> None:
        self.data = json.dumps(list, ensure_ascii=False, indent=1)

    @property
    def data_unpack(self) -> list[str]:
        return json.loads(self.data)

    def __repr__(self) -> str:
        return f"FamilySet: {self.set} {self.count}"


class FamilyIdiom(Base):
    __tablename__ = "family_idiom"
    idiom: Mapped[str] = mapped_column(primary_key=True)
    html: Mapped[str] = mapped_column(default="")
    data: Mapped[str] = mapped_column(default="")
    count: Mapped[int] = mapped_column(default=0)

    # idioms data pack unpack
    def data_pack(self, list: list[str]) -> None:
        self.data = json.dumps(list, ensure_ascii=False, indent=1)

    @property
    def data_unpack(self) -> list[str]:
        return json.loads(self.data)

    def __repr__(self) -> str:
        return f"FamilyIdiom: {self.idiom} {self.count}"


class BoldDefinition(Base):
    __tablename__ = "bold_definitions"

    id: Mapped[int] = mapped_column(primary_key=True)
    file_name: Mapped[str] = mapped_column(default="")
    ref_code: Mapped[str] = mapped_column(default="")
    nikaya: Mapped[str] = mapped_column(default="")
    book: Mapped[str] = mapped_column(default="")
    title: Mapped[str] = mapped_column(default="")
    subhead: Mapped[str] = mapped_column(default="")
    bold: Mapped[str] = mapped_column(default="")
    bold_end: Mapped[str] = mapped_column(default="")
    commentary: Mapped[str] = mapped_column(default="")

    def update_bold_definition(
        self,
        file_name,
        ref_code,
        nikaya,
        book,
        title,
        subhead,
        bold,
        bold_end,
        commentary,
    ):
        self.file_name = file_name
        self.ref_code = ref_code
        self.nikaya = nikaya
        self.book = book
        self.title = title
        self.subhead = subhead
        self.bold = bold
        self.bold_end = bold_end
        self.commentary = commentary

    def __repr__(self) -> str:
        return f"""
{"file_name":<20}{self.file_name}
{"ref_code":<20}{self.ref_code}
{"nikaya":<20}{self.nikaya}
{"book":<20}{self.book}
{"title":<20}{self.title}
{"subhead":<20}{self.subhead}
{"bold":<20}{self.bold}
{"bold_end":<20}{self.bold_end}
{"commentary":<20}{self.commentary}
"""

</file>


<file path="exporter/grammar_dict/README.md">
# exporter/grammar_dict/

## Purpose & Rationale
While the main dictionary provides definitions, the `grammar_dict/` exporter exists to provide a standalone, highly specialized reference focused purely on Pāḷi morphology. Its rationale is to help students and scholars quickly identify the grammatical role of any inflected word-form (e.g., "this word is an aorist 3rd person plural") without the clutter of full dictionary definitions.

## Architectural Logic
This subsystem follows a "Specialized Index Export" pattern:
1.  **Data Extraction:** It pulls "packed" grammatical data from the `Lookup` table, which already contains the pre-calculated results from the `db/grammar/` subsystem.
2.  **HTML Templating:** It uses Mako templates to transform these grammatical possibilities into clean, consistent HTML tables.
3.  **Cross-Format Export:** It utilizes the project's standard export tools (`tools/goldendict_exporter.py` and `tools/mdict_exporter.py`) to generate the final binaries for GoldenDict and MDict.
4.  **Minification:** Like other exporters, it applies CSS and HTML minification to ensure the resulting dictionary files are as efficient as possible.

## Relationships & Data Flow
- **Source:** Pulls from the `Lookup` table in the database, relying on the `grammar` column.
- **Consumption:** Provides a standalone dictionary product that users can install alongside the main DPD.
- **Identity:** Uses project-standard CSS to ensure the tables look consistent with the main dictionary entries.

## Interface
- **Export:** `uv run python exporter/grammar_dict/grammar_dict.py`

</file>


<file path="exporter/grammar_dict/grammar_dict.py">
#!/usr/bin/env python3

"""Compile HTML table of all grammatical possibilities of every inflected word-form."""

from mako.template import Template

from db.db_helpers import get_db_session
from db.models import Lookup
from tools.configger import config_test
from tools.css_manager import CSSManager
from tools.goldendict_exporter import (
    DictEntry,
    DictInfo,
    DictVariables,
    export_to_goldendict_with_pyglossary,
)
from tools.mdict_exporter import export_to_mdict
from tools.niggahitas import add_niggahitas
from tools.paths import ProjectPaths
from tools.printer import printer as pr


class GlobalVars:
    def __init__(self) -> None:
        if config_test("dictionary", "make_mdict", "yes"):
            self.make_mdict = True
        else:
            self.make_mdict = False

        self.pth = ProjectPaths()
        self.db_session = get_db_session(self.pth.dpd_db_path)

        # the grammar dictionaries
        self.html_dict: dict[str, str] = {}  # Renamed from grammar_dict_html

        # goldendict and mdict data_list
        self.dict_data: list[DictEntry] = []

    def close_db(self):
        self.db_session.close()

    def commit_db(self):
        self.db_session.commit()


def main():
    pr.tic()
    pr.title("exporting grammar dictionary")

    if not config_test("exporter", "make_grammar", "yes"):
        pr.green("disabled in config.ini")
        pr.toc()
        return

    g = GlobalVars()

    generate_html_from_lookup(g)  # New function replaces old ones

    g.close_db()  # Close db session when done

    make_data_lists(g)
    prepare_gd_mdict_and_export(g)

    pr.toc()


def render_header_templ(
    __pth__: ProjectPaths, css: str, js: str, header_templ: Template
) -> str:
    """render the html header with css and js"""

    return str(header_templ.render(css=css, js=js))


def generate_html_from_lookup(g: GlobalVars):
    """Generate HTML grammar tables from Lookup table data."""
    pr.green("querying database")

    # Query the Lookup table for entries with grammar data
    lookup_results = (
        g.db_session.query(Lookup)
        .filter(Lookup.grammar.is_not(None), Lookup.grammar != "")
        .all()
    )

    pr.yes(f"{len(lookup_results)}")

    pr.green_title("compiling html")

    html_dict = {}

    # create the header from a template
    header_templ = Template(filename=str(g.pth.grammar_dict_header_templ_path))
    html_header = render_header_templ(g.pth, css="", js="", header_templ=header_templ)

    # Add variables and fonts to header
    css_manager = CSSManager()
    html_header = css_manager.update_style(html_header, "primary")

    html_header += "<body><div class='dpd'><table class='grammar_dict'>"
    html_header += "<thead><tr><th id='col1'>pos ⇅</th><th id='col2'>⇅</th><th id='col3'>⇅</th><th id='col4'>⇅</th><th id='col5'></th><th id='col6'>word ⇅</th></tr></thead><tbody>"

    # Process each lookup entry
    for counter, lookup_entry in enumerate(lookup_results):
        inflected_word = lookup_entry.lookup_key
        grammar_data_list = (
            lookup_entry.grammar_unpack
        )  # [(headword, pos, grammar_str)]

        html_lines = []
        for data_tuple in grammar_data_list:
            headword, pos, grammar_str = data_tuple
            html_line = "<tr>"
            html_line += f"<td><b>{pos}</b></td>"

            # get grammatical_categories from grammar_str
            grammatical_categories = []
            if grammar_str.startswith("reflx"):
                grammatical_categories.append(
                    grammar_str.split()[0] + " " + grammar_str.split()[1]
                )
                grammatical_categories += grammar_str.split()[2:]
                for grammatical_category in grammatical_categories:
                    html_line += f"<td>{grammatical_category}</td>"
            elif grammar_str.startswith("in comps"):
                html_line += f"<td colspan='3'>{grammar_str}</td>"
            else:
                grammatical_categories = grammar_str.split()
                # adding empty values if there are less than 3
                while len(grammatical_categories) < 3:
                    grammatical_categories.append("")
                for grammatical_category in grammatical_categories:
                    html_line += f"<td>{grammatical_category}</td>"

            html_line += "<td>of</td>"
            html_line += f"<td>{headword}</td>"
            html_line += "</tr>"
            html_lines.append(html_line)

        # Assemble the full HTML for the entry
        entry_html = (
            html_header + "".join(html_lines) + "</tbody></table></div></body></html>"
        )
        html_dict[inflected_word] = entry_html

        if counter % 10000 == 0:
            pr.counter(counter, len(lookup_results), inflected_word)

    g.html_dict = html_dict
    pr.yes(len(g.html_dict))


def make_data_lists(g: GlobalVars):
    """Make the data_lists to be consumed by GoldenDict and MDict"""
    pr.green("making data lists")

    dict_data: list[DictEntry] = []
    # Use the refactored html_dict
    for word, html in g.html_dict.items():
        synonyms = add_niggahitas([word])

        dict_data += [
            DictEntry(
                word=word, definition_html=html, definition_plain="", synonyms=synonyms
            )
        ]

    g.dict_data = dict_data
    pr.yes("ok")


def prepare_gd_mdict_and_export(g: GlobalVars):
    """Prepare the metadata and export to goldendict & mdict."""

    dict_info = DictInfo(
        bookname="DPD Grammar",
        author="Bodhirasa",
        description="<h3>DPD Grammar</h3><p>A table of all the grammatical possibilities that a particular inflected word may have. For more information please visit the <a href='https://digitalpalidictionary.github.io/features/grammardict/' target='_blank'>DPD website</a></p>",
        website="https://digitalpalidictionary.github.io/features/grammardict/",
        source_lang="pi",
        target_lang="en",
    )
    dict_name = "dpd-grammar"

    dict_vars = DictVariables(
        css_paths=[g.pth.dpd_css_and_fonts_path],
        js_paths=[g.pth.sorter_js_path],
        gd_path=g.pth.share_dir,
        md_path=g.pth.share_dir,
        dict_name=dict_name,
        icon_path=g.pth.dpd_logo_svg,
        font_path=g.pth.fonts_dir,
        zip_up=False,
        delete_original=False,
    )

    export_to_goldendict_with_pyglossary(dict_info, dict_vars, g.dict_data)

    if g.make_mdict:
        export_to_mdict(dict_info, dict_vars, g.dict_data)


if __name__ == "__main__":
    main()

</file>


<file path="tools/cache_load.py">
#!/usr/bin/env python3

"""Get cf_set and idioms_set from the DbInfo cache."""

import json

from db.db_helpers import get_db_session
from tools.paths import ProjectPaths

pth = ProjectPaths()
db_session = get_db_session(pth.dpd_db_path)

_cf_set_cache = None
_idioms_set_cache = None
_sutta_info_cache = None


def load_sutta_info_set():
    from db.models import SuttaInfo

    global _sutta_info_cache

    if _sutta_info_cache is not None:
        return _sutta_info_cache
    else:
        db = (
            db_session.query(SuttaInfo.dpd_sutta, SuttaInfo.dpd_sutta_var)
            .filter(SuttaInfo.dpd_code != "")
            .all()
        )
        _sutta_info_cache = set([i[0] for i in db])
        # _sutta_info_cache.update([i[1] for i in db if i[1]]) exclude for now
        return _sutta_info_cache


def load_cf_set() -> set[str]:
    """generate a list of all compounds families"""
    from db.models import DbInfo

    global _cf_set_cache

    if _cf_set_cache is not None:
        return _cf_set_cache
    else:
        cf_set_cache = db_session.query(DbInfo).filter_by(key="cf_set").first()
        cf_set = json.loads(cf_set_cache.value)
        _cf_set_cache = cf_set
        return cf_set


def load_idioms_set() -> set[str]:
    """generate a list of all compounds families"""
    from db.models import DbInfo

    global _idioms_set_cache

    if _idioms_set_cache is not None:
        return _idioms_set_cache
    else:
        idioms_set_cache = db_session.query(DbInfo).filter_by(key="idioms_set").first()
        idioms_set = json.loads(idioms_set_cache.value)
        _idioms_set_cache = idioms_set
        return idioms_set


if __name__ == "__main__":
    from db.models import DbInfo

    print(load_cf_set(DbInfo))


# --------------------------------------old-----------------
# pth = ProjectPaths()

# _cached_cf_set: Optional[Set[str]] = None

# def cf_set_gen() -> Set[str]:
#     """generate a list of all compounds families"""
#     global _cached_cf_set

#     if _cached_cf_set is not None:
#         return _cached_cf_set

#     db_session = get_db_session(pth.dpd_db_path)
#     cf_db = db_session.query(FamilyCompound).all()

#     cf_set: Set[str] = set()
#     for i in cf_db:
#         cf_set.add(i.compound_family)

#     _cached_cf_set = cf_set
#     return cf_set

</file>


<file path="tools/configger.py">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Modules for initializing, reading, writing, updating and testing
config.ini file."""

import configparser
from typing import Optional

from tools.printer import printer as pr

config = configparser.ConfigParser()
config.read("config.ini")

DEFAULT_CONFIG = {
    "version": {
        "version": "",
    },
    "regenerate": {
        "inflections": "yes",
        "transliterations": "yes",
        "freq_maps": "yes",
        "db_rebuild": "no",
    },
    "deconstructor": {
        "use_premade": "no",
    },
    "gui": {
        "theme": "DarkGrey10",
        "screen_fraction_width": "0.60",
        "screen_fraction_height": "1",
        "window_x": "0",
        "window_y": "0",
        "font_name": "Noto Sans",
        "font_size": "14",
        "input_text_color": "darkgray",
        "text_color": "#00bfff",
        "element_padding_x": "0",
        "element_padding_y": "0",
        "margin_x": "0",
        "margin_y": "0",
    },
    "goldendict": {"copy_unzip": "no", "path": ""},
    "dictionary": {
        "make_mdict": "yes",
        "show_id": "no",
        "data_limit": "0",
    },
    "exporter": {
        "make_dpd": "yes",
        "make_deconstructor": "no",
        "make_grammar": "no",
        "make_variants": "no",
        "make_tpr": "no",
        "make_ebook": "no",
        "make_tbw": "no",
        "make_pdf": "no",
        "make_abbrev": "no",
        "tarball_db": "no",
        "make_changelog": "no",
        "update_simsapa_db": "no",
    },
    "apis": {"openai": "", "deepseek": "", "gemini": "", "openrouter": ""},
    "anki": {"update": "no", "db_path": "", "backup_path": ""},
    "simsapa": {"app_path": "", "db_path": ""},
    "tpr": {"db_path": ""},
}


def config_initialize() -> None:
    """Initialize config.ini with default values."""
    for section, options in DEFAULT_CONFIG.items():
        if not config.has_section(section):
            config.add_section(section)
        for option, value in options.items():
            if not config.has_option(section, option):
                config.set(section, option, value)
    config_write()


def config_read(
    section: str, option: str, default_value: Optional[str] = None
) -> str | None:
    """Read config.ini. If error, return a specified default value"""
    try:
        return config.get(section, option)
    except (configparser.NoSectionError, configparser.NoOptionError):
        return default_value


def config_write() -> None:
    """Write config.ini."""
    with open("config.ini", "w") as file:
        config.write(file)


def config_update(section: str, option: str, value, silent=False) -> None:
    """Update config.ini with a new section, option & value."""
    if config.has_section(section):
        config.set(section, option, str(value))
    else:
        config.add_section(section)
        config.set(section, option, str(value))
    config_write()
    if not silent:
        pr.green_title(f"config updated: {section}: {option} --> {value}")


def config_test(section: str, option: str, value) -> bool:
    """Test config.ini to see if a section, option equals a value."""
    if config.has_section(section) and config.has_option(section, option):
        return config.get(section, option) == str(value)
    else:
        pr.red(f"unknown config setting: {section}: {option}")
        config_update_default_value(section, option)
        return config.get(section, option, fallback="") == str(value)


def config_update_default_value(section: str, option: str) -> None:
    """Update config.ini with a default value for a missing section or option."""
    if section in DEFAULT_CONFIG and option in DEFAULT_CONFIG[section]:
        default_value = DEFAULT_CONFIG[section].get(option)
        config_update(section, option, default_value)
    else:
        pr.red(f"missing default value for {section}: {option}")


def config_test_section(section):
    """Test config.ini to see if a section exists."""
    if config.has_section(section):
        return True
    else:
        return False


def config_test_option(section, option):
    """Test config.ini to see if a section, option exists."""
    if config.has_section(section):
        return config.has_option(section, option)
    else:
        return False


def print_config_settings(sections_to_print=None) -> None:
    """Print specified sections from config.ini or all if not specified."""
    if sections_to_print is None:
        sections_to_print = config.sections()
    for section in sections_to_print:
        if config.has_section(section):
            pr.info(f"[{section}]")
            for key, value in config.items(section):
                pr.info(f"{key} = {value}")


if __name__ == "__main__":
    config_initialize()

</file>


<file path="tools/css_manager.py">
# -*- coding: utf-8 -*-
"""
1. Create a Single Source of Truth for all CSS files.
    1. WebApp
    2. All GoldenDict exporters
    3. MkDocs

2. Dynamically update the headers of all exported dictionary with the correct variables.
"""

import re

from tools.paths import ProjectPaths


class CSSManager:
    def __init__(self):
        self.pth: ProjectPaths = ProjectPaths()
        self.dpd_css: str = self.pth.dpd_css_path.read_text()
        self.dpd_variables: str = self.pth.dpd_variables_css_path.read_text()
        self.dpd_fonts: str = self.pth.dpd_fonts_css_path.read_text()
        self.variables_reduced = self.dpd_variables

    def update_webapp_css(self) -> None:
        """The WebApp needs variables and dpd.css. No fonts in the CSS."""

        css_list: list[str] = []
        css_list.append(self.dpd_variables)
        css_list.append("")
        css_list.append(self.dpd_css)

        css_file = "\n".join(css_list)
        self.pth.webapp_css_path.write_text(css_file)

    def update_docs_css(self):
        """Save the CSS Variables to the docs folder."""

        self.pth.docs_css_variables_path.write_text(self.dpd_variables)

    def compile_css_and_fonts(self):
        """Compile CSS and fonts into one for GoldenDict exporters."""

        css_and_fonts_list = []
        css_and_fonts_list.append(self.dpd_fonts)
        css_and_fonts_list.append("")
        css_and_fonts_list.append(self.dpd_css)
        css_and_fonts_str = "\n".join(css_and_fonts_list)
        self.pth.dpd_css_and_fonts_path.write_text(css_and_fonts_str)

    def update_style(self, header: str, used_for: str):
        """
        Replace the style in exporter headers with fonts and variables.

        `used_for` values can be
        - `primary`: grammar_dict, spelling, epd
        - `secondary`: help and abbreviations
        - `dpd`: main dict
        - `root`: root dict
        - `variants`: variant dict
        """

        self.reduce_style(used_for)
        self.new_style = f"""
<style>
{self.variables_reduced}
"""
        # print(self.new_style)
        return header.replace("<style>", self.new_style)

    def reduce_style(self, used_for):
        """Based on `used_for`, strip away unnecessary variables"""

        if used_for == "primary":
            self._remove_all_except(["--primary"])
        if used_for == "secondary":
            self._remove_all_except(["--secondary"])
        if used_for == "dpd":
            self._remove_comments_and_whitespace()
            self._remove_only(["shade"])
        if used_for == "root":
            self._remove_all_except(["--primary", "--gray"])
        if used_for == "variants":
            self._remove_all_except(["--primary", "--gray"])
            self._remove_only(["--gray-light", "--gray-dark"])

    def _remove_all_except(self, variables: list[str]):
        """Remove all variables except the ones in the list."""

        lines = self.variables_reduced.splitlines()
        # leave lines with {} and the necessary variable
        lines = [
            line
            for line in lines
            if any(variable in line for variable in variables)
            or ("{" in line or "}" in line)
        ]
        self.variables_reduced = "\n".join(lines)

    def _remove_only(self, variables: list[str]):
        """Only remove the variables in the list."""

        lines = self.variables_reduced.splitlines()
        for variable in variables:
            # leave lines with {} and without necessary variable
            lines = [
                line
                for line in lines
                if variable not in line or ("{" in line or "}" in line)
            ]
        self.variables_reduced = "\n".join(lines)

    def _remove_comments_and_whitespace(self):
        "Strip away comments and empty lines."

        self.variables_reduced = re.sub(r"/\*[\s\S]*?\*/", "", self.variables_reduced)
        lines = self.variables_reduced.splitlines()
        lines = [line for line in lines if line.strip()]
        self.variables_reduced = "\n".join(lines)


if __name__ == "__main__":
    css_manager = CSSManager()
    css_manager.update_docs_css()
    css_manager.update_webapp_css()
    css_manager.compile_css_and_fonts()
    # new_style = css_manager.update_style("<style>", "secondary")
    # print(new_style)

</file>


<file path="tools/date_and_time.py">
"""Get basic day, date, time info."""

from datetime import datetime

now = datetime.now()


def year_month_day_hour_minute_dash():
    return now.strftime("%Y-%m-%d-%H-%M")


def year_month_day_dash():
    return now.strftime("%Y-%m-%d")


def year_month_day():
    return now.strftime("%Y%m%d")


def hour_minute():
    return now.strftime("%H:%M")


def day():
    return now.strftime("%d")


def make_timestamp() -> str:
    """Make current time iso-formatted UTC datetime string"""
    now = datetime.utcnow().replace(microsecond=0)
    return now.isoformat()

</file>


<file path="tools/degree_of_completion.py">
from db.models import DpdHeadword


def degree_of_completion(i: DpdHeadword, html=True):
    """
    Return plain or HTML styled symbol of a word data degree of completion.
    ✔ = complete = meaning_1 and source_1
    ◑ = half-complete = meaning_1 and no source_1
    ✘ = incomplete = no meaning_1
    """

    if i.meaning_1:
        if i.source_1:
            if html:
                return """<span class="gray">✔</span>"""
            else:
                return "✔"

        else:
            if html:
                return """<span class="gray">◑</span>"""
            else:
                return "◑"
    else:
        if html:
            return """<span class="gray">✘</span>"""
        else:
            return "✘"

</file>


<file path="tools/goldendict_exporter.py">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Generic GoldenDict exporter using pyglossary."""

import os
import shutil
from pathlib import Path
from subprocess import Popen
from typing import Optional
from zipfile import ZIP_DEFLATED, ZipFile

import idzip
from pyglossary import Glossary

from tools.date_and_time import make_timestamp
from tools.goldendict_path import make_goldendict_path
from tools.printer import printer as pr


class DictEntry:
    """Data for a single dictionary entry"""

    def __init__(self, word, definition_html, definition_plain, synonyms) -> None:
        self.word: str = word
        self.definition_html: str = definition_html
        self.definition_plain: str = definition_plain
        self.synonyms: list[str] = synonyms


class DictInfo:
    """Dictionary Information"""

    def __init__(
        self, bookname, author, description, website, source_lang, target_lang
    ) -> None:
        self.bookname: str = bookname
        self.author: str = author
        self.description: str = description
        self.website: str = website
        self.source_lang: str = source_lang
        self.target_lang: str = target_lang
        self.date = make_timestamp()


class DictVariables:
    """All relevant dictionary variables.
    Usage:
    dict_vars = DictVariables(
        css_path = css_path,
        js_paths = js_paths,
        gd_path = gd_path,
        md_path = md_path,
        dict_name = dict_name,
        icon_path = None,
        font_path = None
        zip_up = False,
        delete_original = False,
    """

    def __init__(
        self,
        css_paths: Optional[list[Path]],
        js_paths: Optional[list[Path]],
        gd_path: Path,
        md_path: Path,
        dict_name: str,
        icon_path: Optional[Path],
        font_path: Optional[Path] = None,
        zip_up: bool = False,
        delete_original: bool = False,
    ) -> None:
        self.css_paths: Optional[list[Path]] = css_paths
        self.js_paths: Optional[list[Path]] = js_paths

        self.gd_path: Path = gd_path.joinpath(dict_name)
        self.gd_name_name: Path = Path(dict_name).with_suffix(".ifo")
        self.gd_path_name: Path = self.gd_path.joinpath(dict_name).with_suffix(".ifo")

        self.dictfile = self.gd_path_name.with_suffix(".dict")
        self.dictfile_zip = self.gd_path_name.with_suffix(".dict.dz")

        self.synfile: Path = self.gd_path_name.with_suffix(".syn")
        self.synfile_zip: Path = self.synfile.with_suffix(".syn.dz")

        self.slob_path_name: Path = gd_path.joinpath(dict_name).with_suffix(".slob")

        self.mdict_mdx_path: Path = md_path.joinpath(f"{dict_name}-mdict").with_suffix(
            ".mdx"
        )
        self.mdict_mdd_path: Path = md_path.joinpath(f"{dict_name}-mdict").with_suffix(
            ".mdd"
        )
        self.icon_source_path: Optional[Path] = icon_path

        if icon_path:
            self.icon_target_path = self.gd_path_name.with_suffix(".ico")

        self.font_source_path = font_path
        if self.font_source_path:
            self.font_target_dir = self.gd_path_name

        if zip_up:
            self.zip_up = zip_up
        else:
            self.zip_up = False

        if delete_original:
            self.delete_original = delete_original
        else:
            self.delete_original = False

        if gd_path.samefile(md_path):
            self.gd_zip_path = gd_path.joinpath(f"{dict_name}-goldendict").with_suffix(
                ".zip"
            )
            self.md_zip_path = md_path.joinpath(f"{dict_name}-mdict").with_suffix(
                ".zip"
            )
        else:
            self.gd_zip_path = gd_path.joinpath(dict_name).with_suffix(".zip")
            self.md_zip_path = md_path.joinpath(dict_name).with_suffix(".zip")


def delete_old_directory(dict_var: DictVariables) -> bool:
    """Delete old dictionary directory if it exists."""
    pr.white("deleting old directory")
    if dict_var.gd_path.exists():
        try:
            shutil.rmtree(dict_var.gd_path)
            pr.yes("ok")
            return True
        except Exception as e:
            pr.no("error")
            pr.red(str(e))
            return False
    else:
        pr.yes("no")
        return True


def create_glossary(dict_info: DictInfo) -> Glossary:
    """Create Glossary."""

    pr.white("creating glossary")
    Glossary.init()
    glos = Glossary(
        info={
            "bookname": dict_info.bookname,
            "author": dict_info.author,
            "description": dict_info.description,
            "website": dict_info.website,
            "sourceLang": dict_info.source_lang,
            "targetLang": dict_info.target_lang,
            "date": dict_info.date,
        }
    )

    pr.yes("ok")
    return glos


def add_css(glos: Glossary, dict_var: DictVariables) -> Glossary:
    """Add CSS file."""

    pr.white("adding css")
    if dict_var.css_paths:
        for css_path in dict_var.css_paths:
            if css_path and css_path.exists():
                css = css_path.read_bytes()
                glos.addEntry(glos.newDataEntry(css_path.name, css))
        pr.yes("ok")
    else:
        pr.yes("no")
    return glos


def add_js(glos: Glossary, dict_var: DictVariables) -> Glossary:
    """Add JS file."""

    pr.white("adding js")
    if dict_var.js_paths:
        for js_path in dict_var.js_paths:
            if js_path and js_path.exists():
                js = js_path.read_bytes()
                glos.addEntry(glos.newDataEntry(js_path.name, js))
        pr.yes("ok")
    else:
        pr.yes("no")

    return glos


def add_fonts(glos: Glossary, dict_var: DictVariables) -> Glossary:
    """Add the fonts."""

    pr.white("adding fonts")
    if dict_var.font_source_path:
        for font_path in dict_var.font_source_path.iterdir():
            # Check if the file exists and has a valid font extension
            if (
                font_path
                and font_path.exists()
                and font_path.suffix.lower() in [".ttf", ".otf"]  # <-- Added check
            ):
                font_file = font_path.read_bytes()
                glos.addEntry(glos.newDataEntry(font_path.name, font_file))
        pr.yes("ok")
    else:
        pr.yes("no")

    return glos


def add_data(glos: Glossary, dict_data: list[DictEntry]) -> Glossary:
    """Add dictionary data to glossary."""

    pr.white("compiling data")
    for d in dict_data:
        glos.addEntry(
            glos.newEntry(
                word=[d.word] + d.synonyms, defi=d.definition_html, defiFormat="h"
            )  # type:ignore
        )

    pr.yes("ok")
    return glos


def write_to_file(glos: Glossary, dict_var: DictVariables) -> None:
    """Write output files."""

    pr.white("writing goldendict file")
    glos.write(
        filename=str(dict_var.gd_path_name),
        format="Stardict",
        # dictzip=True,
        dictzip=False,
        sametypesequence="h",
        sqlite=True,  # when False, more RAM but faster
    )
    pr.yes("ok")


def zip_dictfile(dict_var: DictVariables) -> None:
    """Compress .dict file into dictzip format using idzip."""

    pr.white("zipping .dict")

    try:
        with (
            open(dict_var.dictfile, "rb") as input_f,
            open(dict_var.dictfile_zip, "wb") as output_f,
        ):
            input_info = os.fstat(input_f.fileno())
            idzip.compressor.compress(  # type:ignore
                input_f,
                input_info.st_size,
                output_f,
                dict_var.dictfile.name,
                int(input_info.st_mtime),
            )
            dict_var.dictfile.unlink()
            pr.yes("ok")
    except FileNotFoundError:
        pr.no(f"error, {dict_var.dictfile} not found")


def zip_synfile(dict_var: DictVariables) -> None:
    """Compress .syn file into dictzip format"""

    pr.white("zipping synonyms")
    try:
        with (
            open(dict_var.synfile, "rb") as input_f,
            open(dict_var.synfile_zip, "wb") as output_f,
        ):
            input_info = os.fstat(input_f.fileno())
            idzip.compressor.compress(  # type:ignore
                input_f,
                input_info.st_size,
                output_f,
                dict_var.synfile.name,
                int(input_info.st_mtime),
            )
            dict_var.synfile.unlink()
            pr.yes("ok")
    except FileNotFoundError:
        pr.yes("no")
    except Exception:
        pr.no(f"error, {dict_var.synfile} not found")


def add_icon(v: DictVariables) -> None:
    """Copy the icon if provided."""

    pr.white("copying icon")
    if v.icon_source_path is not None:
        if v.icon_source_path.exists():
            try:
                Popen(["cp", v.icon_source_path, v.icon_target_path])
                pr.yes("ok")
            except Exception:
                pr.no("error")
    else:
        pr.yes("no")


def copy_dir(v: DictVariables) -> None:
    """Copy to Goldendict dir, cleaning up the destination first."""

    pr.white("copying to GoldenDict dir")
    goldendict_pth: Path | str = make_goldendict_path()
    if goldendict_pth:
        if goldendict_pth.exists():
            target_dir = goldendict_pth.joinpath(v.gd_path.name)

            # Delete old version in GoldenDict dir if it exists
            if target_dir.exists():
                try:
                    shutil.rmtree(target_dir)
                    # pr.yes("ok")
                except Exception as e:
                    pr.no("error")
                    pr.red(str(e))

            # Copy new version
            try:
                shutil.copytree(v.gd_path, target_dir)
                pr.yes("ok")
            except Exception as e:
                pr.no("error")
                pr.red(str(e))
    else:
        pr.yes("no goldendict path found")


def zip_folder(dict_var: DictVariables):
    """Zip up the gd and md files."""

    pr.white("zipping directory")
    with ZipFile(dict_var.gd_zip_path, "w", ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(dict_var.gd_path):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, dict_var.gd_path)
                zipf.write(file_path, relative_path)
    pr.yes("ok")


def delete_original(dict_var: DictVariables):
    """Delete the original output folder"""

    pr.white("deleting folder")
    try:
        shutil.rmtree(dict_var.gd_path)
        pr.yes("ok")
    except Exception as e:
        pr.no("error")
        pr.red(str(e))


def write_to_slob(glos: Glossary, dict_var: DictVariables) -> None:
    """Write to slob format files."""

    pr.white("writing slob file")
    glos.write(
        filename=str(dict_var.slob_path_name),
        format="Aard2Slob",
        compression="",  # "", "bz2", "zlib", "lzma2"
        content_type="text/html; charset=utf-8",
        word_title=True,
    )
    pr.yes("ok")


def export_to_goldendict_with_pyglossary(
    dict_info: DictInfo,
    dict_var: DictVariables,
    dict_data: list[DictEntry],
    include_slob=False,
) -> None:
    """Usage:
    export_to_goldendict_with_pyglossary(
        dict_info,
        dict_var,
        dict_data,
        include_slob = False,
    )
    """

    pr.green_title("exporting to goldendict with pyglossary")

    if not delete_old_directory(dict_var):
        return

    glos = create_glossary(dict_info)
    glos = add_css(glos, dict_var)
    glos = add_js(glos, dict_var)
    glos = add_fonts(glos, dict_var)
    glos = add_data(glos, dict_data)

    write_to_file(glos, dict_var)
    zip_dictfile(dict_var)
    zip_synfile(dict_var)
    add_icon(dict_var)
    copy_dir(dict_var)

    if dict_var.zip_up:
        zip_folder(dict_var)

    if dict_var.delete_original:
        delete_original(dict_var)

    if include_slob:
        write_to_slob(glos, dict_var)

</file>


<file path="tools/goldendict_path.py">
from pathlib import Path
from rich import print
from rich.prompt import Prompt


from tools.configger import config_test, config_test_option, config_update, config_read


def make_goldendict_path() -> Path:
    """Add a Goldendict path if one doesn't exist,
    or return the path if it does."""

    if config_test("goldendict", "copy_unzip", "yes"):
        if not config_test_option("goldendict", "path"):
            goldendict_path = Prompt.ask(
                "[yellow]Enter your GoldenDict directory (or ENTER for None)"
            )
            config_update("goldendict", "path", goldendict_path)
            return Path(goldendict_path)
        else:
            return Path(config_read("goldendict", "path"))


if __name__ == "__main__":
    print(Path(make_goldendict_path()))

</file>


<file path="tools/lemma_traditional.py">
#!/usr/bin/env python3

"""Function to provide traditional lemma endings."""

import re

from db.models import DpdHeadword
from tools.sinhala_tools import translit_ro_to_si

lemma_trad_dict: dict[str, str] = {
    "ant adj": "antu",  # like sīlavant
    "ant masc": "antu",  # like bhagavant
    "ar fem": "u",  # like dhītar
    "ar masc": "u",  # like satthar
    "ar2 masc": "u",  # like pitar
    "arahant masc": "arahanta",  # like arahant
    "as masc": "a",  # like manas
    "bhavant masc": "bhavantu",  # like bhavant
    "mātar fem": "mātu",  # like mātar
}


def find_space_digits(i: DpdHeadword) -> str:
    pattern = r"\s\d.*"
    match = re.search(pattern, i.lemma_1)
    if match:
        return match.group()
    else:
        return ""


def make_lemma_trad_clean(i: DpdHeadword) -> str:
    """Return a traditional noun or adj ending, rather than the DPD ending."""

    if (
        "!" not in i.stem  # only process lemmas, not inflected forms
        and i.pattern in lemma_trad_dict
    ):
        ending = lemma_trad_dict[i.pattern]
        lemma_trad_clean = f"{i.stem}{ending}".replace("!", "").replace("*", "")
        return lemma_trad_clean
    else:
        return i.lemma_clean


def make_lemma_trad(i: DpdHeadword) -> str:
    """Return a traditional noun or adj ending, rather than the DPD ending.
    no trailing number"""

    if (
        # only process lemmas, not inflected forms
        "!" not in i.stem and i.pattern in lemma_trad_dict
    ):
        space_digits = find_space_digits(i)
        ending = lemma_trad_dict[i.pattern]
        lemma_trad = f"{i.stem}{ending}{space_digits}".replace("!", "").replace("*", "")
        # print(f"{i.lemma_1:<40}{lemma_trad}")
        return lemma_trad
    else:
        return i.lemma_1


def make_lemma_trad_si(i: DpdHeadword) -> str:
    """Transcribe traditional lemma into Sinhala."""
    lemma = make_lemma_trad(i)
    return translit_ro_to_si(lemma)

</file>


<file path="tools/mdict_exporter.py">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Generic MDict exporter."""

from functools import reduce
from zipfile import ZIP_DEFLATED, ZipFile

from tools.goldendict_exporter import DictEntry, DictInfo, DictVariables
from tools.printer import printer as pr
from tools.writemdict.writemdict import MDictWriter


class GlobalVars:
    def __init__(
        self,
        dict_info: DictInfo,
        dict_var: DictVariables,
        dict_data: list[DictEntry],
        h3_header: bool,
    ) -> None:
        self.dict_info: DictInfo = dict_info
        self.dict_var: DictVariables = dict_var
        self.dict_data: list[DictEntry] = dict_data
        self.reduced_data: list
        self.needs_h3_header: bool = h3_header
        self.assets: list


def export_to_mdict(
    dict_info: DictInfo,
    dict_var: DictVariables,
    dict_data: list[DictEntry],
    h3_header=True,
) -> None:
    """Export to MDict"""

    pr.green_title("exporting to mdict")
    g = GlobalVars(dict_info, dict_var, dict_data, h3_header)

    replace_goldendict(g)

    if h3_header:
        add_h3_header(g)

    reduce_synonyms(g)
    write_mdx_file(g)
    compile_css_js_assets(g)
    write_mdd_file(g)

    if dict_var.zip_up:
        zip_files(g)

    if dict_var.delete_original:
        delete_original(g)


def replace_goldendict(g: GlobalVars) -> None:
    pr.white("adding 'mdict'")
    for i in g.dict_data:
        i.definition_html = i.definition_html.replace("GoldenDict", "MDict")
    pr.yes("ok")


def add_h3_header(g: GlobalVars) -> None:
    pr.white("adding h3 tag")
    for i in g.dict_data:
        i.definition_html = f"<h3>{i.word}</h3>{i.definition_html}"
    pr.yes("ok")


def reduce_synonyms(g: GlobalVars) -> None:
    pr.white("reducing synonyms")
    try:
        g.reduced_data = reduce(make_synonyms, g.dict_data, [])
        pr.yes("ok")
    except Exception as e:
        pr.no("error")
        pr.red(e)


def make_synonyms(all_items, item: DictEntry):
    all_items.append((item.word, item.definition_html))
    for word in item.synonyms:
        if word != item.word:
            all_items.append((word, f"""@@@LINK={item.word}"""))
    return all_items


def write_mdx_file(g: GlobalVars) -> None:
    pr.white("writing .mdx file")
    try:
        writer = MDictWriter(
            g.reduced_data,
            title=g.dict_info.bookname,
            description=g.dict_info.description,
        )
        with open(g.dict_var.mdict_mdx_path, "wb") as outfile:
            writer.write(outfile)
        pr.yes("ok")
    except Exception as e:
        pr.no("error")
        pr.red(e)


def compile_css_js_assets(g: GlobalVars) -> None:
    """Add CSS and JS and create a list with the format:
    (file_path, file_content_binary)"""

    pr.white("compiling css and js assets")

    try:
        g.assets = []
        file_list = g.dict_var.css_paths if g.dict_var.css_paths else []
        file_list += g.dict_var.js_paths if g.dict_var.js_paths else []

        for file in file_list:
            if file and file.is_file():
                with open(file, "rb") as f:
                    file_content = f.read()

                # mdd files expect the path to start with \ (windows) or /
                #  possible workaround (if this is a problem): <img src'../img.jpg'> and dont preppend a pathseparator
                file = f"\\{file.name}"

                # windows: goldendict will not display a linux path (path separator: /),
                #  but linux programs will display when path separator is \
                #  => transform all / to  \
                file = file.replace("/", r"\\")

                # Append the tuple to the list with either text or binary content
                g.assets.append((file, file_content))
        pr.yes("ok")

    except Exception as e:
        pr.no("error")
        pr.red(e)


def write_mdd_file(g: GlobalVars) -> None:
    pr.white("writing .mdd file")
    try:
        writer = MDictWriter(
            g.assets,
            title=g.dict_info.bookname,
            description=g.dict_info.description,
            is_mdd=True,
        )
        with open(g.dict_var.mdict_mdd_path, "wb") as f:
            writer.write(f)
        pr.yes("ok")

    except Exception as e:
        pr.no("error")
        pr.red(e)


def zip_files(g: GlobalVars) -> None:
    pr.white("zipping mdict files")
    try:
        with ZipFile(g.dict_var.md_zip_path, "w", ZIP_DEFLATED) as zipf:
            for file_path in [g.dict_var.mdict_mdd_path, g.dict_var.mdict_mdx_path]:
                zipf.write(file_path, file_path.name)
        pr.yes("ok")

    except Exception as e:
        pr.no("error")
        pr.red(e)


def delete_original(g: GlobalVars) -> None:
    """Delete the original output folder"""

    pr.white("deleting original files")
    try:
        g.dict_var.mdict_mdd_path.unlink()
        g.dict_var.mdict_mdx_path.unlink()
        pr.yes("ok")

    except Exception as e:
        pr.no("error")
        pr.red(e)

</file>


<file path="tools/meaning_construction.py">
"""Functions for:
1. Summarizing meaning and literal meaning,
2. Summarizing construction,
3. Cleaning construction of all brackets and phonetic changes,
4. Creating an HTML styled symbol of a word data's degree of completion."""

from pathlib import Path
import re

from db.db_helpers import get_db_session
from db.models import DpdHeadword


def make_meaning_combo(i: DpdHeadword) -> str:
    """Compile meaning_1 and literal meaning, or return meaning_2."""
    if i.meaning_1:
        meaning: str = i.meaning_1
        if i.meaning_lit:
            meaning += f"; lit. {i.meaning_lit}"
        return meaning
    elif i.meaning_2:
        return i.meaning_2
    else:
        return ""


def make_meaning_combo_html(i: DpdHeadword) -> str:
    """Compile meaning_1 in bold tags and meaning_lit,
    or return meaning_2 and meaning_lit."""

    if i.meaning_1:
        meaning: str = f"<b>{i.meaning_1}</b>"
        if i.meaning_lit:
            meaning += f"; lit. {i.meaning_lit}"
        return meaning
    else:
        if "; lit." in i.meaning_2:
            return i.meaning_2
        elif i.meaning_lit:
            return f"{i.meaning_2}; lit. {i.meaning_lit}"
        else:
            return i.meaning_2


def make_grammar_line(i: DpdHeadword) -> str:
    """Compile grammar line"""

    grammar = i.grammar
    if i.neg:
        grammar += f", {i.neg}"
    if i.verb:
        grammar += f", {i.verb}"
    if i.trans:
        grammar += f", {i.trans}"
    if i.plus_case:
        grammar += f" ({i.plus_case})"
    return grammar


def summarize_construction(i: DpdHeadword) -> str:
    """Create a summary of a word's construction,
    excluding brackets and phonetic changes."""

    if "<b>" in i.construction:
        i.construction = i.construction.replace("<b>", "").replace("</b>", "")

    # if no meaning then show root, word family or nothing
    if not i.meaning_1 and i.origin not in ["pass1", "pass2"]:
        if i.root_key:
            return i.family_root.replace(" ", " + ")
        elif i.family_word:
            return i.family_word
        else:
            return ""

    elif i.meaning_1 or (not i.meaning_1 and i.origin in ["pass1", "pass2"]):
        if not i.construction:
            return ""

        # clean construction
        # remove line2
        construction = re.sub(r"\n.+$", "", i.construction)
        # remove phonetic changes
        construction = re.sub("> .[^ ]*? ", "", construction)
        # remove phonetic changes at end
        construction = re.sub(" > .[^ ]*?$", "", construction)
        # remove brackets
        construction = construction.replace("(", "").replace(")", "")
        # remove [insertions]
        construction = re.sub(r"^\[.*\] \+| \[.*\] \+| \+ \[.*\]$", "", construction)

        if not i.root_base:
            if construction:
                return construction
            else:
                return ""

        else:
            # cleanup the base and base_construction
            # remove types
            base_clean = re.sub(" \\(.+\\)$", "", i.root_base)
            # remove base root + sign
            base = re.sub("(.+ )(.+?$)", "\\2", base_clean)
            # remove base
            base_construction = re.sub("(.+)( > .+?$)", "\\1", base_clean)
            # remove phonetic changes
            base_construction = re.sub(" >.*", "", base_construction)

            if i.pos != "fut":
                # replace base with root + sign
                root_sign_plus = i.root_sign.replace(" ", " + ")
                root_plus_sign = f"{i.root_clean} + {root_sign_plus}"
                construction = re.sub(base, root_plus_sign, construction)
            else:
                # replace base with base construction
                construction = re.sub(base, base_construction, construction)
            return construction
    else:
        return ""


def clean_construction(construction):
    """Clean construction of all brackets and phonetic changes."""
    # strip line 2
    construction = re.sub(r"\n.+", "", construction)
    # remove > ... +
    construction = re.sub(r" >.+?( \+)", "\\1", construction)
    # remove [] ... +
    construction = re.sub(r" \+ \[.+?( \+)", "\\1", construction)
    # remove [] at beginning
    construction = re.sub(r"^\[.+?( \+ )", "", construction)
    # remove [] at end
    construction = re.sub(r" \+ \[.*\]$", "", construction)
    # remove ??
    construction = re.sub("\\?\\? ", "", construction)
    return construction


if __name__ == "__main__":
    session = get_db_session(Path("dpd.db"))
    results = (
        session.query(DpdHeadword)
        .filter(DpdHeadword.root_sign.contains(" "))
        .limit(50)
        .all()
    )
    for i in results:
        print(i.lemma_1, ": ", summarize_construction(i))

</file>


<file path="tools/niggahitas.py">
"""Add all variants of niggahita character (ŋ ṁ) to a list."""

from typing import List


def add_niggahitas(words: List[str], all=True) -> List[str]:
    """Add various types of niggahitas (ŋ ṁ) to a list."""

    for word in words:
        if "ṃ" in word:
            words += [word.replace("ṃ", "ṁ")]
            if all == True:
                words += [word.replace("ṃ", "ŋ")]

    return list(set(words))

</file>


<file path="tools/pali_sort_key.py">
"""Functions for sorting by Pāḷi alphabetical order."""

import re

letter_to_number = {
    "√": "00",
    "a": "01",
    "ā": "02",
    "i": "03",
    "ī": "04",
    "u": "05",
    "ū": "06",
    "e": "07",
    "o": "08",
    "k": "09",
    "kh": "10",
    "g": "11",
    "gh": "12",
    "ṅ": "13",
    "c": "14",
    "ch": "15",
    "j": "16",
    "jh": "17",
    "ñ": "18",
    "ṭ": "19",
    "ṭh": "20",
    "ḍ": "21",
    "ḍh": "22",
    "ṇ": "23",
    "t": "24",
    "th": "25",
    "d": "26",
    "dh": "27",
    "n": "28",
    "p": "29",
    "ph": "30",
    "b": "31",
    "bh": "32",
    "m": "33",
    "y": "34",
    "r": "35",
    "l": "36",
    "v": "37",
    "s": "38",
    "h": "39",
    "ḷ": "40",
    "ṃ": "41",
}

sanksrit_letter_to_number = {
    "√": "00",
    "a": "01",
    "ā": "02",
    "i": "03",
    "ī": "04",
    "u": "05",
    "ū": "06",
    "ṛ": "07",
    "ṝ": "08",
    "ḷ": "09",
    "ḹ": "10",
    "e": "11",
    "ai": "12",
    "o": "13",
    "au": "14",
    "ḥ": "15",
    "ṃ": "16",
    "k": "17",
    "kh": "18",
    "g": "19",
    "gh": "20",
    "ṅ": "21",
    "c": "22",
    "ch": "23",
    "j": "24",
    "jh": "25",
    "ñ": "26",
    "ṭ": "27",
    "ṭh": "28",
    "ḍ": "29",
    "ḍh": "30",
    "ṇ": "31",
    "t": "32",
    "th": "33",
    "d": "34",
    "dh": "35",
    "n": "36",
    "p": "37",
    "ph": "38",
    "b": "39",
    "bh": "40",
    "m": "41",
    "y": "42",
    "r": "43",
    "l": "44",
    "v": "45",
    "ś": "46",
    "ṣ": "47",
    "s": "48",
    "h": "49",
}


def pali_list_sorter(words: list[str] | set[str]) -> list:
    """Sort a list or a set of words in Pāḷi alphabetical order.
    Usage:
    pali_list_sorter(list_of_pali_words)"""

    if isinstance(words, set):
        words = list(words)

    if words is None:
        return []

    else:
        pattern = "|".join(key for key in letter_to_number.keys())

        def replace(match):
            return letter_to_number[match.group(0)]

        sorted_words = sorted(words, key=lambda word: re.sub(pattern, replace, word))

        return sorted_words


def pali_sort_key(word: str) -> str:
    """A key for sorting in Pāḷi alphabetical order."
    Usage:
    list = sorted(list, key=pali_sort_key)
    db = sorted(db, key=lambda x: pali_sort_key(x.lemma_1))
    df.sort_values(
        by="lemma_1", inplace=True, ignore_index=True,
        key=lambda x: x.map(pali_sort_key))"""

    pattern = "|".join(re.escape(key) for key in letter_to_number.keys())

    def replace(match):
        return letter_to_number[match.group(0)]

    if isinstance(word, int):
        return word
    else:
        return re.sub(pattern, replace, word)


def sanskrit_sort_key(word: str) -> str:
    """A key for sorting in Sanskrit alphabetical order."
    Usage:
    list = sorted(list, key=pali_sort_key)
    db = sorted(db, key=lambda x: sanskrit_sort_key(x.lemma_1))
    df.sort_values(
        by="lemma_1", inplace=True, ignore_index=True,
        key=lambda x: x.map(sanskrit_sort_key))"""

    pattern = "|".join(re.escape(key) for key in sanksrit_letter_to_number.keys())

    def replace(match):
        return sanksrit_letter_to_number[match.group(0)]

    if isinstance(word, int):
        return word
    else:
        return re.sub(pattern, replace, word)

</file>


<file path="tools/paths.py">
"""All file paths that get used in the Project."""

import os
from pathlib import Path
from typing import Optional


class ProjectPaths:
    def __init__(self, base_dir: Optional[Path] = None, create_dirs=True):
        if base_dir is None:
            # The current working directory of the shell.
            base_dir = Path(os.path.abspath("."))

        # root
        self.dpd_db_path = base_dir / "dpd.db"
        self.pyproject_path = base_dir / "pyproject.toml"

        # backup_tsv
        self.pali_root_path = base_dir / "db/backup_tsv/dpd_roots.tsv"
        self.pali_word_path = base_dir / "db/backup_tsv/dpd_headwords.tsv"
        self.sutta_info_tsv_path = base_dir / "db/backup_tsv/sutta_info.tsv"

        # db/bold_definitions
        self.bold_definitions_json_path = (
            base_dir / "db/bold_definitions/bold_definitions.json"
        )
        self.bold_definitions_tsv_path = (
            base_dir / "db/bold_definitions/bold_definitions.tsv"
        )

        # db/suttas
        self.dv_catalogue_suttas_tsv_path = (
            base_dir / "db/suttas/dv_catalogue_suttas.tsv"
        )

        # audio
        self.dpd_audio_db_path = base_dir / "audio/db/dpd_audio.db"
        self.dpd_audio_mp3_dir = base_dir / "audio/mp3s"
        self.dpd_audio_male1_dir = (
            base_dir / "audio/mp3s/Kannada_kn-m4_Neutral_0.85"
        )
        self.dpd_audio_male2_dir = (
            base_dir / "audio/mp3s/Kannada_kn-m1_Neutral_0.85"
        )
        self.dpd_audio_female1_dir = (
            base_dir / "audio/mp3s/Kannada_kn-f4_Neutral_0.85"
        )

        # exporter/kindle/
        self.epub_dir = base_dir / "exporter/kindle/epub/"
        self.kindlegen_path = base_dir / "exporter/kindle/kindlegen"

        # exporter/kindle/epub
        self.epub_abbreviations_path = (
            base_dir / "exporter/kindle/epub/OEBPS/Text/abbreviations.xhtml"
        )
        self.epub_content_opf_path = base_dir / "exporter/kindle/epub/OEBPS/content.opf"
        self.epub_text_dir = base_dir / "exporter/kindle/epub/OEBPS/Text"
        self.epub_titlepage_path = (
            base_dir / "exporter/kindle/epub/OEBPS/Text/titlepage.xhtml"
        )

        # exporter/kindle/templates
        self.ebook_abbrev_entry_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_abbreviation_entry.html"
        )
        self.ebook_content_opf_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_content_opf.html"
        )
        self.ebook_deconstructor_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_deconstructor_entry.html"
        )
        self.ebook_entry_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_entry.html"
        )
        self.ebook_example_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_example.html"
        )
        self.ebook_grammar_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_grammar.html"
        )
        self.ebook_letter_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_letter.html"
        )
        self.ebook_title_page_templ_path = (
            base_dir / "exporter/kindle/templates/ebook_titlepage.html"
        )

        # shared_data/help/
        self.abbreviations_tsv_path = base_dir / "shared_data/help/abbreviations.tsv"
        self.bibliography_tsv_path = base_dir / "shared_data/help/bibliography.tsv"
        self.help_tsv_path = base_dir / "shared_data/help/help.tsv"
        self.thanks_tsv_path = base_dir / "shared_data/help/thanks.tsv"

        # exporter/goldendict/javascript/
        self.buttons_js_path = base_dir / "exporter/goldendict/javascript/buttons.js"
        self.family_compound_json = (
            base_dir / "exporter/goldendict/javascript/family_compound_json.js"
        )
        self.family_compound_template_js = (
            base_dir / "exporter/goldendict/javascript/family_compound_template.js"
        )
        self.family_idiom_json = (
            base_dir / "exporter/goldendict/javascript/family_idiom_json.js"
        )
        self.family_idiom_template_js = (
            base_dir / "exporter/goldendict/javascript/family_idiom_template.js"
        )
        self.family_root_json = (
            base_dir / "exporter/goldendict/javascript/family_root_json.js"
        )
        self.family_root_template_js = (
            base_dir / "exporter/goldendict/javascript/family_root_template.js"
        )
        self.family_set_json = (
            base_dir / "exporter/goldendict/javascript/family_set_json.js"
        )
        self.family_set_template_js = (
            base_dir / "exporter/goldendict/javascript/family_set_template.js"
        )
        self.family_word_json = (
            base_dir / "exporter/goldendict/javascript/family_word_json.js"
        )
        self.family_word_template_js = (
            base_dir / "exporter/goldendict/javascript/family_word_template.js"
        )
        self.feedback_template_js = (
            base_dir / "exporter/goldendict/javascript/feedback_template.js"
        )
        self.frequency_template_js = (
            base_dir / "exporter/goldendict/javascript/frequency_template.js"
        )
        self.main_js_path = base_dir / "exporter/goldendict/javascript/main.js"
        self.sorter_js_path = base_dir / "exporter/goldendict/javascript/sorter.js"

        # exporter/share
        self.dpd_deconstructor_goldendict_dir = (
            base_dir / "exporter/share/dpd-deconstructor/"
        )
        self.dpd_deconstructor_goldendict_dir2 = (
            base_dir / "exporter/share/dpd-deconstructor2/"
        )
        self.dpd_epub_path = base_dir / "exporter/share/dpd-kindle.epub"
        self.dpd_goldendict_dir = base_dir / "exporter/share/dpd/"
        self.dpd_goldendict_zip_path = base_dir / "exporter/share/dpd-goldendict.zip"
        self.dpd_grammar_goldendict_dir = base_dir / "exporter/share/dpd-grammar/"
        self.dpd_mdict_zip_path = base_dir / "exporter/share/dpd-mdict.zip"
        self.dpd_mobi_path = base_dir / "exporter/share/dpd-kindle.mobi"
        self.dpd_variants_goldendict_dir = base_dir / "exporter/share/dpd-variants/"
        self.share_dir = base_dir / "exporter/share"
        self.release_notes_md_path = base_dir / "exporter/share/release_notes.md"
        self.change_log_md_path = base_dir / "exporter/share/change_log.md"
        self.dpd_txt_path = base_dir / "exporter/share/dpd.txt"
        self.dpd_txt_zip_path = base_dir / "exporter/share/dpd-txt.zip"

        # exporter/share/mdict
        self.dpd_deconstructor_mdd_path = (
            base_dir / "exporter/share/dpd-deconstructor-mdict.mdd"
        )
        self.dpd_deconstructor_mdx_path = (
            base_dir / "exporter/share/dpd-deconstructor-mdict.mdx"
        )
        self.dpd_grammar_mdd_path = base_dir / "exporter/share/dpd-grammar-mdict.mdd"
        self.dpd_grammar_mdx_path = base_dir / "exporter/share/dpd-grammar-mdict.mdx"
        self.dpd_mdd_path = base_dir / "exporter/share/dpd-mdict.mdd"
        self.dpd_mdx_path = base_dir / "exporter/share/dpd-mdict.mdx"
        self.dpd_variants_mdd_path = base_dir / "exporter/share/dpd-variants-mdict.mdd"
        self.dpd_variants_mdx_path = base_dir / "exporter/share/dpd-variants-mdict.mdx"

        # exporter/deconstructor/templates
        self.deconstructor_header_templ_path = (
            base_dir / "exporter/deconstructor/deconstructor_header.html"
        )
        self.deconstructor_templ_path = (
            base_dir / "exporter/deconstructor/deconstructor.html"
        )

        # exporter/templates
        self.button_box_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_button_box.html"
        )
        self.dpd_definition_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_definition.html"
        )
        self.dpd_header_plain_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_header_plain.html"
        )
        self.dpd_header_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_header.html"
        )
        self.example_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_example.html"
        )
        self.family_compound_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_family_compound.html"
        )
        self.family_idiom_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_family_idiom.html"
        )
        self.family_root_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_family_root.html"
        )
        self.family_set_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_family_set.html"
        )
        self.family_word_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_family_word.html"
        )
        self.feedback_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_feedback.html"
        )
        self.frequency_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_frequency.html"
        )
        self.grammar_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_grammar.html"
        )
        self.inflection_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_inflection.html"
        )
        self.root_header_templ_path = (
            base_dir / "exporter/goldendict/templates/root_header.html"
        )
        self.spelling_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_spelling_mistake.html"
        )
        self.templates_dir = base_dir / "exporter/templates"
        self.variant_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_variant_reading.html"
        )
        self.sutta_info_templ_path = (
            base_dir / "exporter/goldendict/templates/dpd_sutta_info.html"
        )

        # FIXME delete these and whatever uses them
        # exporter/jinja templates
        self.complete_word_templ_path = (
            base_dir / "exporter/templates_jinja/dpd_complete_word.html"
        )
        self.jinja_templates_dir = base_dir / "exporter/templates_jinja/"
        self.temp_html_file_path = base_dir / "temp/temp_html_file.html"

        # exporter/goldendict/templates - root
        self.root_button_templ_path = (
            base_dir / "exporter/goldendict/templates/root_buttons.html"
        )
        self.root_definition_templ_path = (
            base_dir / "exporter/goldendict/templates/root_definition.html"
        )
        self.root_families_templ_path = (
            base_dir / "exporter/goldendict/templates/root_families.html"
        )
        self.root_info_templ_path = (
            base_dir / "exporter/goldendict/templates/root_info.html"
        )
        self.root_matrix_templ_path = (
            base_dir / "exporter/goldendict/templates/root_matrix.html"
        )

        # exporter/goldendict/templates - other
        self.abbrev_templ_path = (
            base_dir / "exporter/goldendict/templates/help_abbrev.html"
        )
        self.epd_templ_path = base_dir / "exporter/goldendict/templates/epd.html"
        self.help_templ_path = base_dir / "exporter/goldendict/templates/help_help.html"

        # exporter/tpr
        self.tpr_deconstructor_tsv_path = (
            base_dir / "exporter/tpr/output/deconstructor.tsv"
        )
        self.tpr_dpd_tsv_path = base_dir / "exporter/tpr/output/dpd.tsv"
        self.tpr_i2h_tsv_path = base_dir / "exporter/tpr/output/i2h.tsv"
        self.tpr_output_dir = base_dir / "exporter/tpr/output"
        self.tpr_sql_file_path = base_dir / "exporter/tpr/output/dpd.sql"

        # exporter/grammar_dict/

        self.grammar_dict_header_templ_path = (
            base_dir / "exporter/grammar_dict/grammar_dict_header.html"
        )

        # exporter/grammar_dict/output

        self.grammar_dict_output_dir = base_dir / "exporter/grammar_dict/output"
        self.grammar_dict_output_html_dir = (
            base_dir / "exporter/grammar_dict/output/html"
        )
        self.grammar_dict_pickle_path = (
            base_dir / "exporter/grammar_dict/output/grammar_dict_pickle"
        )
        self.grammar_dict_tsv_path = (
            base_dir / "exporter/grammar_dict/output/grammar_dict.tsv"
        )

        # exporter/other_dictionaries/css
        self.cone_css_path = base_dir / "exporter/other_dictionaries/code/cone/cone.css"
        self.dppn_css_path = (
            base_dir / "exporter/other_dictionaries/code/dppn/dppn.css/"
        )
        self.dpr_css_path = base_dir / "exporter/other_dictionaries/code/dpr/dpr.css/"
        self.whitney_css_path = (
            base_dir / "exporter/other_dictionaries/code/whitney/whitney.css/"
        )

        # exporter/other_dictionaries/source
        self.bhs_source_path = (
            base_dir / "exporter/other_dictionaries/code/bhs/source/bhs.xml"
        )
        self.cone_front_matter_path = (
            base_dir
            / "exporter/other_dictionaries/code/cone/source/cone_front_matter.json"
        )
        self.cone_source_path = (
            base_dir / "exporter/other_dictionaries/code/cone/source/cone_dict.json"
        )
        self.cpd_source_path = (
            base_dir / "exporter/other_dictionaries/code/cpd/source/en-critical.json"
        )
        self.dppn_source_path = (
            base_dir / "exporter/other_dictionaries/code/dppn/source/DPPN.json"
        )
        self.dpr_source_path = (
            base_dir / "exporter/other_dictionaries/code/dpr/source/dpr.json"
        )
        self.eng_sin_source_path = (
            base_dir
            / "exporter/other_dictionaries/code/sin_eng_sin/source/english-sinhala.tab"
        )
        self.mw_source_path = (
            base_dir / "exporter/other_dictionaries/code/mw/source/mw_from_simsapa.json"
        )
        self.peu_source_path = (
            base_dir / "exporter/other_dictionaries/code/peu/source/latest.json"
        )
        self.sin_eng_source_path = (
            base_dir
            / "exporter/other_dictionaries/code/sin_eng_sin/source/sinhala-english.tab"
        )
        self.vri_source_path = (
            base_dir / "exporter/other_dictionaries/code/vri/source/vri.csv"
        )
        self.whitney_source_dir = (
            base_dir / "exporter/other_dictionaries/code/whitney/source/"
        )

        # exporter/other_dictionaries/goldendict
        self.bhs_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.cone_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.cpd_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.dpr_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.mw_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.peu_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.simsapa_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.sin_eng_sin_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.vri_gd_path = base_dir / "exporter/other_dictionaries/goldendict/vri.zip"
        self.whitney_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"
        self.dppn_gd_path = base_dir / "exporter/other_dictionaries/goldendict/"

        # exporter/other_dictionaries/json
        self.bhs_json_path = base_dir / "exporter/other_dictionaries/json/bhs.json"
        self.cone_json_path = base_dir / "exporter/other_dictionaries/json/cone.json"
        self.cpd_json_path = base_dir / "exporter/other_dictionaries/json/cpd.json"
        self.dppn_json = base_dir / "exporter/other_dictionaries/json/dppn.json"
        self.dpr_json_path = base_dir / "exporter/other_dictionaries/json/dpr.json"
        self.mw_json_path = base_dir / "exporter/other_dictionaries/json/mw.json"
        self.peu_json_path = base_dir / "exporter/other_dictionaries/json/peu.json"
        self.simsapa_json_path = (
            base_dir / "exporter/other_dictionaries/json/simsapa.json"
        )
        self.sin_eng_sin_json_path = (
            base_dir / "exporter/other_dictionaries/json/sin_eng_sin.json"
        )
        self.vri_json_path = base_dir / "exporter/other_dictionaries/json/vri.json"
        self.whitney_json_path = (
            base_dir / "exporter/other_dictionaries/json/whitney.json"
        )

        # exporter/other_dictionaries/mdict
        self.bhs_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.cone_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.cpd_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.dppn_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.dpr_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.mw_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.peu_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.simsapa_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.sin_eng_sin_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"
        self.vri_mdict_path = base_dir / "exporter/other_dictionaries/mdict/vri.mdx"
        self.whitney_mdict_path = base_dir / "exporter/other_dictionaries/mdict/"

        # exporter/pdf
        self.typst_data_path = base_dir / "exporter/pdf/typst_data.typ"
        self.typst_pdf_path = base_dir / "exporter/share/dpd.pdf"
        self.typst_lite_data_path = base_dir / "exporter/pdf/typst_data_lite.typ"
        self.typst_lite_pdf_path = base_dir / "exporter/share/dpd.pdf"
        self.typst_lite_zip_path = base_dir / "exporter/share/dpd-pdf.zip"
        self.typst_lite_abbreviations_path = (
            base_dir / "exporter/share/abbreviations.pdf"
        )

        # exporter/variants
        self.variants_header_path = base_dir / "exporter/variants/variants_header.html"

        # exporter/webapp
        self.webapp_css_path = base_dir / "exporter/webapp/static/dpd.css"
        self.webapp_templates_dir = base_dir / "exporter/webapp/templates"
        self.webapp_static_dir = base_dir / "exporter/webapp/static"
        self.webapp_js_path = base_dir / "exporter/webapp/static/dpd.js"
        self.webapp_home_simple_css_path = (
            base_dir / "exporter/webapp/static/home_simple.css"
        )
        self.webapp_home_css_path = base_dir / "exporter/webapp/static/home.css"
        self.webapp_bold_definitions_js_path = (
            base_dir / "exporter/webapp/static/bold_definitions.js"
        )
        self.webapp_logo_svg_path = base_dir / "exporter/webapp/static/dpd-logo.svg"
        self.webapp_logo_dark_svg_path = (
            base_dir / "exporter/webapp/static/dpd-logo-dark.svg"
        )
        self.webapp_app_js_path = base_dir / "exporter/webapp/static/app.js"
        self.webapp_switch_css_path = base_dir / "exporter/webapp/static/switch.css"

        # webapp template names as constants
        self.template_dpd_summary = "dpd_summary.html"
        self.template_dpd_headword = "dpd_headword.html"
        self.template_root_summary = "root_summary.html"
        self.template_root = "root.html"
        self.template_abbreviations_summary = "abbreviations_summary.html"
        self.template_abbreviations = "abbreviations.html"
        self.template_deconstructor_summary = "deconstructor_summary.html"
        self.template_deconstructor = "deconstructor.html"
        self.template_grammar_summary = "grammar_summary.html"
        self.template_grammar = "grammar.html"
        self.template_help_summary = "help_summary.html"
        self.template_help = "help.html"
        self.template_epd_summary = "epd_summary.html"
        self.template_epd = "epd.html"
        self.template_variant_summary = "variant_summary.html"
        self.template_variant = "variant.html"
        self.template_spelling_summary = "spelling_summary.html"
        self.template_spelling = "spelling.html"

        # identity/
        self.dpd_css_path = base_dir / "identity/css/dpd.css"
        self.dpd_variables_css_path = base_dir / "identity/css/dpd-variables.css"
        self.dpd_fonts_css_path = base_dir / "identity/css/dpd-fonts.css"
        self.dpd_css_and_fonts_path = base_dir / "identity/css/dpd-css-and-fonts.css"

        # identity/logo
        self.dpd_logo_svg = base_dir / "identity/logo/dpd-logo.svg"
        self.dpd_logo_dark_svg = base_dir / "identity/logo/dpd-logo-dark.svg"
        self.dpd_logo_dark_bmp = base_dir / "identity/logo/dpd-logo-dark.bmp"

        # identity/fonts
        self.fonts_dir = base_dir / "identity/fonts"

        # gui
        self.pass2_checked_path = base_dir / "gui/pass2_checked.json"

        # gui/stash
        self.daily_record_path = base_dir / "gui/stash/daily_record"
        self.example_stash_path = base_dir / "gui/stash/example"
        self.save_state_path = base_dir / "gui/stash/gui_state"
        self.stash_dir = base_dir / "gui/stash/"
        self.stash_path = base_dir / "gui/stash/stash"

        # gui2
        self.load_example_dump = base_dir / "gui2/find_words_with_examples_dump.json"

        # db/inflections/
        self.inflection_templates_path = (
            base_dir / "db/inflections/inflection_templates.xlsx"
        )

        # resources/bw/js
        self.tbw_i2h_js_path = base_dir / "resources/bw2/js/dpd_i2h.js"
        self.tbw_dpd_ebts_js_path = base_dir / "resources/bw2/js/dpd_ebts.js"
        self.tbw_deconstructor_js_path = (
            base_dir / "resources/bw2/js/dpd_deconstructor.js"
        )

        # resources/fdg_dpd
        self.fdg_i2h_js_path = (
            base_dir / "resources/fdg_dpd/assets/standalone-dpd/dpd_i2h.js"
        )
        self.fdg_dpd_ebts_js_path = (
            base_dir / "resources/fdg_dpd/assets/standalone-dpd/dpd_ebts.js"
        )
        self.fdg_deconstructor_js_path = (
            base_dir / "resources/fdg_dpd/assets/standalone-dpd/dpd_deconstructor.js"
        )

        # resources/sc-data

        self.sc_data_dir = base_dir / "resources/sc-data/sc_bilara_data/root/pli/ms/"
        self.sc_variants_dir = (
            base_dir / "resources/sc-data/sc_bilara_data/variant/pli/ms/"
        )

        self.sc_pli2en_dpd_json = (
            base_dir / "resources/sc-data/dictionaries/simple/en/pli2en_dpd.json"
        )  # final dictionary format

        # FIXME part of the old sc exporter, delete when tested
        # self.sc_data_dpd_dir = base_dir / "resources/sc-data/dpd/"

        # self.sc_i2h_js_path = base_dir / "resources/sc-data/dpd/dpd_i2h.js"
        # self.sc_i2h_json_path = base_dir / "resources/sc-data/dpd/dpd_i2h.json"

        # self.sc_dpd_ebts_js_path = base_dir / "resources/sc-data/dpd/dpd_ebts.js"
        # self.sc_dpd_ebts_json_path = base_dir / "resources/sc-data/dpd/dpd_ebts.json"

        # self.sc_deconstructor_js_path = (
        #     base_dir / "resources/sc-data/dpd/dpd_deconstructor.js"
        # )
        # self.sc_deconstructor_json_path = (
        #     base_dir / "resources/sc-data/dpd/dpd_deconstructor.json"
        # )

        # resources/syāmaraṭṭha_1927
        self.sya_dir = base_dir / "resources/syāmaraṭṭha_1927/"

        # resources/dpd_submodules/cst
        self.cst_txt_dir = base_dir / "resources/dpd_submodules/cst/romn_txt/"
        self.cst_xml_dir = base_dir / "resources/dpd_submodules/cst/romn/"

        # resources/dpd_submodules/bjt
        self.bjt_dir = base_dir / "resources/dpd_submodules/bjt/public/static/"
        self.bjt_sinhala_dir = (
            base_dir / "resources/dpd_submodules/bjt/public/static/text/"
        )
        self.bjt_roman_json_dir = (
            base_dir / "resources/dpd_submodules/bjt/public/static/roman_json/"
        )
        self.bjt_roman_txt_dir = (
            base_dir / "resources/dpd_submodules/bjt/public/static/roman_txt/"
        )
        self.bjt_books_dir = (
            base_dir / "resources/dpd_submodules/bjt/public/static/books/"
        )

        # resources/other_pali_texts
        self.other_pali_texts_dir = base_dir / "resources/other_pali_texts"

        # resources/tipitaka_translation_db
        self.tipitaka_translation_db_dir = (
            base_dir / "resources/tipitaka_translation_db"
        )
        self.tipitaka_translation_db_path = (
            base_dir / "resources/tipitaka_translation_db/tipitaka-translation-data.db"
        )
        self.tipitaka_translation_db_tarball = (
            base_dir
            / "resources/tipitaka_translation_db/tipitaka-translation-data.db.zip"
        )

        # resources/tpr
        self.tpr_beta_path = (
            base_dir / "resources/tpr_downloads/release_zips/dpd_beta.zip"
        )
        self.tpr_download_list_path = (
            base_dir
            / "resources/tpr_downloads/download_source_files/download_list.json"
        )
        self.tpr_release_path = (
            base_dir / "resources/tpr_downloads/release_zips/dpd.zip"
        )

        # resources/deconstructor_output repo
        self.deconstructor_output_json = (
            base_dir / "resources/deconstructor_output/deconstructor_output.json"
        )
        self.deconstructor_output_tar_path = (
            base_dir / "resources/deconstructor_output/deconstructor_output.json.tar.gz"
        )
        self.deconstructor_output_dir = base_dir / "resources/deconstructor_output/"

        # docs
        self.mk_docs_yaml = base_dir / "mkdocs.yaml"
        self.docs_css_path = base_dir / "docs/stylesheets/extra.css"
        self.docs_css_variables_path = base_dir / "docs/stylesheets/dpd-variables.css"
        self.docs_dir = base_dir / "docs/"
        self.docs_bibliography_md_path = base_dir / "docs/bibliography.md"
        self.docs_abbreviations_md_path = base_dir / "docs/abbreviations.md"
        self.docs_changelog_md_path = base_dir / "docs/changelog.md"
        self.docs_thanks_md_path = base_dir / "docs/thanks.md"

        # shared_data/deconstructor
        self.decon_manual_corrections = (
            base_dir / "shared_data/deconstructor/manual_corrections.tsv"
        )
        self.decon_exceptions = base_dir / "shared_data/deconstructor/exceptions.tsv"
        self.decon_checked = base_dir / "shared_data/deconstructor/checked.csv"
        self.sandhi_rules_path = base_dir / "shared_data/deconstructor/sandhi_rules.tsv"
        self.spelling_mistakes_path = (
            base_dir / "shared_data/deconstructor/spelling_mistakes.tsv"
        )
        self.variant_readings_path = (
            base_dir / "shared_data/deconstructor/variant_readings.tsv"
        )

        # db/sanskrit
        self.root_families_sanskrit_path = (
            base_dir / "db/sanskrit/root_families_sanskrit.tsv"
        )

        # share
        self.changed_headwords_path = base_dir / "shared_data/changed_headwords"
        self.headword_stem_pattern_dict_path = (
            base_dir / "shared_data/headword_stem_pattern_dict"
        )
        self.inflection_templates_pickle_path = (
            base_dir / "shared_data/inflection_templates"
        )
        self.inflections_from_translit_json_path = (
            base_dir / "shared_data/inflections_from_translit.json"
        )
        self.inflections_to_translit_json_path = (
            base_dir / "shared_data/inflections_to_translit.json"
        )
        self.lookup_from_translit_path = (
            base_dir / "shared_data/lookup_from_translit.json"
        )
        self.lookup_to_translit_path = base_dir / "shared_data/lookup_to_translit.json"
        self.template_changed_path = base_dir / "shared_data/changed_templates"

        self.user_dict_path = base_dir / "shared_data/user_dictionary.txt"

        self.additions_tsv_path = base_dir / "shared_data/additions.tsv"
        self.additions_pickle_path = base_dir / "shared_data/additions"
        self.corrections_tsv_path = base_dir / "shared_data/corrections.tsv"
        # self.deleted_words_history_pth = base_dir / "shared_data/deleted_words_history.tsv"
        self.major_change_meaning_history_pth = (
            base_dir / "shared_data/major_change_meaning_history.tsv"
        )

        # share/frequency
        self.cst_file_freq = base_dir / "shared_data/frequency/cst_file_freq.json"
        self.cst_freq_json = base_dir / "shared_data/frequency/cst_freq.json"
        self.cst_wordlist = base_dir / "shared_data/frequency/cst_wordlist.json"

        self.bjt_file_freq = base_dir / "shared_data/frequency/bjt_file_freq.json"
        self.bjt_freq_json = base_dir / "shared_data/frequency/bjt_freq.json"
        self.bjt_wordlist = base_dir / "shared_data/frequency/bjt_wordlist.json"

        self.sya_file_freq = base_dir / "shared_data/frequency/sya_file_freq.json"
        self.sya_freq_json = base_dir / "shared_data/frequency/sya_freq.json"
        self.sya_wordlist = base_dir / "shared_data/frequency/sya_wordlist.json"

        self.sc_file_freq = base_dir / "shared_data/frequency/sc_file_freq.json"
        self.sc_freq_json = base_dir / "shared_data/frequency/sc_freq.json"
        self.sc_wordlist = base_dir / "shared_data/frequency/sc_wordlist.json"

        # temp
        self.temp_dir = base_dir / "temp/"

        # tools
        self.sandhi_contractions_path = base_dir / "tools/sandhi_contractions.json"
        self.hyphenations_dict_path = base_dir / "tools/hyphenations.json"
        self.uposatha_day_ini = base_dir / "tools/uposatha_day.ini"

        # db_tests/
        self.internal_tests_path = base_dir / "db_tests/db_tests_columns.tsv"

        # db_tests/single/
        self.antonym_dict_path = base_dir / "db_tests/single/test_antonyms.json"
        self.bahubbihi_dict_path = base_dir / "db_tests/single/test_bahubbihis.json"
        self.bold_example_path = base_dir / "db_tests/single/test_bold.json"
        self.compound_type_path = base_dir / "db_tests/single/add_compound_type.tsv"
        self.digu_json_path = base_dir / "db_tests/single/test_digu.json"
        self.hyphenations_dict_path_old = (
            base_dir / "db_tests/single/test_hyphenations.json"
        )
        self.hyphenations_scratchpad_path = (
            base_dir / "db_tests/single/test_hyphenations.txt"
        )
        self.idioms_exceptions_dict = base_dir / "db_tests/single/test_idioms.json"

        self.maha_exceptions_list = (
            base_dir / "db_tests/single/test_maha_exceptions.json"
        )

        self.neg_compound_exceptions = (
            base_dir / "db_tests/single/test_neg_compound_exceptions.json"
        )
        self.phonetic_changes_path = (
            base_dir / "db_tests/single/add_phonetic_changes.tsv"
        )
        self.phonetic_changes_vowels_path = (
            base_dir / "db_tests/single/add_phonetic_changes_vowels.tsv"
        )
        self.sukha_dukkha_finder_path = (
            base_dir / "db_tests/single/test_sukha_dukkha_finder.json"
        )
        self.syn_var_exceptions_old_path = (
            base_dir / "db_tests/single/add_synonym_variant_exceptions"
        )
        self.syn_var_exceptions_path = (
            base_dir / "db_tests/single/add_synonym_variant.json"
        )
        self.wf_exceptions_list = (
            base_dir / "db_tests/single/add_word_family_exceptions"
        )

        # db_tests_gui
        self.add_antonyms_sync_dict = (
            base_dir / "db_tests_gui/add_antonyms_sync_dict.json"
        )

        # .. external
        self.old_dpd_full_path = base_dir / "../csvs/dpd-full.csv"
        self.old_roots_csv_path = base_dir / "../csvs/roots.csv"

        # go_modules
        self.go_deconstructor_output_dir = base_dir / "go_modules/deconstructor/output"
        self.go_deconstructor_output_json = (
            base_dir / "go_modules/deconstructor/output/deconstructor_output.json"
        )

        if create_dirs:
            self.create_dirs()

    def create_dirs(self):
        for d in [
            self.bjt_books_dir,
            self.bjt_roman_json_dir,
            self.bjt_roman_txt_dir,
            self.cst_txt_dir,
            self.epub_text_dir,
            self.go_deconstructor_output_dir,
            self.grammar_dict_output_dir,
            self.grammar_dict_output_html_dir,
            self.share_dir,
            self.stash_dir,
            self.temp_dir,
            self.tpr_output_dir,
        ]:
            d.mkdir(parents=True, exist_ok=True)

</file>


<file path="tools/pos.py">
"""Categorizing parts of speech (pos) into various lists."""

INDECLINABLES = [
    "abbrev",
    "abs",
    "ger",
    "ind",
    "inf",
    "prefix",
    "sandhi",
    "suffix",
    "idiom",
    "var",
]

CONJUGATIONS = ["aor", "cond", "fut", "imp", "imperf", "opt", "perf", "pr"]

DECLENSIONS = [
    "adj",
    "card",
    "cs",
    "fem",
    "letter",
    "masc",
    "nt",
    "ordin",
    "pp",
    "pron",
    "prp",
    "ptp",
    "root",
    "ve",
]

POS = [
    "abbrev",
    "abs",
    "adj",
    "aor",
    "card",
    "cond",
    "cs",
    "fem",
    "fut",
    "ger",
    "idiom",
    "imp",
    "imperf",
    "ind",
    "inf",
    "letter",
    "masc",
    "nt",
    "opt",
    "ordin",
    "perf",
    "pp",
    "pr",
    "prefix",
    "pron",
    "prp",
    "ptp",
    "root",
    "sandhi",
    "suffix",
    "ve",
]

VERBS = [
    "abs",
    "aor",
    "cond",
    "fut",
    "ger",
    "imp",
    "imperf",
    "inf",
    "opt",
    "perf",
    "pp",
    "pr",
    "prp",
    "ptp",
]

PARTICIPLES = [
    "pp",
    "prp",
    "ptp",
]

NOUNS = [
    "fem",
    "masc",
    "nt",
]

EXCLUDE_FROM_FREQ: set = {
    "abbrev",
    "cs",
    "idiom",
    "letter",
    "prefix",
    "root",
    "suffix",
    "ve",
}

</file>


<file path="tools/printer.py">
"""Utilities for colored console output with timing and structured TSV logging."""

import logging
from datetime import datetime
import time
from pathlib import Path
from rich import print
from typing import Union, Optional, ClassVar


class TSVFormatter(logging.Formatter):
    """Format log records as TSV with headers."""

    FIELDS = [
        "timestamp",
        "level",
        "operation",
        "type",  # title, status, success, error, etc
        "message",  # original message
        "elapsed",  # timing from bop()
        "count",  # for numerical outputs
        "session",  # to group related operations
    ]

    def __init__(self):
        super().__init__()
        self.header_written = False

    def format(self, record):
        # Write header on first use
        if not self.header_written:
            self.header_written = True
            return "\t".join(self.FIELDS)

        # Add extra fields to record
        for k, v in record.__dict__.get("extra", {}).items():
            setattr(record, k, v)

        # Extract values for each field
        values = []
        for field in self.FIELDS:
            if field == "timestamp":
                values.append(self.formatTime(record))
            elif field == "level":
                values.append(record.levelname)
            elif field == "message":
                values.append(record.getMessage())
            else:
                # Get fields from record attributes
                values.append(str(getattr(record, field, "")))

        return "\t".join(values)


class Printer:
    """Colored console output with timing and TSV logging."""

    def __init__(self, log_file: Optional[Path] = None):
        self.line = "-" * 40
        self.start_time: float | None = None
        self.session = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Set up logging if log_file provided
        self.logger = None
        if log_file:
            self.logger = logging.getLogger("dpd")
            self.logger.setLevel(logging.INFO)

            # File handler with TSV formatting
            handler = logging.FileHandler(log_file)
            handler.setFormatter(TSVFormatter())
            self.logger.addHandler(handler)

    def _log(self, level: int, operation: str, msg: str, **kwargs) -> None:
        """Log with additional context if logging is enabled."""
        if self.logger:
            # Create dictionary of extra fields
            extra = {"operation": operation, "session": self.session}

            # Add timing if available
            if self.start_time is not None:
                extra["elapsed"] = self.bop()

            # Add any additional kwargs
            extra.update(kwargs)

            # Log with extra fields
            self.logger.log(level, msg, extra=extra)

    # Class variable for main timer
    _ticx: ClassVar[datetime | None] = None

    @classmethod
    def tic(cls) -> None:
        """Start the main clock."""
        cls._ticx = datetime.now()

    @classmethod
    def toc(cls) -> None:
        """Stop the main clock and print elapsed time."""
        if cls._ticx is None:
            print("[red]Error: tic() not called before toc()[/red]")
            return

        tocx = datetime.now()
        tictoc = tocx - cls._ticx
        print("[cyan]" + ("-" * 40))
        print(f"[cyan]{tictoc}")
        print()

    def bip(self) -> None:
        """Start a mini clock."""
        self.start_time = time.time()

    def bop(self) -> str:
        """End mini clock and return elapsed time."""
        if self.start_time is None:
            return "0.000"
        elapsed_time = time.time() - self.start_time
        return f"{elapsed_time:.3f}"

    def print_bop(self) -> None:
        """Print the elapsed time right-aligned."""
        print(f"{self.bop():>10}")

    # Printer methods
    def title(self, text: str) -> None:
        """Print bright yellow title and start timer."""
        print(f"[bright_yellow]{text}")
        self.bip()
        self._log(logging.INFO, "title", text, type="title")

    def green_title(self, message: str) -> None:
        """Print green title and start timer."""
        print(f"[green]{message}")
        self.bip()
        self._log(logging.INFO, "green_title", message, type="title")

    def green(self, message: str) -> None:
        """Print left-aligned green message and start timer."""
        print(f"[green]{message:<35}", end="")
        self.bip()
        self._log(logging.INFO, "status", message, type="status")

    def cyan(self, message: str) -> None:
        """Print left-aligned cyan message and start timer."""
        print(f"[cyan]{message:<35}", end="")
        self.bip()
        self._log(logging.INFO, "status", message, type="status")

    def white(self, message: str) -> None:
        """Print indented white message and start timer."""
        print(f"{'':<5}[white]{message:<30}", end="")
        self.bip()
        self._log(logging.INFO, "info", message, type="info")

    def yes(self, message: Union[int, str]) -> None:
        """Print right-aligned blue message with timing."""
        if isinstance(message, int):
            formatted = f"{message:>10,}"
            self._log(
                logging.INFO, "success", str(message), type="success", count=message
            )
        else:
            formatted = f"{message:>10}"
            self._log(logging.INFO, "success", message, type="success")
        print(f"[blue]{formatted}", end="")
        self.print_bop()

    def no(self, message: Union[int, str]) -> None:
        """Print right-aligned red message with timing."""
        print(f"[red]{message:>10}", end="")
        self.print_bop()
        self._log(logging.WARNING, "failure", str(message), type="failure")

    def red(self, message: str) -> None:
        """Print red message."""
        print(f"[red]{message}")
        self._log(logging.ERROR, "error", message, type="error")

    def counter(self, counter: int, total: int, word: str) -> None:
        """Print progress counter with timing."""
        print(f"{counter:>10,} / {total:<10,} {word[:20]:<20} {self.bop():>10}")
        self.bip()
        self._log(
            logging.INFO, "counter", word, type="progress", count=counter, total=total
        )

    def summary(self, key: str, value: str | int) -> None:
        """Print key-value summary in green."""
        print(f"[green]{key:<20}[/green]{value}")
        self._log(logging.INFO, "summary", f"{key}: {value}", type="summary", key=key)

    # basic logging messages

    def info(self, message: str) -> None:
        """Print green message."""

        print(f"[green]{message}")
        self._log(logging.INFO, "info", message, type="info")

    def warning(self, message: str) -> None:
        """Print amber message."""

        print(f"[yellow]{message}")
        self._log(logging.WARNING, "warning", message, type="warning")

    def error(self, message: str) -> None:
        """Print red message."""

        print(f"[red]{message}")
        self._log(logging.ERROR, "error", message, type="error")


# Create singleton instance with optional log file
printer = Printer(Path("dpd_operations.log"))

</file>


<file path="tools/sinhala_tools.py">
from aksharamukha import transliterate

pos_dict = {
    "letter": {"pos_si": "අ", "pos_si_full": "අකුර"},
    "prefix": {"pos_si": "උප", "pos_si_full": "උපසර්ග"},
    "cs": {"pos_si": "විප්‍ර", "pos_si_full": "විකරණ ප්‍රත්‍යය"},
    "abbrev": {"pos_si": "කෙටියෙ", "pos_si_full": "කෙටි යෙදුම්"},
    "adj": {"pos_si": "විශේ", "pos_si_full": "විශෙෂණ පද"},
    "pron": {"pos_si": "සර්", "pos_si_full": "සර්වනාම"},
    "ptp": {"pos_si": "කි", "pos_si_full": "කිතක/කිච්ච ප්‍රත්‍යය"},
    "pp": {"pos_si": "අකෘ", "pos_si_full": "අතීත කෘදන්ත"},
    "masc": {"pos_si": "පු", "pos_si_full": "පුල්ලිංග"},
    "aor": {"pos_si": "අතී", "pos_si_full": "අජ්ජතනී, අතීත කාළ"},
    "nt": {"pos_si": "න", "pos_si_full": "නපුංසක ලිංග"},
    "fem": {"pos_si": "ඉ", "pos_si_full": "ඉත්ථි ලිංග"},
    "ind": {"pos_si": "නි", "pos_si_full": "නිපාත. අව්‍ය පද"},
    "prp": {"pos_si": "මික්‍රි", "pos_si_full": "මිශ්‍ර ක්‍රියා"},
    "abs": {"pos_si": "පූ", "pos_si_full": "පූර්ව ක්‍රියා"},
    "cond": {"pos_si": "කා", "pos_si_full": "කාලාතිපත්ති,සත්තමී"},
    "imperf": {"pos_si": "අකා", "pos_si_full": "අතීත කාළ/හීයත්තනී"},
    "sandhi": {"pos_si": "සන්ධි", "pos_si_full": "සන්ධි"},
    "idiom": {"pos_si": "භා", "pos_si_full": "භාෂා රීතියට අනුගත පද"},
    "pr": {"pos_si": "වකා", "pos_si_full": "වර්තමාන කාළ"},
    "inf": {"pos_si": "අව්‍ය", "pos_si_full": "තුමන්ත,අව්‍ය"},
    "ger": {"pos_si": "භාව", "pos_si_full": "අව්‍යය, භාව පද"},
    "card": {"pos_si": "සං", "pos_si_full": "සංඛ්‍යා ශබ්ද"},
    "ordin": {"pos_si": "සං.පූ", "pos_si_full": "සංඛ්‍යා පූරණ ශබ්ද"},
    "imp": {"pos_si": "වික්‍රි", "pos_si_full": "පඤ්චමී/විධි ක්‍රියා"},
    "opt": {"pos_si": "සත්", "pos_si_full": "ඉච්ඡිතාර්ථය,සත්තමී"},
    "ve": {"pos_si": "වාච්‍ය", "pos_si_full": "වාච්‍ය"},
    "root": {"pos_si": "ධාතුව", "pos_si_full": "ධාතුව"},
    "suffix": {"pos_si": "ප්‍රත්‍ය", "pos_si_full": "ප්‍රත්‍ය"},
    "perf": {"pos_si": "පරො", "pos_si_full": "පරොක්ඛා "},
    "fut": {"pos_si": "අකා", "pos_si_full": "අනාගත කාළ"},
}


def pos_si(pos: str):
    return pos_dict[pos]["pos_si"]


def pos_si_full(pos: str):
    return pos_dict[pos]["pos_si_full"]


def translit_ro_to_si(text: str) -> str:
    return transliterate.process(
        "IASTPali",
        "Sinhala",
        text,
        post_options=["SinhalaPali", "SinhalaConjuncts"],
    )  # type:ignore


def translit_si_to_ro(text: str) -> str:
    text_translit = transliterate.process(
        "Sinhala",
        "IASTPali",
        text,
        post_options=["SinhalaPali", "SinhalaConjuncts"],
    )  # type:ignore

    text_translit = (
        text_translit.replace("ï", "i")
        .replace("ü", "u")
        .replace("ĕ", "e")
        .replace("ŏ", "o")
    )

    return text_translit


gram_dict = {
    "nom": "පඨමා",
    "acc": "දුතියා",
    "instr": "තතියා",
    "dat": "චතුත්ථී",
    "abl": "පඤ්චමී",
    "gen": "ඡට්ඨි",
    "loc": "සත්තමී",
    "voc": "ආලපන",
    "In comps": "සන්ධිවල",
    "fem sg": "ඉ ඒක",
    "fem pl": "ඉ බහු",
    "masc sg": "පු ඒක",
    "masc pl": "පු බහු",
    "neut sg": "න ඒක",
    "neut pl": "න බහු",
    "declension, conjugation": "වරනැගීම",
    "sg": "ඒක",
    "pl ": "බහු",
    "reflexive sg": "අත්තනො ඒක",
    "reflexive pl": "අත්තනො බහු",
    "pr  3rd": "වත් පඨ",
    "pr 2nd": "වත් මජ්",
    "pr 1st": "වත් උත්",
    "imp 3rd": "පඤ් පඨ",
    "imp 2nd": "පඤ් මජ්",
    "imp 1st": "පඤ් උත්",
    "opt 3rd": "සත් පඨ",
    "opt 2nd": "සත් මජ්",
    "opt 1st": "සත් උත්",
    "fut 3rd": "භවි පඨ",
    "fut 2nd": "භවි මජ්",
    "fut 1st": "භවි උත්",
    "aor 3rd": "අජ් පඨ",
    "aor 2nd": "අජ් මජ්",
    "aor 1st": "අජ් උත්",
    "pref 3rd": "පරො පඨ",
    "pref 2nd": "පරො මජ්",
    "pref 1st": "පරො උත්",
    "cond 3rd": "කාලාති පඨ",
    "cond 2nd": "කාලාති මජ්",
    "cond 1st": "කාලාති උත්",
    "Imperf 3rd": "හීය පඨ",
    "Imperf 2nd": "හීය මජ්",
    "Imperf 1st": "හීය උත්",
    "1st sg": "උත් ඒක ",
    "1st pl": "උත් බහු",
    "pron": "සර්",
    "subject": "කර්තෘ",
    "object": "කර්මය",
    "2nd sg": "මජ් ඒක",
    "2nd pl": "මජ් බහු",
    "3rd sg": "පඨ ඒක ",
    "3rd pl": "පඨ බහු",
    "Inflections not found in the Chaṭṭha Saṅgāyana corpus, or within processed sandhi compounds are grayed out. They might still occur elsewhere, within compounds or in other versions of the Pāḷi texts.": " ඡට්ඨ සංගායනා ත්‍රිපිටකයෙහි පද හෝ සන්ධි පද තුළ දක්නට නොලැබෙන  පද අළු පැහැයෙන් යුක්ත වේ. ඒවා  වෙනත්  පොත්වල හෝ සන්ධි තුළ හෝ වෙනත් ත්‍රිපිටක අනුවාදවල තිබිය හැක.",
    "Did you spot a mistake in the declension table? Something missing? Report it here.": "වරනැගීම් වගුවේ වැරැද්දක් ඔබ දුටුවාද? යමක් අඩු වී තිබේද? එය මෙතනින් වාර්තා කරන්න.",
    "Abbreviation": "කෙටි යෙදුම",
    "Meaning": "තේරුම",
    "Pāḷi": "පාලි",
    "Example": "උදාහරණ",
    "Information": "තොරතුරු",
    "Nominative case": "පඨමා විභත්ති",
    "Paṭhamā, paccattavacana": "පඨමා, පච්චත්තවචන",
    "The category of nouns serving as the grammatical subject of a verb": "ක්‍රියා පදයට විෂය වන පදය හෙවත් උක්ත පද කාණ්ඩය",
    "Present tense": "වර්තමාන කාලය",
    "past tense": "අතීත කාලය",
    "future tense": "අනාගත කාලය",
    "A verb tense that expresses actions or states at the time of speaking (e.g. lives; appears; sees)": "මේ මොහොතේදී සිදුවන ක්‍රියාවක් හෝ සිදුවීමක් ප්‍රකාශ කරන ක්‍රියා පදය (උදා: ජීවත්වෙයි; දිස්වේ; දකී)",
    "Aorist verb": "අතීත කාල ක්‍රියාව",
    "A form of a verb that, in the indicative mood, expresses past action. (e.g. was; sat down; arose)": "ක්‍රියා පදයක ආකාරයක්, අතීත ක්‍රියාව ප්‍රකාශ කරයි. (උදා. විය, වාඩිවිය, පැන නැගුණි)",
    "Accusative case": "දුතියා විභත්ති",
    "The object of the sentence (e.g. me)": "වාක්‍යයේ කර්මය (උදා: මම)",
    "Dutiyā, upayogavacana, kammavacana": "දුතියා, උපයොගවචන, කම්මවචන",
}


def si_grammar(text) -> str:
    if text in gram_dict:
        return gram_dict[text]
    else:
        return text

</file>


<file path="tools/sutta_codes.py">
""" "Return a list of sutta codes"""

import re

from db.models import SuttaInfo
from tools.pali_sort_key import pali_list_sorter


def generate_range_of_sutta_codes(code_with_dash: str) -> list[str]:
    if "." in code_with_dash:
        # find the base of the code, 'SN12.' in 'SN12.5-8'
        code_base = re.sub(r"(?<=\.).+", "", code_with_dash)
    else:
        # if no '.' e.g. DHP1-20 return early
        return []

    # find the part with dashes, '5-8.' in 'SN12.5-8'
    code_dash = re.sub(code_base, "", code_with_dash)

    # find the first digit, 5 in 'SN12.5-8'
    code_first = int(re.sub(r"-.+", "", code_dash))

    # find the last digit, 8 in 'SN12.5-8'
    code_last = int(re.sub(r".+-", "", code_dash))

    sutta_code_list = []
    for num in range(code_first, code_last + 1):
        sutta_code_list.append(f"{code_base}{num}")
    return sutta_code_list


def make_list_of_sutta_codes(su: SuttaInfo) -> list[str]:
    sutta_codes_set: set[str] = set()
    sutta_codes_set.add(su.dpd_code)
    if "-" in su.dpd_code:
        sutta_codes_set.update(generate_range_of_sutta_codes(su.dpd_code))
    sutta_codes_set.add(su.sc_code)
    if "-" in su.sc_code:
        sutta_codes_set.update(generate_range_of_sutta_codes(su.sc_code))

    return pali_list_sorter(sutta_codes_set)


if __name__ == "__main__":
    su = SuttaInfo()
    su.dpd_code = "SN12.34-47"
    su.sc_code = "SN12.33-46"
    results = make_list_of_sutta_codes(su)
    print(results)

</file>


<file path="tools/writemdict/pureSalsa20.py">
#!/usr/bin/env python
# coding: utf-8

"""
    pureSalsa20.py -- a pure Python implementation of the Salsa20 cipher, ported to Python 3

    v4.0: Added Python 3 support, dropped support for Python <= 2.5.
    
    // zhansliu

    Original comments below.

    ====================================================================
    There are comments here by two authors about three pieces of software:
        comments by Larry Bugbee about
            Salsa20, the stream cipher by Daniel J. Bernstein 
                 (including comments about the speed of the C version) and
            pySalsa20, Bugbee's own Python wrapper for salsa20.c
                 (including some references), and
        comments by Steve Witham about
            pureSalsa20, Witham's pure Python 2.5 implementation of Salsa20,
                which follows pySalsa20's API, and is in this file.

    Salsa20: a Fast Streaming Cipher (comments by Larry Bugbee)
    -----------------------------------------------------------

    Salsa20 is a fast stream cipher written by Daniel Bernstein 
    that basically uses a hash function and XOR making for fast 
    encryption.  (Decryption uses the same function.)  Salsa20 
    is simple and quick.  
    
    Some Salsa20 parameter values...
        design strength    128 bits
        key length         128 or 256 bits, exactly
        IV, aka nonce      64 bits, always
        chunk size         must be in multiples of 64 bytes
    
    Salsa20 has two reduced versions, 8 and 12 rounds each.
    
    One benchmark (10 MB):
        1.5GHz PPC G4     102/97/89 MB/sec for 8/12/20 rounds
        AMD Athlon 2500+   77/67/53 MB/sec for 8/12/20 rounds
          (no I/O and before Python GC kicks in)
    
    Salsa20 is a Phase 3 finalist in the EU eSTREAM competition 
    and appears to be one of the fastest ciphers.  It is well 
    documented so I will not attempt any injustice here.  Please 
    see "References" below.
    
    ...and Salsa20 is "free for any use".  
    
    
    pySalsa20: a Python wrapper for Salsa20 (Comments by Larry Bugbee)
    ------------------------------------------------------------------

    pySalsa20.py is a simple ctypes Python wrapper.  Salsa20 is 
    as it's name implies, 20 rounds, but there are two reduced 
    versions, 8 and 12 rounds each.  Because the APIs are 
    identical, pySalsa20 is capable of wrapping all three 
    versions (number of rounds hardcoded), including a special 
    version that allows you to set the number of rounds with a 
    set_rounds() function.  Compile the version of your choice 
    as a shared library (not as a Python extension), name and 
    install it as libsalsa20.so.
    
    Sample usage:
        from pySalsa20 import Salsa20
        s20 = Salsa20(key, IV)
        dataout = s20.encryptBytes(datain)   # same for decrypt
    
    This is EXPERIMENTAL software and intended for educational 
    purposes only.  To make experimentation less cumbersome, 
    pySalsa20 is also free for any use.      
    
    THIS PROGRAM IS PROVIDED WITHOUT WARRANTY OR GUARANTEE OF
    ANY KIND.  USE AT YOUR OWN RISK.  
    
    Enjoy,
      
    Larry Bugbee
    bugbee@seanet.com
    April 2007

    
    References:
    -----------
      http://en.wikipedia.org/wiki/Salsa20
      http://en.wikipedia.org/wiki/Daniel_Bernstein
      http://cr.yp.to/djb.html
      http://www.ecrypt.eu.org/stream/salsa20p3.html
      http://www.ecrypt.eu.org/stream/p3ciphers/salsa20/salsa20_p3source.zip

     
    Prerequisites for pySalsa20:
    ----------------------------
      - Python 2.5 (haven't tested in 2.4)


    pureSalsa20: Salsa20 in pure Python 2.5 (comments by Steve Witham)
    ------------------------------------------------------------------

    pureSalsa20 is the stand-alone Python code in this file.
    It implements the underlying Salsa20 core algorithm
    and emulates pySalsa20's Salsa20 class API (minus a bug(*)).

    pureSalsa20 is MUCH slower than libsalsa20.so wrapped with pySalsa20--
    about 1/1000 the speed for Salsa20/20 and 1/500 the speed for Salsa20/8,
    when encrypting 64k-byte blocks on my computer.

    pureSalsa20 is for cases where portability is much more important than
    speed.  I wrote it for use in a "structured" random number generator.

    There are comments about the reasons for this slowness in
          http://www.tiac.net/~sw/2010/02/PureSalsa20

    Sample usage:
        from pureSalsa20 import Salsa20
        s20 = Salsa20(key, IV)
        dataout = s20.encryptBytes(datain)   # same for decrypt

    I took the test code from pySalsa20, added a bunch of tests including
    rough speed tests, and moved them into the file testSalsa20.py.  
    To test both pySalsa20 and pureSalsa20, type
        python testSalsa20.py

    (*)The bug (?) in pySalsa20 is this.  The rounds variable is global to the
    libsalsa20.so library and not switched when switching between instances
    of the Salsa20 class.
        s1 = Salsa20( key, IV, 20 )
        s2 = Salsa20( key, IV, 8 )
    In this example,
        with pySalsa20, both s1 and s2 will do 8 rounds of encryption.
        with pureSalsa20, s1 will do 20 rounds and s2 will do 8 rounds.
    Perhaps giving each instance its own nRounds variable, which
    is passed to the salsa20wordtobyte() function, is insecure.  I'm not a 
    cryptographer.

    pureSalsa20.py and testSalsa20.py are EXPERIMENTAL software and 
    intended for educational purposes only.  To make experimentation less 
    cumbersome, pureSalsa20.py and testSalsa20.py are free for any use.

    Revisions:
    ----------
      p3.2   Fixed bug that initialized the output buffer with plaintext!
             Saner ramping of nreps in speed test.
             Minor changes and print statements.
      p3.1   Took timing variability out of add32() and rot32().
             Made the internals more like pySalsa20/libsalsa .
             Put the semicolons back in the main loop!
             In encryptBytes(), modify a byte array instead of appending.
             Fixed speed calculation bug.
             Used subclasses instead of patches in testSalsa20.py .
             Added 64k-byte messages to speed test to be fair to pySalsa20.
      p3     First version, intended to parallel pySalsa20 version 3.

    More references:
    ----------------
      http://www.seanet.com/~bugbee/crypto/salsa20/          [pySalsa20]
      http://cr.yp.to/snuffle.html        [The original name of Salsa20]
      http://cr.yp.to/snuffle/salsafamily-20071225.pdf [ Salsa20 design]
      http://www.tiac.net/~sw/2010/02/PureSalsa20
    
    THIS PROGRAM IS PROVIDED WITHOUT WARRANTY OR GUARANTEE OF
    ANY KIND.  USE AT YOUR OWN RISK.  

    Cheers,

    Steve Witham sw at remove-this tiac dot net
    February, 2010
"""
import sys
assert(sys.version_info >= (2, 6))

if sys.version_info >= (3,):
	integer_types = (int,)
	python3 = True
else:
	integer_types = (int, long)
	python3 = False

from struct import Struct
little_u64 = Struct( "<Q" )      #    little-endian 64-bit unsigned.
                                 #    Unpacks to a tuple of one element!

little16_i32 = Struct( "<16i" )  # 16 little-endian 32-bit signed ints.
little4_i32 = Struct( "<4i" )    #  4 little-endian 32-bit signed ints.
little2_i32 = Struct( "<2i" )    #  2 little-endian 32-bit signed ints.

_version = 'p4.0'

#----------- Salsa20 class which emulates pySalsa20.Salsa20 ---------------

class Salsa20(object):
    def __init__(self, key=None, IV=None, rounds=20 ):
        self._lastChunk64 = True
        self._IVbitlen = 64             # must be 64 bits
        self.ctx = [ 0 ] * 16
        if key:
            self.setKey(key)
        if IV:
            self.setIV(IV)

        self.setRounds(rounds)


    def setKey(self, key):
        assert type(key) == bytes
        ctx = self.ctx
        if len( key ) == 32:  # recommended
            constants = b"expand 32-byte k"
            ctx[ 1],ctx[ 2],ctx[ 3],ctx[ 4] = little4_i32.unpack(key[0:16])
            ctx[11],ctx[12],ctx[13],ctx[14] = little4_i32.unpack(key[16:32])
        elif len( key ) == 16:
            constants = b"expand 16-byte k"
            ctx[ 1],ctx[ 2],ctx[ 3],ctx[ 4] = little4_i32.unpack(key[0:16])
            ctx[11],ctx[12],ctx[13],ctx[14] = little4_i32.unpack(key[0:16])
        else:
            raise Exception( "key length isn't 32 or 16 bytes." )
        ctx[0],ctx[5],ctx[10],ctx[15] = little4_i32.unpack( constants )

        
    def setIV(self, IV):
        assert type(IV) == bytes
        assert len(IV)*8 == 64, 'nonce (IV) not 64 bits'
        self.IV = IV
        ctx=self.ctx
        ctx[ 6],ctx[ 7] = little2_i32.unpack( IV )
        ctx[ 8],ctx[ 9] = 0, 0  # Reset the block counter.

    setNonce = setIV            # support an alternate name


    def setCounter( self, counter ):
        assert( type(counter) in integer_types )
        assert( 0 <= counter < 1<<64 ), "counter < 0 or >= 2**64"
        ctx = self.ctx
        ctx[ 8],ctx[ 9] = little2_i32.unpack( little_u64.pack( counter ) )

    def getCounter( self ):
        return little_u64.unpack( little2_i32.pack( *self.ctx[ 8:10 ] ) ) [0]


    def setRounds(self, rounds, testing=False ):
        assert testing or rounds in [8, 12, 20], 'rounds must be 8, 12, 20'
        self.rounds = rounds


    def encryptBytes(self, data):
        assert type(data) == bytes, 'data must be byte string'
        assert self._lastChunk64, 'previous chunk not multiple of 64 bytes'
        lendata = len(data)
        munged = bytearray(lendata)
        for i in range( 0, lendata, 64 ):
            h = salsa20_wordtobyte( self.ctx, self.rounds, checkRounds=False )
            self.setCounter( ( self.getCounter() + 1 ) % 2**64 )
            # Stopping at 2^70 bytes per nonce is user's responsibility.
            for j in range( min( 64, lendata - i ) ):
                if python3:
                    munged[ i+j ] = data[ i+j ] ^ h[j]
                else:
                    munged[ i+j ] = ord(data[ i+j ]) ^ ord(h[j])

        self._lastChunk64 = not lendata % 64
        return bytes(munged)
    
    decryptBytes = encryptBytes # encrypt and decrypt use same function

#--------------------------------------------------------------------------

def salsa20_wordtobyte( input, nRounds=20, checkRounds=True ):
    """ Do nRounds Salsa20 rounds on a copy of 
            input: list or tuple of 16 ints treated as little-endian unsigneds.
        Returns a 64-byte string.
        """

    assert( type(input) in ( list, tuple )  and  len(input) == 16 )
    assert( not(checkRounds) or ( nRounds in [ 8, 12, 20 ] ) )

    x = list( input )

    def XOR( a, b ):  return a ^ b
    ROTATE = rot32
    PLUS   = add32

    for i in range( nRounds // 2 ):
        # These ...XOR...ROTATE...PLUS... lines are from ecrypt-linux.c
        # unchanged except for indents and the blank line between rounds:
        x[ 4] = XOR(x[ 4],ROTATE(PLUS(x[ 0],x[12]), 7));
        x[ 8] = XOR(x[ 8],ROTATE(PLUS(x[ 4],x[ 0]), 9));
        x[12] = XOR(x[12],ROTATE(PLUS(x[ 8],x[ 4]),13));
        x[ 0] = XOR(x[ 0],ROTATE(PLUS(x[12],x[ 8]),18));
        x[ 9] = XOR(x[ 9],ROTATE(PLUS(x[ 5],x[ 1]), 7));
        x[13] = XOR(x[13],ROTATE(PLUS(x[ 9],x[ 5]), 9));
        x[ 1] = XOR(x[ 1],ROTATE(PLUS(x[13],x[ 9]),13));
        x[ 5] = XOR(x[ 5],ROTATE(PLUS(x[ 1],x[13]),18));
        x[14] = XOR(x[14],ROTATE(PLUS(x[10],x[ 6]), 7));
        x[ 2] = XOR(x[ 2],ROTATE(PLUS(x[14],x[10]), 9));
        x[ 6] = XOR(x[ 6],ROTATE(PLUS(x[ 2],x[14]),13));
        x[10] = XOR(x[10],ROTATE(PLUS(x[ 6],x[ 2]),18));
        x[ 3] = XOR(x[ 3],ROTATE(PLUS(x[15],x[11]), 7));
        x[ 7] = XOR(x[ 7],ROTATE(PLUS(x[ 3],x[15]), 9));
        x[11] = XOR(x[11],ROTATE(PLUS(x[ 7],x[ 3]),13));
        x[15] = XOR(x[15],ROTATE(PLUS(x[11],x[ 7]),18));

        x[ 1] = XOR(x[ 1],ROTATE(PLUS(x[ 0],x[ 3]), 7));
        x[ 2] = XOR(x[ 2],ROTATE(PLUS(x[ 1],x[ 0]), 9));
        x[ 3] = XOR(x[ 3],ROTATE(PLUS(x[ 2],x[ 1]),13));
        x[ 0] = XOR(x[ 0],ROTATE(PLUS(x[ 3],x[ 2]),18));
        x[ 6] = XOR(x[ 6],ROTATE(PLUS(x[ 5],x[ 4]), 7));
        x[ 7] = XOR(x[ 7],ROTATE(PLUS(x[ 6],x[ 5]), 9));
        x[ 4] = XOR(x[ 4],ROTATE(PLUS(x[ 7],x[ 6]),13));
        x[ 5] = XOR(x[ 5],ROTATE(PLUS(x[ 4],x[ 7]),18));
        x[11] = XOR(x[11],ROTATE(PLUS(x[10],x[ 9]), 7));
        x[ 8] = XOR(x[ 8],ROTATE(PLUS(x[11],x[10]), 9));
        x[ 9] = XOR(x[ 9],ROTATE(PLUS(x[ 8],x[11]),13));
        x[10] = XOR(x[10],ROTATE(PLUS(x[ 9],x[ 8]),18));
        x[12] = XOR(x[12],ROTATE(PLUS(x[15],x[14]), 7));
        x[13] = XOR(x[13],ROTATE(PLUS(x[12],x[15]), 9));
        x[14] = XOR(x[14],ROTATE(PLUS(x[13],x[12]),13));
        x[15] = XOR(x[15],ROTATE(PLUS(x[14],x[13]),18));

    for i in range( len( input ) ):
        x[i] = PLUS( x[i], input[i] )
    return little16_i32.pack( *x )

#--------------------------- 32-bit ops -------------------------------

def trunc32( w ):
    """ Return the bottom 32 bits of w as a Python int.
        This creates longs temporarily, but returns an int. """
    w = int( ( w & 0x7fffFFFF ) | -( w & 0x80000000 ) )
    assert type(w) == int
    return w


def add32( a, b ):
    """ Add two 32-bit words discarding carry above 32nd bit,
        and without creating a Python long.
        Timing shouldn't vary.
    """
    lo = ( a & 0xFFFF ) + ( b & 0xFFFF )
    hi = ( a >> 16 ) + ( b >> 16 ) + ( lo >> 16 )
    return ( -(hi & 0x8000) | ( hi & 0x7FFF ) ) << 16 | ( lo & 0xFFFF )


def rot32( w, nLeft ):
    """ Rotate 32-bit word left by nLeft or right by -nLeft
        without creating a Python long.
        Timing depends on nLeft but not on w.
    """
    nLeft &= 31  # which makes nLeft >= 0
    if nLeft == 0:
        return w

    # Note: now 1 <= nLeft <= 31.
    #     RRRsLLLLLL   There are nLeft RRR's, (31-nLeft) LLLLLL's,
    # =>  sLLLLLLRRR   and one s which becomes the sign bit.
    RRR = ( ( ( w >> 1 ) & 0x7fffFFFF ) >> ( 31 - nLeft ) )
    sLLLLLL = -( (1<<(31-nLeft)) & w ) | (0x7fffFFFF>>nLeft) & w
    return RRR | ( sLLLLLL << nLeft )


# --------------------------------- end -----------------------------------

</file>


<file path="tools/writemdict/ripemd128.py">
""" 
ripemd128.py - A simple ripemd128 library in pure Python.

Supports both Python 2 (versions >= 2.6) and Python 3.

Usage:
    from ripemd128 import ripemd128
    digest = ripemd128(b"The quick brown fox jumps over the lazy dog")
    assert(digest == b"\x3f\xa9\xb5\x7f\x05\x3c\x05\x3f\xbe\x27\x35\xb2\x38\x0d\xb5\x96")
"""
      


import struct


# follows this description: http://homes.esat.kuleuven.be/~bosselae/ripemd/rmd128.txt

def f(j, x, y, z):
	assert(0 <= j and j < 64)
	if j < 16:
		return x ^ y ^ z
	elif j < 32:
		return (x & y) | (z & ~x)
	elif j < 48:
		return (x | (0xffffffff & ~y)) ^ z
	else:
		return (x & z) | (y & ~z)

def K(j):
	assert(0 <= j and j < 64)
	if j < 16:
		return 0x00000000
	elif j < 32:
		return 0x5a827999
	elif j < 48:
		return 0x6ed9eba1
	else:
		return 0x8f1bbcdc

def Kp(j):
	assert(0 <= j and j < 64)
	if j < 16:
		return 0x50a28be6
	elif j < 32:
		return 0x5c4dd124
	elif j < 48:
		return 0x6d703ef3
	else:
		return 0x00000000

def padandsplit(message):
	"""
	returns a two-dimensional array X[i][j] of 32-bit integers, where j ranges
	from 0 to 16.
	First pads the message to length in bytes is congruent to 56 (mod 64), 
	by first adding a byte 0x80, and then padding with 0x00 bytes until the
	message length is congruent to 56 (mod 64). Then adds the little-endian
	64-bit representation of the original length. Finally, splits the result
	up into 64-byte blocks, which are further parsed as 32-bit integers.
	"""
	origlen = len(message)
	padlength = 64 - ((origlen - 56) % 64) #minimum padding is 1!
	message += b"\x80"
	message += b"\x00" * (padlength - 1)
	message += struct.pack("<Q", origlen*8)
	assert(len(message) % 64 == 0)
	return [
	         [
	           struct.unpack("<L", message[i+j:i+j+4])[0]
	           for j in range(0, 64, 4)
	         ]
	         for i in range(0, len(message), 64)
	       ]


def add(*args):
	return sum(args) & 0xffffffff

def rol(s,x):
	assert(s < 32)
	return (x << s | x >> (32-s)) & 0xffffffff

r =  [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,
       7, 4,13, 1,10, 6,15, 3,12, 0, 9, 5, 2,14,11, 8,
       3,10,14, 4, 9,15, 8, 1, 2, 7, 0, 6,13,11, 5,12,
       1, 9,11,10, 0, 8,12, 4,13, 3, 7,15,14, 5, 6, 2]
rp = [ 5,14, 7, 0, 9, 2,11, 4,13, 6,15, 8, 1,10, 3,12,
       6,11, 3, 7, 0,13, 5,10,14,15, 8,12, 4, 9, 1, 2,
      15, 5, 1, 3, 7,14, 6, 9,11, 8,12, 2,10, 0, 4,13,
       8, 6, 4, 1, 3,11,15, 0, 5,12, 2,13, 9, 7,10,14]
s =  [11,14,15,12, 5, 8, 7, 9,11,13,14,15, 6, 7, 9, 8,
       7, 6, 8,13,11, 9, 7,15, 7,12,15, 9,11, 7,13,12,
      11,13, 6, 7,14, 9,13,15,14, 8,13, 6, 5,12, 7, 5,
      11,12,14,15,14,15, 9, 8, 9,14, 5, 6, 8, 6, 5,12]
sp = [ 8, 9, 9,11,13,15,15, 5, 7, 7, 8,11,14,14,12, 6,
       9,13,15, 7,12, 8, 9,11, 7, 7,12, 7, 6,15,13,11,
       9, 7,15,11, 8, 6, 6,14,12,13, 5,14,13,13, 7, 5,
      15, 5, 8,11,14,14, 6,14, 6, 9,12, 9,12, 5,15, 8]


def ripemd128(message):
	h0 = 0x67452301
	h1 = 0xefcdab89
	h2 = 0x98badcfe
	h3 = 0x10325476
	X = padandsplit(message)
	for i in range(len(X)):
		(A,B,C,D) = (h0,h1,h2,h3)
		(Ap,Bp,Cp,Dp) = (h0,h1,h2,h3)
		for j in range(64):
			T = rol(s[j], add(A, f(j,B,C,D), X[i][r[j]], K(j)))
			(A,D,C,B) = (D,C,B,T)
			T = rol(sp[j], add(Ap, f(63-j,Bp,Cp,Dp), X[i][rp[j]], Kp(j)))
			(Ap,Dp,Cp,Bp)=(Dp,Cp,Bp,T)
		T = add(h1,C,Dp)
		h1 = add(h2,D,Ap)
		h2 = add(h3,A,Bp)
		h3 = add(h0,B,Cp)
		h0 = T
	
	
	return struct.pack("<LLLL",h0,h1,h2,h3)

def hexstr(bstr):
	return "".join("{0:02x}".format(b) for b in bstr)
	

</file>


<file path="tools/writemdict/writemdict.py">
# -*- coding: utf-8 -*-
"""
writemdict.py - a library for creating dictionary files in the MDict file format.

Optional dependencies:
  python-lzo: Required to write dictionaries using LZO compression. (Other compression schemes are available.)

Simple usage example:

    from __future__ import unicode_literals
    from writemdict import MDictWriter

    dictionary = {"doe": "a deer, a female deer.",
                  "ray": "a drop of golden sun.",
                  "me": "a name I call myself.",
                  "far": "a long, long way to run."}

    writer = MDictWriter(dictionary, title="Example Dictionary",
                         description="This is an example dictionary.")
    outfile = open("dictionary.mdx", "wb")
    writer.write(outfile)
    outfile.close()

  This will create an MDX file called "dictionary.mdx", with four entries: "doe", "ray", "me", "far", and the
  corresponding definitions.

  For further options, see the documentation for MdxWriter.__init__().
"""

from __future__ import unicode_literals
import re
import string
import struct
import functools
import locale

import zlib
import datetime

from html import escape
from tools.writemdict.ripemd128 import ripemd128
from tools.writemdict.pureSalsa20 import Salsa20

# try:
#     import lzo
#     HAVE_LZO = True
# except ImportError:
#     lzo: Any = {}
#     HAVE_LZO = False

# Not using lzo compression.
HAVE_LZO = False

class ParameterError(Exception):
    ### Raised when some parameter to MdxWriter is invalid or uninterpretable.
    pass


def _mdx_compress(data, compression_type=2):
    header = (struct.pack(b"<L", compression_type) +
           struct.pack(b">L", zlib.adler32(data) & 0xffffffff))  # depending on python version, zlib.adler32 may return a signed number.
    if compression_type == 0:  # no compression
        return header + data
    elif compression_type == 2:
        return header + zlib.compress(data)
    elif compression_type == 1:
        # Not using lzo compression.
        raise NotImplementedError()
        # if HAVE_LZO:
        #     return header + lzo.compress(data)[5:]  # python-lzo adds a 5-byte header.
        # else:
        #     raise NotImplementedError()
    else:
        raise ParameterError("Unknown compression type")


def _fast_encrypt(data, key):
    b = bytearray(data)
    key = bytearray(key)
    previous = 0x36
    for i in range(len(b)):
        t = b[i] ^ previous ^ (i & 0xff) ^ key[i % len(key)]
        previous = b[i] = ((t >> 4) | (t << 4)) & 0xff
    return bytes(b)


def _mdx_encrypt(comp_block):
    key = ripemd128(comp_block[4:8] + struct.pack(b"<L", 0x3695))
    return comp_block[0:8] + _fast_encrypt(comp_block[8:], key)


def _salsa_encrypt(plaintext, dict_key):
    assert(isinstance(dict_key, bytes))
    assert(isinstance(plaintext, bytes))
    encrypt_key = ripemd128(dict_key)
    s20 = Salsa20(key=encrypt_key, IV=b"\x00"*8, rounds=8)
    return s20.encryptBytes(plaintext)


def _hexdump(bytes_blob):
    # Returns a hexadecimal representation of bytes_blob, as a (unicode) string.
    #
    # bytes_blob should have type bytes.

    # In Python 2.6+, bytes is an alias for str, and indexing into a bytes
    # object gives a string of length 1.
    # In Python 3, indexing into a bytes object gives a number.
    # The following should work on both versions.
    if bytes == str:
        return "".join("{:02X}".format(ord(c)) for c in bytes_blob)
    else:
        return "".join("{:02X}".format(c) for c in bytes_blob)


def encrypt_key(dict_key, **kwargs):
    """
    Generates a hexadecimal key for use with the official MDict program.

    Parameters:
      dict_key: a bytes object, representing the dictionary password.

    Keyword parameters:
      Exactly one of email and device_id should be specified. They should be unicode strings,
      representing either the user's email address, or the device ID of the machine on which
      the dictionary is to be opened.

    Return value:
      a string of 32 hexadecimal digits. This should be placed in a file of its own,
      with the same name and location as the mdx file but the extension changed to '.key'.

    Example usage:
        key = encrypt_key(b"password", email="username@example.com")

        key = encrypt_key(b"password", device_id="12345678-9012-3456-7890-1234")
    """

    if(("email" not in kwargs and "device_id" not in kwargs) or ("email" in kwargs and "device_id" in kwargs)):
        raise ParameterError(
            "Expected exactly one of email and device_id as keyword argument")

    if "email" in kwargs:
        owner_info_digest = ripemd128(kwargs["email"].encode("ascii"))
    else:
        owner_info_digest = ripemd128(kwargs["device_id"].encode("ascii"))

    dict_key_digest = ripemd128(dict_key)

    s20 = Salsa20(key=owner_info_digest, IV=b"\x00"*8, rounds=8)
    output_key = s20.encryptBytes(dict_key_digest)
    return _hexdump(output_key)


class _OffsetTableEntry(object):
    # Each OffsetTableEntry represents one key/record pair of the dictionary.
    # In addition to the values themselves, it contains information about
    # the offset at which this entry will be placed (i.e. the total length
    # of records before it) which is required by the MDX format.
    def __init__(self, key, key_null, key_len, offset, record_null):
        self.key = key
        self.key_null = key_null
        self.key_len = key_len
        self.offset = offset
        self.record_null = record_null


class MDictWriter(object):

    def __init__(self, d, title, description,
                 block_size=65536,
                 encrypt_index=False,
                 encoding="utf8",
                 compression_type=2,
                 version="2.0",
                 encrypt_key=None,
                 register_by=None,
                 user_email=None,
                 user_device_id=None,
                 is_mdd=False):
        """
        Prepares the records. A subsequent call to write() writes
        the mdx or mdd file.

        d is a dictionary. The keys should be (unicode) strings. If used for an mdx
          file (the parameter is_mdd is False), then the values should also be
          (unicode) strings, containing HTML snippets. If used to write an mdd
          file (the parameter is_mdd is True), then the values should be binary
          strings (bytes objects), containing the raw data for the corresponding
          file object.

        title is a (unicode) string, with the title of the dictionary
          description is a (unicode) string, with a short description of the
          dictionary.

        block_size is the approximate number of bytes (uncompressed)
          before starting a new block.


        encrypt_index is true if the keyword index should be encrypted.

        encoding is the character encoding to use in the files. Valid options are
          "utf8", "utf16", "gbk", and "big5". If used to write an mdd file (the
          parameter is_mdd is True), then this is ignored.

        compression_type is an integer specifying the compression type to use.
          Valid options are 0 (no compression), 1 (LZO compression), or 2 (gzip
          compression).

        version specifies the version of the file format to use. Recognized options are
          "2.0" and "1.2".

        encrypt_key should be a string, containing the dictionary key. If
          encrypt_key is None, no encryption will be applied. If encrypt_key is
          not None, you need to specify register_by.

        register_by should be either "email" or "device_id". Ignored unless
          encrypt_key is not None. Specifies whether the user's email or user's
          device ID should be used to encrypt the encryption key.

        user_email is ignored unless encrypt_key is not None and register_by is
          "email". If it is specified, an encrypted form of encrypt_key will be
          written into the dictionary header. The file can then be opened by
          anyone who has set their email (in the MDict client) this this value.
          If it is not specified, the MDict client will look for this encrypted
          key in a separate .key file.

        user_device_id is ignored unless encrypt_key is not None and register_by
          is "device_id". If it is specified, an encrypted form of encrypt_key
          will be written into the dictionary header. The file can then be opened
          by anyone whose device ID (as determined by the MDict client) equals this
          value. If it is not specified, the MDict client will look for this
          encrypted key in a separate .key file.

        is_mdd is a boolean specifying whether the file written will be an mdx file
          or an mdd file. By default this is False, meaning that an mdd file will
          be written.
        """

        self._num_entries = len(d)
        self._title = title
        self._description = description
        self._block_size = block_size
        self._encrypt_index = encrypt_index
        self._encrypt = (encrypt_key is not None)
        self._encrypt_key = encrypt_key
        if register_by not in ["email", "device_id", None]:
            raise ParameterError("Unkonwn register_by type")
        self._register_by = register_by
        self._user_email = user_email
        self._user_device_id = user_device_id
        self._compression_type = compression_type
        self._is_mdd = is_mdd

        # encoding is set to the string used in the mdx header.
        # python_encoding is passed on to the python .encode()
        # function to encode the data.
        # encoding_length is the size of one unit of the encoding,
        # used to calculate the length for keys in the key index.
        if not is_mdd:
            encoding = encoding.lower()
            if encoding in ["utf8", "utf-8"]:
                self._python_encoding = "utf_8"
                self._encoding = "UTF-8"
                self._encoding_length = 1
            elif encoding in ["utf16", "utf-16"]:
                self._python_encoding = "utf_16_le"
                self._encoding = "UTF-16"
                self._encoding_length = 2
            elif encoding == "gbk":
                self._python_encoding = "gbk"
                self._encoding = "GBK"
                self._encoding_length = 1
            elif encoding == "big5":
                self._python_encoding = "big5"
                self._encoding = "BIG5"
                self._encoding_length = 1
            else:
                raise ParameterError("Unknown encoding")
        else:
            self._python_encoding = "utf_16_le"
            self._encoding_length = 2
        if version not in ["2.0", "1.2"]:
            raise ParameterError("Unknown version")
        self._version = version
        self._build_offset_table(d)
        self._build_key_blocks()
        self._build_keyb_index()
        self._build_record_blocks()
        self._build_recordb_index()

    def _build_offset_table(self, d):
        # Sets self._offset_table to a table of entries _OffsetTableEntry objects e.
        #
        # where:
        #  e.key: encoded version of the key, not null-terminated
        #  e.key_null: encoded version of the key, null-terminated
        #  e.key_len: the length of the key, in either bytes or 2-byte units, not counting the null character
        #        (as required by the MDX format in the keyword index)
        #  e.offset: the cumulative sum of len(record_null) for preceding records
        #  e.record_null: encoded version of the record, null-terminated
        #
        # Also sets self._total_record_len to the total length of all record fields.
        def mdict_cmp(item1, item2, prevent_link_to_link=True, sort_definitions=False):
            # sort following mdict standard

            key1 = item1[0]
            key2 = item2[0]

            key1 = key1.lower()
            key2 = key2.lower()
            if not self._is_mdd:
                key1 = regex_strip.sub('', key1)
                key2 = regex_strip.sub('', key2)
            # locale key
            key1 = locale.strxfrm(key1)
            key2 = locale.strxfrm(key2)
            if key1 > key2:
                return 1
            elif key1 < key2:
                return -1
            # reverse
            if len(key1) > len(key2):
                return -1
            elif len(key1) < len(key2):
                return 1
            key1 = key1.rstrip(string.punctuation)
            key2 = key2.rstrip(string.punctuation)
            if key1 > key2:
                return -1
            elif key1 < key2:
                return 1

            # dpd: link to link bug prevention (08.03.2023)
            if prevent_link_to_link or sort_definitions:
                #if key1 & key2 are equal: compare the values
                #if value 1&2 are links to another definition: dont change order
                value1 = item1[1].lower()
                value2 = item2[1].lower()
                if value1.startswith("@@@link=") and value2.startswith("@@@link="):
                    if sort_definitions:
                        value1 = locale.strxfrm(value1)
                        value2 = locale.strxfrm(value2)
                        if value1 > value2:
                            return 1
                        if value1 < value2:
                            return -1
                    return 0
                #if value1 is link, but value2 is not: link->lower pos
                if value1.startswith("@@@link="):
                    return 1
                #if value2 is link, but value1 is not: link->lower pos
                if value2.startswith("@@@link="):
                    return -1
                #disabled by default: sorting definitions is probably not neccessary..
                if sort_definitions:
                    #if value1&2 are both normal definitions, only compare the fist 50 characters 
                    # (otherwise its probably some long html where the difference could just be some css)
                    value1 = value1[:50]
                    value2 = value2[:50]
                    if value1 > value2:
                        return 1
                    if value1 < value2:
                        return -1
            return 0

        pattern = '[%s ]+' % string.punctuation
        regex_strip = re.compile(pattern)

        if isinstance(d, dict):
            items = list(d.items())
        else:
            items = list(d)
        items.sort(key=functools.cmp_to_key(mdict_cmp))

        self._offset_table = []
        offset = 0
        for key, record in items:
            key_enc = key.encode(self._python_encoding)
            key_null = (key+"\0").encode(self._python_encoding)
            key_len = len(key_enc) // self._encoding_length

            # set record_null to a the the value of the record. If it's
            # an MDX file, append an extra null character.
            if self._is_mdd:
                record_null = record
            else:
                record_null = (record+"\0").encode(self._python_encoding)
            self._offset_table.append(_OffsetTableEntry(
                key=key_enc,
                key_null=key_null,
                key_len=key_len,
                record_null=record_null,
                offset=offset))
            offset += len(record_null)
        self._total_record_len = offset

    def _split_blocks(self, block_type):
        # Split either the records or the keys into blocks for compression.
        #
        # Returns a list of _MdxBlock, where the decompressed size of each block is (as
        # far as practicable) less than self._block_size.
        #
        # block_type should be a subclass of _MdxBlock, i.e. either _MdxRecordBlock or
        # _MdxKeyBlock.

        this_block_start = 0
        cur_size = 0
        blocks = []
        for ind in range(len(self._offset_table)+1):
            if ind != len(self._offset_table):
                t = self._offset_table[ind]
            else:
                t = None

            if ind == 0:
                flush = False
                # nothing to flush yet
                # this part is needed in case the first entry is longer than
                # self._block_size.
            elif ind == len(self._offset_table):
                flush = True  # always flush the last block
            elif cur_size + block_type._len_block_entry(t) > self._block_size:
                flush = True  # Adding this entry to make us larger than
                     #self._block_size, so flush now.
            else:
                flush = False
            if flush:
                blocks.append(block_type(
                    self._offset_table[this_block_start:ind], self._compression_type, self._version))
                cur_size = 0
                this_block_start = ind
            if t is not None:  # mentally add this entry to list of things
                cur_size += block_type._len_block_entry(t)
        return blocks

    def _build_key_blocks(self):
        # Sets self._key_blocks to a list of _MdxKeyBlocks.
        self._key_blocks = self._split_blocks(_MdxKeyBlock)

    def _build_record_blocks(self):
        self._record_blocks = self._split_blocks(_MdxRecordBlock)

    def _build_keyb_index(self):
        # Sets self._keyb_index to a bytes object, containing the index of key blocks, in
        # a format suitable for direct writing to the file.
        #
        # Also sets self._keyb_index_comp_size and self._keyb_index_decomp_size.

        decomp_data = b"".join(b.get_index_entry() for b in self._key_blocks)
        self._keyb_index_decomp_size = len(decomp_data)
        if self._version == "2.0":
            self._keyb_index = _mdx_compress(decomp_data, self._compression_type)
            if self._encrypt_index:
                self._keyb_index = _mdx_encrypt(self._keyb_index)
            self._keyb_index_comp_size = len(self._keyb_index)
        elif self._encrypt_index:
            raise ParameterError("Key index encryption not supported in version 1.2")
        else:
            self._keyb_index = decomp_data

    def _build_recordb_index(self):
        # Sets self._recordb_index to a bytes object, containing the index of key blocks,
        # in a format suitable for direct writing to the file.

        # Also sets self._recordb_index_size.

        self._recordb_index = b"".join(
            (b.get_index_entry() for b in self._record_blocks))
        self._recordb_index_size = len(self._recordb_index)

    def _write_key_sect(self, outfile):
        # Writes the key section header, key block index, and all the key blocks to
        # outfile.

        # outfile: a file-like object, opened in binary mode.

        keyblocks_total_size = sum(len(b.get_block()) for b in self._key_blocks)
        if self._version == "2.0":
            preamble = struct.pack(b">QQQQQ",
                len(self._key_blocks),
                self._num_entries,
                self._keyb_index_decomp_size,
                self._keyb_index_comp_size,
                keyblocks_total_size)
            preamble_checksum = struct.pack(b">L", zlib.adler32(preamble))
            if(self._encrypt):
                preamble = _salsa_encrypt(preamble, self._encrypt_key)
            outfile.write(preamble)
            outfile.write(preamble_checksum)
        else:
            preamble = struct.pack(b">LLLL",
                len(self._key_blocks),
                self._num_entries,
                self._keyb_index_decomp_size,
                keyblocks_total_size)
            if(self._encrypt):
                preamble = _salsa_encrypt(preamble, self._encrypt_key)
            outfile.write(preamble)

        outfile.write(self._keyb_index)
        for b in self._key_blocks:
            outfile.write(b.get_block())

    def _write_record_sect(self, outfile):
        # Writes the record section header, record block index, and all the record blocks
        # to outfile.
        #
        # outfile: a file-like object, opened in binary mode.

        recordblocks_total_size = sum(
            (len(b.get_block()) for b in self._record_blocks))
        if self._version == "2.0":
            format = b">QQQQ"
        else:
            format = b">LLLL"
        outfile.write(struct.pack(format,
                            len(self._record_blocks),
                            self._num_entries,
                            self._recordb_index_size,
                            recordblocks_total_size))
        outfile.write(self._recordb_index)
        for b in self._record_blocks:
            outfile.write(b.get_block())

    def write(self, outfile):
        """ 
        Write the mdx file to outfile.
        
        outfile: a file-like object, opened in binary mode.
        """

        self._write_header(outfile)
        self._write_key_sect(outfile)
        self._write_record_sect(outfile)

    def _write_header(self, f):
        encrypted = 0
        if self._encrypt_index:
            encrypted = encrypted | 2
        if self._encrypt:
            encrypted = encrypted | 1

        if self._encrypt and self._register_by == "email":
            register_by_str = "EMail"
            if self._user_email is not None:
                regcode = encrypt_key(self._encrypt_key, email=self._user_email)
            else:
                regcode = ""
        elif self._encrypt and self._register_by == "device_id":
            register_by_str = "DeviceID"
            if self._user_device_id is not None:
                regcode = encrypt_key(self._encrypt_key, device_id=self._user_device_id)
            else:
                regcode = ""
        else:
            register_by_str = ""
            regcode = ""

        if not self._is_mdd:
            header_string = (
                            """<Dictionary """
                            """GeneratedByEngineVersion="{version}" """
                            """RequiredEngineVersion="{version}" """
                            """Encrypted="{encrypted}" """
                            """Encoding="{encoding}" """
                            """Format="Html" """
                            """CreationDate="{date.year}-{date.month}-{date.day}" """
                            """Compact="No" """
                            """Compat="No" """
                            """KeyCaseSensitive="No" """
                            """Description="{description}" """
                            """Title="{title}" """
                            """DataSourceFormat="106" """
                            """StyleSheet="" """
                            """RegisterBy="{register_by_str}" """
                            """RegCode="{regcode}"/>\r\n\x00""").format(
                version=self._version,
                encrypted=encrypted,
                encoding=self._encoding,
                date=datetime.date.today(),
                description=escape(self._description, quote=True),
                title=escape(self._title, quote=True),
                register_by_str=register_by_str,
                regcode=regcode
            ).encode("utf_16_le")
        else:
            header_string = (
                            """<Library_Data """
                            """GeneratedByEngineVersion="{version}" """
                            """RequiredEngineVersion="{version}" """
                            """Encrypted="{encrypted}" """
                            """Format="" """
                            """CreationDate="{date.year}-{date.month}-{date.day}" """
                            """Compact="No" """
                            """Compat="No" """
                            """KeyCaseSensitive="No" """
                            """Description="{description}" """
                            """Title="{title}" """
                            """DataSourceFormat="106" """
                            """StyleSheet="" """
                            """RegisterBy="{register_by_str}" """
                            """RegCode="{regcode}"/>\r\n\x00""").format(
                version=self._version,
                encrypted=encrypted,
                date=datetime.date.today(),
                description=escape(self._description, quote=True),
                title=escape(self._title, quote=True),
                register_by_str=register_by_str,
                regcode=regcode
            ).encode("utf_16_le")
        f.write(struct.pack(b">L", len(header_string)))
        f.write(header_string)
        f.write(struct.pack(b"<L", zlib.adler32(header_string) & 0xffffffff))


class _MdxBlock(object):
    # Abstract base class for _MdxRecordBlock and _MdxKeyBlock.
    #
    # In the MDX file format, the keyword section and the record section have a
    # similar structure:
    #
    #   section header
    #   index entry for block 0
    #   ...
    #   index entry for block k
    #   block 0
    #   ...
    #   block k
    #
    # This class represents one such block. It defines a common interface for
    # record blocks and keyword blocks, to allow the two sections to
    # be built in a uniform manner.
    #

    def __init__(self, offset_table, compression_type, version):
        # Builds the data from offset_table.
        #
        # offset_table is a iterable containing _OffsetTableEntry objects.

        decomp_data = b"".join(
            type(self)._block_entry(t, version)
            for t in offset_table)
        self._decomp_size = len(decomp_data)
        self._comp_data = _mdx_compress(decomp_data, compression_type)
        self._comp_size = len(self._comp_data)
        self._version = version

    def get_block(self):
        # Returns a bytes object, containing the data for this block.
        return self._comp_data

    def get_index_entry(self):
        # Returns a bytes object, containing the entry for this block in the
        # corresponding key block index or record block index.

        raise NotImplementedError()

    @staticmethod
    def _block_entry(__t__, __version__):
        # Returns the data corresponding to a single entry in offset.
        #
        # t is an _OffsetTableEntry object

        raise NotImplementedError()

    @staticmethod
    def _len_block_entry(__t__):
        # Should be approximately equal to len(_block_entry(t)).
        #
        # Used by MdxWriter._split_blocks() to determine where to split into blocks."""
        raise NotImplementedError()


class _MdxRecordBlock(_MdxBlock):
    # A class representing a record block.
    #
    # Has the ability to return (in the format suitable for insertion in an mdx file)
    # both the block itself, as well as the entry in the record block index for that
    # block.

    def __init__(self, offset_table, compression_type, version):
        # Builds the data for offset_table.
        #
        # offset_table is a iterable containing _OffsetTableEntry objects.
        #
        # Actually only uses the record parts.

        _MdxBlock.__init__(self, offset_table, compression_type, version)

    def get_index_entry(self):
        # Returns a bytes object, containing the entry for this block in the record
        # block index.

        if self._version == "2.0":
            format = b">QQ"
        else:
            format = b">LL"
        return struct.pack(format, self._comp_size, self._decomp_size)

    @staticmethod
    def _block_entry(t, __version__):
        return t.record_null

    @staticmethod
    def _len_block_entry(t):
        return len(t.record_null)


class _MdxKeyBlock(_MdxBlock):
    # A class representing a key block.
    #
    # Has the ability to return (in the format suitable for insertion in an mdx file)
    # both the block itself, as well as the entry in the record block index for that
    # block.
    def __init__(self, offset_table, compression_type, version):
        # Builds the data for offset_table.
        #
        # offset_table is a iterable containing _OffsetTableEntry objects.
        #
        # Only uses the key, key_len, key_null and offset fields, and effectively ignores record_null.

        _MdxBlock.__init__(self, offset_table, compression_type, version)
        self._num_entries = len(offset_table)
        if version == "2.0":
            self._first_key = offset_table[0].key_null
            self._last_key = offset_table[len(offset_table)-1].key_null
        else:
            self._first_key = offset_table[0].key
            self._last_key = offset_table[len(offset_table)-1].key
        self._first_key_len = offset_table[0].key_len
        self._last_key_len = offset_table[len(offset_table)-1].key_len

    @staticmethod
    def _block_entry(t, version):
        if version == "2.0":
            format = b">Q"
        else:
            format = b">L"
        return struct.pack(format, t.offset)+t.key_null

    @staticmethod
    def _len_block_entry(t):
        # This is only accurate for version 2.0, but we only need approximate size anyway
        return 8 + len(t.key_null)

    def get_index_entry(self):
        # Returns a bytes object, containing the header data for this block
        if self._version == "2.0":
            long_format = b">Q"
            short_format = b">H"
        else:
            long_format = b">L"
            short_format = b">B"
        return (
            struct.pack(long_format, self._num_entries)
                          + struct.pack(short_format, self._first_key_len)
                          + self._first_key
                          + struct.pack(short_format, self._last_key_len)
                          + self._last_key
          + struct.pack(long_format, self._comp_size)
          + struct.pack(long_format, self._decomp_size)
                )

</file>
